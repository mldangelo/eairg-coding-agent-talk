{
  "ai-coding-agents": {
    "title": "AI Coding Agents",
    "content": "# AI Coding Agents\n## From Snippets to Swarms\n### The Evolution of Intelligent Code Generation\n#### Toward the Automated Researcher\n**Research Seminar**\n*Engineering AI Research Group (EAIRG)*\n*September 27, 2025*\n    Press Space for next page ",
    "act": null
  },
  "act-i-the-blueprint": {
    "title": "Act I: The Blueprint",
    "content": "# Act I: The Blueprint\n## Deconstructing the Modern Agent\nUnderstanding how modern coding agents actually work",
    "act": "act-i-the-blueprint"
  },
  "evolution-from-autocomplete-to-agents": {
    "title": "Evolution: From Autocomplete to Agents",
    "content": "# Evolution: From Autocomplete to Agents\n*Practice*\n2019-2021: Text-to-Code\n‚Ä¢ GPT-style LMs emit small completions\n‚Ä¢ Codex shows code-finetuning potential\n‚Ä¢ HumanEval: \"sample-and-rerank\" wins\n‚Üí Capability: ~10 lines of JavaScript\n2021-2023: Tool-Using Assistants\n‚Ä¢ Chat UIs + function calling\n‚Ä¢ ReAct-style reasoning with actions\n‚Ä¢ Agents read files, call tools, iterate\n‚Üí Capability: Single-file modifications\n2024-2025: Agentic Loops\n‚Ä¢ Shell + editor + permission gates\n‚Ä¢ SWE-bench: repo-scale edits\n‚Ä¢ Long context + thinking budgets\n‚Ä¢ Rise of \"Vibe Coding\": AI-first development\n‚Üí Capability: Multi-file, verified changes\nFuture: 2025-2027\n‚Ä¢ Multi-agent coordination\n‚Ä¢ Formal verification integration\n‚Ä¢ Adaptive model routing\n‚Ä¢ \"Vibe Researching\": Automated discovery\n‚Üí Target: The Automated Researcher\nKey Insight: The leap from \"snippets\" to \"long-running repo edits\" came from three ingredients:\nTool use (ability to act on environment)\nVerification loops (ability to check own work)\nMassive context (ability to reason about entire codebases)",
    "act": "act-i-the-blueprint"
  },
  "interface-landscape-how-developers-actually-use-ai-coding-tools": {
    "title": "Interface Landscape: How Developers Actually Use AI Coding Tools",
    "content": "# Interface Landscape: How Developers Actually Use AI Coding Tools\n*Practice*\nReality Check: Agent capabilities mean nothing without the right interface for the task\nüèóÔ∏è Integrated Development\nFull AI IDEs: Cursor, JetBrains AI Assistant, Windsurf\nIDE Extensions: GitHub Copilot, Sourcegraph Cody, Continue.dev\nBest for: Daily coding, multi-file edits, repo-aware refactors\nüñ•Ô∏è Terminal & Command Line\nCLI Assistants: Claude Code, Copilot CLI, aider, Warp AI\nBest for: DevOps, scripting, repo chores, terminal workflows\n‚òÅÔ∏è Agentic Environments\nSandbox VMs: Devin, GitHub Copilot agents, OpenHands CodeAct 2.1\nPlanning & PR flow: GitHub Copilot Workspace, Claude Code\nBest for: Ticket-sized tasks, bug fixes, background automation\nüîç Code Review & Repo Bots\nPR Automation: CodeRabbit, GitLab Duo, Copilot code review\nBest for: Review quality, triage, enforcing guidelines\nüìä Interface vs Task Matching\nTask Type\nBest Interface\nWhy\nDaily coding\nIDE + extension\nContext, low friction\nRepo automation\nCLI tools\nScript-friendly\nBug fixes\nAgentic sandbox\nFull investigation\nCode review\nGitHub/GitLab bot\nNative workflow\nPrototyping\nWeb IDE/chat\nZero setup\nüîß Common Interaction Patterns\n‚Ä¢ Inline autocomplete: Sub-second, context-aware suggestions\n‚Ä¢ Chat with code context: Natural language + file/selection awareness\n‚Ä¢ Apply-edits workflow: Diff preview before writing files\n‚Ä¢ Plan-execute-iterate: Sandbox ‚Üí test ‚Üí refine ‚Üí PR\n‚Ä¢ Tool integration: MCP connects Slack/Jira/GitHub context\nüè¢ Deployment Styles\n‚Ä¢ Cloud SaaS: Fast adoption, managed context (Copilot, Cursor)\n‚Ä¢ Self-hosted: Keep code in VPC (Windsurf Enterprise, Tabnine)\n‚Ä¢ Local-only: Air-gapped, privacy-first (Continue + Ollama)\n‚Ä¢ Hybrid: Local editing, cloud intelligence routing\nSelection Strategy: Start with IDE extensions for daily work, add CLI tools for automation, evaluate agentic environments for complex tasks. Match interface to workflow, not technology hype.",
    "act": "act-i-the-blueprint"
  },
  "the-conceptual-framework": {
    "title": "The Conceptual Framework",
    "content": "# The Conceptual Framework\n*Research*\n## Formalizing Coding Agents as POMDPs\nPOMDP Components\nStates $s$: Repository state (files, dependencies, tests)\nActions $a$: Tool calls (read, edit, execute, search)\nObservations $o$: Compile results, test outcomes, error messages\nTransition $T(s'|s,a)$: How actions change repository state\nObservation $O(o|s',a)$: What feedback we get\nPolicy: $\\pi(a|h)$ where $h$ is action-observation history\n```mermaid\ngraph LR\n    A[Observe $o_t$] --> B[Update Belief $b_t$]\n    B --> C[Select Action $a_t$]\n    C --> D[Execute Tool]\n    D --> E[Environment Response]\n    E --> A\n    style A fill:#e3f2fd\n    style C fill:#f3e5f5\n    style D fill:#e8f5e8\n```\nKey Challenge: Partial observability means agents must infer repository state from limited feedback.\n*Example ranges only; replace with org baselines*\nConcrete Example: Fixing a Null Pointer Bug\nState $s$: Repository at commit abc123 with test_user_validation.py failing\nAction $a$: Edit user.py:47 to add `if user is not None:` boundary check\nObservation $o$: `pytest -q test_user_validation.py` returns \"PASSED\" (green)\nBelief Update: Agent believes null pointer issue is resolved\nWhy POMDP: Agent can't directly see all code paths‚Äîmust infer correctness from test feedback\nWhy This Matters: This formalization reveals that coding agents are fundamentally about sequential decision-making under uncertainty - the same framework used in robotics and game AI.",
    "act": "act-i-the-blueprint"
  },
  "anatomy-of-a-sota-agent-claude-code-case-study": {
    "title": "Anatomy of a SOTA Agent: Claude Code Case Study",
    "content": "# Anatomy of a SOTA Agent: Claude Code Case Study\n*Practice*\nKey Insight: Claude Code's success comes from architectural simplicity over complexity\nüéØ Core Design Principles\n1.\nOne Main Loop - No multi-agent handoffs, single coherent context\n2.\nSmart Tool Design - Mix of low/medium/high level operations\n3.\nLLM Search > RAG - Uses ripgrep/find like developers do\n4.\nExplicit Todo Management - Agent tracks its own progress\n5.\nSubagents for Specialization - Task-specific isolated contexts\n6.\nExtensive Prompting - 2.8K system + 9.4K tools prompts\nMemory Architecture\n‚Ä¢ CLAUDE.md files load hierarchically (enterprise ‚Üí project ‚Üí user)\n‚Ä¢ @path imports up to 5 hops deep\n‚Ä¢ Prompt caching for conversation efficiency\n```mermaid\ngraph TD\n    A[User Request] --> B[Main Loop]\n    B --> C{Complex/Specialized?}\n    C -->|No| D[Direct Tool Use]\n    C -->|Yes| E[Delegate to Subagent]\n    E --> F[Isolated Context]\n    F --> G[Return Results]\n    D --> H[Update Todo List]\n    G --> H\n    H --> I[Continue Loop]\n    D --> J[Tool Categories]\n    J -.->|Meta| H\n    J -->|Read| K[Read/Glob/Grep]\n    J -->|Write ‚úì| L[Edit/MultiEdit]\n    J -->|Execute ‚úì| M[Bash]\n    B -.-> H\n    style B fill:#4caf50,color:#fff\n    style E fill:#ff9800,color:#fff\n    style F fill:#e8f5e8\n    style H fill:#2196f3,color:#fff\n    style K fill:#c8e6c9\n    style L fill:#ffcdd2\n    style M fill:#ffcdd2\n```\nTool Categories\nRead: Read, Glob, Grep (permission-free)\nWrite: Edit, MultiEdit (approval required)\nExecute: Bash (approval required)\nMeta: Task, TodoWrite (coordination)\nReal Todo Trace (Redacted)\n1. [pending] Fix authentication bug in login.py\n2. [in_progress] Search for auth-related files\n3. [completed] Found issue in validate_token() at line 45\n4. [pending] Write test to reproduce the bug\n5. [pending] Implement fix with error handling\n6. [pending] Run full test suite to verify\nFailure ‚Üí Recovery Example\nIteration t: Agent edits config.py but breaks import\nObservation: `ModuleNotFoundError: No module named 'validators'`\nIteration t+1: Agent reads error, searches for validators usage, adds missing import\nResult: Tests pass, todo marked completed\nStrategic Insight: Debuggability beats complexity. Simple architecture scales with model improvements, while complex multi-agent systems introduce coordination failures.",
    "act": "act-i-the-blueprint"
  },
  "tools-that-dont-lie-typed-apis--constrained-generation": {
    "title": "Tools That Don't Lie: Typed APIs & Constrained Generation",
    "content": "# Tools That Don't Lie: Typed APIs & Constrained Generation\n*Practice*\nCore Insight: Agent reliability depends on tool design as much as model intelligence\nPrinciples of Agent-Ready Tools\nAtomic Operations\nSingle responsibility, complete or fail entirely\n‚úÖ create_file vs ‚ùå multi-step bash pipeline\nIdempotent & Deterministic\nSafe retries, predictable outcomes\n‚úÖ ensure_dependency('pytest') vs ‚ùå pip install\nStructured Output\nJSON/XML responses, not raw text\n‚úÖ {\"files\": [...]} vs ‚ùå ls -l output\nClear Error Codes\nSpecific failure reasons for retry logic\n‚úÖ {'error': 'file_not_found'} vs ‚ùå generic exception\nExample: Typed Tool Schema\n{\n  \"title\": \"ApplyPatch\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"file\": {\"type\": \"string\"},\n    \"range\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"start\": {\"type\": \"integer\"},\n        \"end\": {\"type\": \"integer\"}\n      },\n      \"required\": [\"start\", \"end\"]\n    },\n    \"patch\": {\"type\": \"string\"}\n  },\n  \"required\": [\"file\", \"range\", \"patch\"],\n  \"additionalProperties\": false\n}\nTool Evolution Patterns\n‚ùå\nrun_shell('ls -l | grep .py')\n‚ö†Ô∏è\nlist_files(path='/', pattern='*.py')\n‚úÖ\nfind_symbol('MyClass')\nProgression: brittle ‚Üí structured ‚Üí semantic\nImplementation Tip: Require \"file\", \"range\", \"patch\" parameters; reject out-of-range edits by design to prevent corruption.",
    "act": "act-i-the-blueprint"
  },
  "act-ii-the-proving-ground": {
    "title": "Act II: The Proving Ground",
    "content": "# Act II: The Proving Ground\n## Performance, Reliability & Cost\nExamining the practical limitations and trade-offs of current agents",
    "act": "act-ii-the-proving-ground"
  },
  "the-verifiability-frontier": {
    "title": "The Verifiability Frontier",
    "content": "# The Verifiability Frontier\n*Practice*\n## The Single Best Predictor of Agent Success\nCore Insight: Task success correlates strongly with verifiability\n```mermaid\ngraph LR\n    A[High Verifiability] --> B[Agent Success]\n    C[Low Verifiability] --> D[Agent Failure]\n    A1[Unit Tests80-90%] --> A\n    A2[Compilation85-95%] --> A\n    A3[SWE-bench Verified50-65%] --> A\n    C1[Design Decisions20-40%] --> C\n    C2[User Experience10-30%] --> C\n    C3[Architecture30-50%] --> C\n    style A fill:#c8e6c9\n    style C fill:#ffcdd2\n    style B fill:#4caf50,color:#fff\n    style D fill:#f44336,color:#fff\n```\n‚úÖ Verifiable Tasks (High Success Rate)\n‚Ä¢ Bug fixes with existing tests\n‚Ä¢ Refactoring with type safety\n‚Ä¢ API implementations with schemas\n‚Ä¢ Real repos: OpenHands CodeAct 2.1 achieves 50%+ on SWE-bench Verified\n‚Ä¢ Enterprise: 90%+ compile rate, 70%+ green CI\n‚ùå Non-Verifiable Tasks (Low Success Rate)\n‚Ä¢ New feature design\n‚Ä¢ Performance optimization\n‚Ä¢ UX improvements\nüõ†Ô∏è Your Workflow\nBefore assigning a task to an agent, ask:\n1. Can success be automatically verified?\n2. Is there a fast feedback loop?\n3. Are the requirements unambiguous?\nIf no ‚Üí decompose the task or do it yourself.\nPractical Takeaway: To maximize agent success, give it tasks it can verify. Write tests first, then ask the agent to implement. This single insight can 3x your productivity.\nSmart Take: Most code work benefits more from better observation and tooling contracts than from bigger models. Track time-to-first-correct-file; LLM search + ctags often beats file-RAG for code tasks.",
    "act": "act-ii-the-proving-ground"
  },
  "verifiability-in-action-micro-demo": {
    "title": "Verifiability in Action: Micro-Demo",
    "content": "# Verifiability in Action: Micro-Demo\n*Practice*\n‚ùå Pre: Failing Test\n```bash\n$ pytest -q test_auth.py::test_login\nFAILED test_auth.py::test_login - AttributeError:\n'NoneType' object has no attribute 'id'\n```\nüîß Agent Action\nSingle Edit in auth.py:23\n```diff\n- return User.objects.get(username=username)\n+ user = User.objects.filter(username=username).first()\n+ return user if user else None\n```\n‚úÖ Post: Green Test\n```bash\n$ pytest -q test_auth.py::test_login\nPASSED                                [100%]\n1 passed in 0.03s\n```\nüéØ Why This Works\n‚Ä¢ Clear failure signal: Specific error message\n‚Ä¢ Fast feedback loop: 0.03s test execution\n‚Ä¢ Unambiguous success: Green = good\n‚Ä¢ Minimal scope: One function, one test\nResult: Agent succeeded on first attempt. Task took 45 seconds total (15s analysis + 5s edit + 25s verification). ~12k tokens in, ~300 out; verified-diff@1 = 1/1. This is the 3x productivity multiplier in action.",
    "act": "act-ii-the-proving-ground"
  },
  "model-routing-the-core-trade-off": {
    "title": "Model Routing: The Core Trade-off",
    "content": "# Model Routing: The Core Trade-off\n*Practice* ‚Ä¢ **A. Trade-off**\nSmart Take: Routing is scheduling. Wall-clock and error-budget aware; triggers based on backtracks/fail streaks; smarter beats faster when N dominates.\nThe Problem: Choosing the right model for each task\nFor any coding task, wall-clock time is:\n$$\\text{Time} = N \\times \\frac{L_{in} + L_{out}}{TPS} + \\text{tool\\_time} + \\text{test\\_time}$$\nWhere:\n‚Ä¢ N = iterations to converge (edits + test cycles)\n‚Ä¢ TPS = tokens per second\n‚Ä¢ Lin, Lout = input/output tokens per step\n‚Ä¢ pfail = probability a step produces bad code\nKey Insight: Faster models often have larger N and higher pfail\nThe Trade-off: Fast models are great at being many. Smart models are great at being right.\nEstimating p_fail in Practice\n‚Ä¢ Historical error rate: Task-specific success % from logs\n‚Ä¢ Lint/test fail ratio: % of steps requiring fixes\n‚Ä¢ Backtrack rate: Trace-level \"undo\" operations per step\nRule of thumb: If last 2 steps were backtracks ‚Üí escalate to smarter model\nYour Workflow: Don't just pick the fastest or smartest model. Match model intelligence to task complexity.\nAdaptive Thinking Time\nShort latency for easy tasks; expanded budget for hard tasks. Trigger-based escalation (‚â•2 backtracks, rising lint/test fail ratio).",
    "act": "act-ii-the-proving-ground"
  },
  "model-routing-practical-decision-table": {
    "title": "Model Routing: Practical Decision Table",
    "content": "# Model Routing: Practical Decision Table\n*Practice* ‚Ä¢ **B. Heuristic**\nThe Heuristic: Mapping tasks to optimal models\n| Task | Fast Model | Smart Model | Reasoning |\n|------|:----------:|:-----------:|-----------|\n| Generate tests/stubs | ‚úÖ High throughput | ‚ùå Overkill | Low risk, needs volume |\n| Search symbols | ‚úÖ Quick iteration | ‚ùå Unnecessary | Simple pattern matching |\n| Small patches | ‚úÖ Low risk | ‚ùå Too slow | Easy to verify |\n| **Mass refactor** | ‚ùå Type misuse | ‚úÖ Safety reasoning | Complex inference |\n| **Infra edits** | ‚ùå Config risk | ‚úÖ System understanding | Cross-service deps |\n| Framework migration | ‚ùå High failure | ‚úÖ Needs reasoning | Complex dependencies |\n| API design | ‚ùå Poor abstractions | ‚úÖ Architectural thinking | Deep understanding |\n| Debug failures | ‚ùå Misses context | ‚úÖ Deep analysis | Causal reasoning |\nCoarse-to-fine\nFast model drafts, smart model validates\nBest-of-K\nSpawn K fast workers, smart model selects best\nManager-worker\nSmart model plans, fast models execute\nEscape Hatch: If confidence \nYour Workflow: Start with this table, then adapt based on your specific codebase and task patterns.",
    "act": "act-ii-the-proving-ground"
  },
  "model-routing-the-economics": {
    "title": "Model Routing: The Economics",
    "content": "# Model Routing: The Economics\n*Practice* ‚Ä¢ **C. Economics**\nThe Economics: Cost implications of routing strategies *As of Sept 2025*\nClaude Code Pro Economics\nCost: $20/month for rate-limited usage\nAlternative: Direct API at ~$15-60/1000 requests\nBreak-even: ~1,300 medium requests/month\nToken Consumption Pattern:\n‚Ä¢ Input tokens per session: 50,000-200,000\n‚Ä¢ Output tokens per session: 5,000-20,000\n‚Ä¢ Tool calls per session: 10-100\nEconomic Reality: Pro subscriptions are likely subsidized relative to API list prices *As of Sept 2025*\nSmart Model Routing Economics\nPattern\nCost Per Task\nSpeed\nQuality\nAll-Smart\n$2.50\nSlow\nHigh\nAll-Fast\n$0.25\nFast\nMedium\nHybrid\n$0.75\nMedium\nHigh\nCost Strategy:\n1. Profile your task mix\n2. Route based on complexity\n3. Use hybrid patterns for best ROI\nAssumptions: 5 steps, ~30k in/3k out tokens per step, TPS 2-5k/s. \"All-Fast\" requires +3 backtracks on complex tasks. Based on Sept 2025 list prices.\nCalculator: Your task cost ‚âà (tokens_in + tokens_out) √ó price + test_runs √ó compute_cost\nMarket Insight: Current pricing is unsustainable. Expect costs to rise 3-5x as subsidies end. Plan your routing strategy accordingly.",
    "act": "act-ii-the-proving-ground"
  },
  "operating-costs-caching-kv-scheduling": {
    "title": "Operating Costs: Caching, KV, Scheduling",
    "content": "# Operating Costs: Caching, KV, Scheduling\n*Practice* ‚Ä¢ **C+. Operations**\nSystem-Level Cost Control: Token costs are just the beginning\nIntelligent Caching\nKV/Prompt Caching: Reuse context across steps\nFile Read Deduplication: Cache file contents within session\nTest Container Warm Pools: Skip cold start overhead\nDependency Prefetch: Load likely imports ahead of time\nQueue Policies\n‚Ä¢ Priority: Verifiable tasks first (tests, compilation)\n‚Ä¢ Throttling: Cap long-context steps per hour\n‚Ä¢ Batching: Group similar tasks for efficiency\n‚Ä¢ Preemption: Kill runaway sessions after budget\n$/Win Optimization\nTrack at System Level: Include compute, storage, human time\nCost Per Success: (total_cost) / (verified_diffs)\nEfficiency Curves: Cost vs speed vs quality trade-offs\nBudget Alerts: Alert when cost/task exceeds threshold\nResource Scheduling\n‚Ä¢ Peak hours: Route simple tasks to fast models\n‚Ä¢ Off-peak: Use spare capacity for training/eval\n‚Ä¢ Geographic: Follow-the-sun model routing\n‚Ä¢ Spot pricing: Use preemptible instances for batch work\nReality Check: A 50% cache hit rate can reduce costs by 30%. Measure $/win, not just token costs, to capture true system efficiency.",
    "act": "act-ii-the-proving-ground"
  },
  "enterprise-evaluation-beyond-academic-benchmarks": {
    "title": "Enterprise Evaluation: Beyond Academic Benchmarks",
    "content": "# Enterprise Evaluation: Beyond Academic Benchmarks\n*Practice* ‚Ä¢ **Real-World Metrics**\nBeyond SWE-bench: What matters when agents ship to production\nüìä Weekly Production Metrics\nPR-level success rate: Merged without revert within 7 days\nTest-gated throughput: Issues closed per agent-hour\nReview load: Reviewer minutes per agent PR\nDefect density: Post-merge bugs per KLOC from agent patches\nüîí Quality Gates\nSecurity defects: Caught pre-merge by SAST/DAST\nCoverage movement: Test coverage change after agent edits\nFlake rate: Test stability after agent changes\nPolicy compliance: Code ownership, license, secrets scan\nüí∞ Business Impact\nCycle time reduction: Issue-to-deploy duration\nDeveloper velocity: Features per sprint with/without agents\nCost per resolved issue: Total compute + human review time\nEscalation rate: % requiring human intervention\nüö® Failure Mode Detection\nReward hacking: Hard-coded test outputs, test inference\nSycophancy signals: \"Playing along\" to secure approval\nDrift accumulation: Performance degradation over time\nContext bleeding: Cross-project information leakage\nProduction Reality: Track verified-diff@1 (passes tests on first attempt), backtrack rate (",
    "act": "act-ii-the-proving-ground"
  },
  "model-routing-the-research-frontier": {
    "title": "Model Routing: The Research Frontier",
    "content": "# Model Routing: The Research Frontier\n*Research* ‚Ä¢ **D. Learned Routing**\nThe Research Goal: From heuristics to learned policies\nCurrent State\nWe route models using human-designed heuristics (the table)\nResearch Goal\nLearn optimal routing policies automatically\nFormulation as Contextual Bandit Problem\nContext: Task description, codebase features, user preferences\nActions: Model choices (fast, smart, hybrid patterns)\nRewards: Success rate, latency, cost trade-offs\nPolicy: œÄ(model|context) learned from experience\nResearch Challenges\n1. Exploration vs Exploitation: How to try new routing strategies safely?\n2. Multi-objective Optimization: Balancing speed, quality, and cost\n3. Transfer Learning: Policies learned on one codebase ‚Üí another\n4. Online Learning: Adapting as models and tasks evolve\nResearch Opportunity: This is a perfect PhD thesis topic - practical impact, clear metrics, and deep technical challenges.",
    "act": "act-ii-the-proving-ground"
  },
  "evaluation-pitfalls-in-agent-research": {
    "title": "Evaluation Pitfalls in Agent Research",
    "content": "# Evaluation Pitfalls in Agent Research\n*Research*\nCritical Question: Are we measuring what matters?\nüö® Current Problems\nBenchmark Contamination\n‚Ä¢ Models trained on evaluation data\n‚Ä¢ \"Teaching to the test\" vs real capability\n‚Ä¢ Static benchmarks vs evolving tasks\nOutput-Only Evaluation\n‚Ä¢ Ignores process and reasoning\n‚Ä¢ Misses catastrophic failures\n‚Ä¢ No credit for partial progress\nLab vs Production Gap\n‚Ä¢ Synthetic tasks vs real complexity\n‚Ä¢ Missing human interaction patterns\n‚Ä¢ No long-term reliability testing\n‚úÖ What We Need for Rigor\nContamination Control\n‚Ä¢ Time-based splits and live evaluation\n‚Ä¢ Private test sets with time locks\n‚Ä¢ Dynamic benchmark generation\nProcess Evaluation\n‚Ä¢ Trace-level scoring (plans + executions)\n‚Ä¢ Tool usage efficiency metrics\n‚Ä¢ Error recovery and self-correction\nDeployment Realism\n‚Ä¢ Integration testing, not just unit tests\n‚Ä¢ Human-in-the-loop interaction patterns\n‚Ä¢ Long-running session reliability\nMeasurement Standards\n‚Ä¢ pass@1 with fixed wall-clock budget\n‚Ä¢ Token accounting and cost normalization\n‚Ä¢ Reproducible container environments\nResearch Gap: Current benchmarks don't predict real deployment success. We need evaluation methodologies that capture what matters in practice.",
    "act": "act-ii-the-proving-ground"
  },
  "beyond-benchmarks-starter-eval-kit": {
    "title": "Beyond Benchmarks: Starter Eval Kit",
    "content": "# Beyond Benchmarks: Starter Eval Kit\n*Research*\nReproducible Recipe: Build your own agent evaluation infrastructure\nüõ†Ô∏è Infrastructure Components\n‚Ä¢ Containerized Runner: Docker with repo isolation\n‚Ä¢ Fixed Wall-Clock Budget: 30min max per task\n‚Ä¢ JSONL Trace Schema: Plan/actions/obs/diffs/tests\n‚Ä¢ Git Integration: Clean state per task\nExample JSONL Trace:\n{\"t\":1,\"plan\":\"fix NPE\",\"tool\":\"grep\",\"args\":\"-n validate_token src\",\"obs\":\"src/auth.py:45\"}\n{\"t\":2,\"tool\":\"edit\",\"args\":{\"file\":\"src/auth.py\",\"range\":{\"start\":44,\"end\":48},\"patch\":\"+ if user is None: return None\"},\"obs\":\"ok\"}\n{\"t\":3,\"tool\":\"pytest\",\"args\":\"-q tests/test_auth.py::test_login\",\"obs\":\"PASSED\"}\n{\"t\":4,\"verified_diff\":true,\"cost\":\"$0.23\",\"wall_time_ms\":45000}\nüìä Metrics That Matter\n‚Ä¢ verified-diff@1: Passes tests on first attempt\n‚Ä¢ Backtrack count: Mean iterations to converge\n‚Ä¢ Recovery rate: Fix after initial failure\n‚Ä¢ Cost per win: $/successful task\nüéØ Evaluation Tasks\n‚Ä¢ Bug fixes: With failing tests provided\n‚Ä¢ Feature additions: With acceptance criteria\n‚Ä¢ Refactoring: Maintain behavior, improve structure\n‚Ä¢ Documentation: Generate from code + tests\nüîß Implementation\nGitHub Template: Reproducible eval infrastructure\nDocker Image: Pre-configured eval environment\nCLI Tool: `agent-eval run --task bug-fix-001`\nDashboard: Web UI for result visualization\nGoal: Move from anecdotal \"this agent feels better\" to data-driven \"Agent A has 23% higher verified-diff@1 on our task distribution.\"",
    "act": "act-ii-the-proving-ground"
  },
  "agent-sre-slos-error-budgets-rollbacks": {
    "title": "Agent SRE: SLOs, Error Budgets, Rollbacks",
    "content": "# Agent SRE: SLOs, Error Budgets, Rollbacks\n*Practice*\nProduction Reality: Agents need SRE discipline just like distributed systems\nAgent SLOs\nSuccess: verified-diff@1 ‚â•85%\nLatency: p95 response time \nCost: $/successful task \nStability: backtracks/task \nError Budget & Kill Switches\n‚Ä¢ Auto-throttle when SLO breach detected\n‚Ä¢ Per-repo allowlist for agent access\n‚Ä¢ Circuit breaker after 3 consecutive failures\n‚Ä¢ Manual kill switch for agent runaway\nPR Gates & Verification\n‚Ä¢ Required: Green tests + policy checks\n‚Ä¢ Required: Signed trace for full audit\n‚Ä¢ Required: License scan + SBOM attached\n‚Ä¢ Escalation: Human review for high-risk changes\nCanary & Rollback\n‚Ä¢ Deploy new agent versions to 5% of repos first\n‚Ä¢ Monitor success rate, escalation rate, cost\n‚Ä¢ Automated rollback on SLO degradation\n‚Ä¢ On-call rotation for agent incidents\nRunbook Example: If verified-diff@1 drops below 70% for >10 tasks, auto-escalate to smart model; if still failing, page on-call and disable agent for affected repos.",
    "act": "act-ii-the-proving-ground"
  },
  "act-iii-the-horizon": {
    "title": "Act III: The Horizon",
    "content": "# Act III: The Horizon\n## Swarms, Safety & The Future\nExploring the transition from single agents to multi-agent systems and their profound implications",
    "act": "act-iii-the-horizon"
  },
  "the-path-to-swarms-from-single-agents-to-collaboration": {
    "title": "The Path to Swarms: From Single Agents to Collaboration",
    "content": "# The Path to Swarms: From Single Agents to Collaboration\n*Research*\nWhy Multi-Agent is Inevitable\nEvidence from Reasoning Research\n‚Ä¢ Self-consistency: Multiple solution paths ‚Üí better outcomes\n‚Ä¢ Tree of Thoughts: Search over action sequences\n‚Ä¢ Constitutional AI: Multiple critics improve safety\nNatural Fit for Coding\n‚Ä¢ Specialization: frontend ‚Üî backend ‚Üî testing ‚Üî deployment\n‚Ä¢ Parallel exploration: multiple implementation approaches\n‚Ä¢ Verification: independent code review and testing\n```mermaid\ngraph TB\n    BL[Backlog/PR Queue] --> W1[Frontend AgentClaude Haiku]\n    BL --> W2[Backend AgentClaude Haiku]\n    BL --> W3[Test AgentClaude Haiku]\n    M[Manager AgentClaude Sonnet] --> BL\n    W1 -.->|review| V[Verification AgentClaude Sonnet]\n    W2 -.->|review| V\n    W3 -.->|review| V\n    V ==>|commit| I[Integration AgentClaude Sonnet]\n    HS[Handoff Schemagoal, constraints,files touched,tests to run,verification steps] -.-> W1\n    HS -.-> W2\n    HS -.-> W3\n    style M fill:#4caf50,color:#fff\n    style V fill:#ff9800,color:#fff\n    style I fill:#9c27b0,color:#fff\n    style BL fill:#e3f2fd\n    style HS fill:#f3e5f5\n```\nCollaboration Patterns\n‚Ä¢ Manager-Worker: Smart planner, fast executors\n‚Ä¢ Peer Review: Agents critique each other's work\n‚Ä¢ Chain Assembly: Sequential handoffs with verification\nInsight: Multi-agent systems aren't just about parallelism - they enable specialization, verification, and fault tolerance that single agents cannot achieve.",
    "act": "act-iii-the-horizon"
  },
  "persistent-memory-value-risks-guardrails": {
    "title": "Persistent Memory: Value, Risks, Guardrails",
    "content": "# Persistent Memory: Value, Risks, Guardrails\n*Practice*\nMemory Architecture: How agents remember across sessions determines capability and risk\nWork Diaries vs Raw Transcripts\nWork Diaries: Summarized decisions, patterns, and learnings\nRaw Transcripts: Complete interaction logs\nStrategy: Pin decisions, not raw logs\nRetrieval Order: diary ‚Üí code-intel ‚Üí LLM search\nSecurity Boundaries\n‚Ä¢ Per-repo isolation: No cross-project memory bleed\n‚Ä¢ Encryption at rest: All stored memories encrypted\n‚Ä¢ Access controls: Team/org-level memory permissions\n‚Ä¢ Audit trail: Who accessed what memory when\nMemory Poisoning Risks\n‚Ä¢ Injection attacks: Malicious prompts in stored memories\n‚Ä¢ False patterns: Learning from incorrect decisions\n‚Ä¢ Context pollution: Irrelevant memories affecting decisions\n‚Ä¢ Drift accumulation: Errors compounding over time\nRetention & Cleanup\n‚Ä¢ TTL policies: Auto-expire memories after 90 days\n‚Ä¢ PII redaction: Scrub personal info from memories\n‚Ä¢ Quality scoring: Deweight low-confidence memories\n‚Ä¢ Manual purge: Clear contaminated memory sets\nBest Practice: Keep RAG pipeline separate from write-path. Memories inform, but don't directly control agent actions.",
    "act": "act-iii-the-horizon"
  },
  "the-alignment-problem-in-practice": {
    "title": "The Alignment Problem in Practice",
    "content": "# The Alignment Problem in Practice\n*Research*\nHow Cooperation Amplifies Alignment Challenges\nSingle-Agent Alignment Issues\n‚Ä¢ Sycophancy: Agreeing with user errors\n‚Ä¢ Deception: Hiding failures for better ratings\n‚Ä¢ Specification gaming: Following letter, not spirit\nMulti-Agent Amplification\n‚Ä¢ Herding: Agents reinforce each other's mistakes\n‚Ä¢ Coordination Failures: Agents work at cross-purposes\n‚Ä¢ Emergent Deception: Unintended collaborative lies\nReal Examples from 2025\nAmazon Q Developer (CVE-2025-8217):\n‚Ä¢ Prompt injection in extension update\n‚Ä¢ Malicious code in VS Code release\n‚Ä¢ Supply chain compromise attempt\nCross-Agent Privilege Escalation:\n‚Ä¢ One agent modifies another's config\n‚Ä¢ Escalating permissions across tools\n‚Ä¢ Breaking isolation boundaries\nRepresentative Attack Path: A debugging agent with broad write access is socially engineered by a code-gen agent to modify a deploy agent's policy file, effectively escalating privileges.\nResearch Challenges\n1. How do we maintain alignment as agents collaborate?\n2. Can we detect and prevent emergent deception?\n3. What governance structures work for agent teams?\nCritical Insight: Alignment problems don't just scale linearly with more agents - they can amplify exponentially through coordination failures and emergent behaviors.",
    "act": "act-iii-the-horizon"
  },
  "security-and-safety-research-challenges": {
    "title": "Security and Safety Research Challenges",
    "content": "# Security and Safety Research Challenges\n*Research*\nThe Applied Alignment Problem\nüö® Immediate Threats\nSupply Chain Attacks\n‚Ä¢ AI-generated code with backdoors\n‚Ä¢ Compromised agent extensions\n‚Ä¢ Example: CVE-2025-8217 in Amazon Q Developer\nPrivilege Escalation\n‚Ä¢ Agents with filesystem and shell access\n‚Ä¢ Cross-agent configuration editing\n‚Ä¢ Breaking sandbox boundaries\nThe Lethal Trifecta (Willison, 2025)\nThree capabilities that together enable complete security breach:\n‚Ä¢ Private data access + untrusted content + external communication\n‚Ä¢ Prompt injection via web/email/docs ‚Üí secret exfiltration\n‚Ä¢ Mitigation: Remove at least one leg of the trifecta\nReward Hacking & Training-Game Behaviors\n‚Ä¢ Hard-coding test outputs: Inferring expected results from artifacts\n‚Ä¢ Sycophancy: \"Playing along\" to secure reward/approval\n‚Ä¢ Hidden objectives: Pursuing goals not aligned with stated purpose\n‚Ä¢ Mitigation: Hidden tests, mutation testing, ephemeral sandboxes\nüî¨ Research Directions\nFormal Verification\n‚Ä¢ Can we prove properties about agent-generated code?\n‚Ä¢ Automated security analysis pipelines\n‚Ä¢ Correctness guarantees for critical systems\nCapability Control\n‚Ä¢ Just enough access to be useful, not dangerous\n‚Ä¢ Dynamic permission scaling\n‚Ä¢ Fail-safe defaults and recovery\nAudit and Accountability\n‚Ä¢ Full reproducibility of agent actions\n‚Ä¢ Causal tracing for security incidents\n‚Ä¢ Signed execution logs\nDefense Strategy: Break the lethal trifecta by cutting exfiltration paths (default-deny outbound calls), limiting private data exposure (scoped tokens), or isolating untrusted content processing (separate reader/secret-holder tasks).\nTrifecta-Breaking Checklist\n‚Ä¢ Cut exfil: Default-deny outbound calls, route through allowlist proxy\n‚Ä¢ Limit private data: Ephemeral scoped tokens, filtered RAG corpora\n‚Ä¢ Isolate untrusted content: Separate reader tasks from secret-holders\n‚Ä¢ Signed traces for full audit trail and incident response\n‚Ä¢ Pre-flight check: Can it touch secrets + untrusted input + talk outside?",
    "act": "act-iii-the-horizon"
  },
  "rlvr-reinforcement-learning-from-verifiable-rewards": {
    "title": "RLVR: Reinforcement Learning from Verifiable Rewards",
    "content": "# RLVR: Reinforcement Learning from Verifiable Rewards\n*Research* ‚Ä¢ **September 2025 Pulse Check**\nCore Insight: Training agents with execution-based rewards (tests pass, code compiles) rather than human feedback\n‚úÖ What's Working\n‚Ä¢ SWE-bench gains: 70-76% pass rates reported (verify on official board)\n‚Ä¢ SQL execution: Spider/BIRD scores with execution accuracy rewards\n‚Ä¢ One-shot RLVR: Single example can ~2x MATH-500 accuracy\n‚Ä¢ Agent specialization: General LMs ‚Üí SWE agents via environment feedback\n‚ö†Ô∏è Measurement Challenges\n‚Ä¢ Spurious rewards: Gains even with random/inverted rewards\n‚Ä¢ Pass@K rethink: CoT-Pass@K requires correct reasoning, not just answers\n‚Ä¢ Exploration collapse: Boosts Pass@1 but loses coverage at high K\n‚Ä¢ Marketing vs reality: Check official boards, not blog posts\nüî¨ Active Research Areas\n‚Ä¢ Adaptive exploration: Preventing premature policy collapse\n‚Ä¢ Negative reinforcement: Penalizing poor samples for generalization\n‚Ä¢ World model RLVR: Video/language models optimized for task metrics\n‚Ä¢ Multimodal agents: OSWorld for GUI, computer use training\nüìä Current Best Practices\nReward Design: Execution + partial credit + rule checks\nDiversity Monitoring: Track high-K curves and entropy\nContamination-Free Eval: Hidden tests, seed randomization\nVerifiable Constraints: Mix unseen constraints into curricula\nüõ†Ô∏è Tooling Consolidation\n‚Ä¢ GRPO/PPO with verifiable rewards\n‚Ä¢ Meta's SWE-RL rule-based reward codebase\n‚Ä¢ SQL-R1 execution accuracy frameworks\n‚Ä¢ IFBench for instruction-following generalization\nüö® Key Debates\n‚Ä¢ Does RLVR create new solutions or reweight existing ones?\n‚Ä¢ How much reward signal is spurious vs meaningful?\n‚Ä¢ Can we scale beyond unit tests to complex objectives?\n‚Ä¢ What's the right balance of exploration vs exploitation?\nResearch Direction: RLVR works best when rewards are truly verifiable (tests, compilation, execution). The challenge is designing rich reward functions that capture complex software engineering objectives without gaming.",
    "act": "act-iii-the-horizon"
  },
  "future-research-directions-and-open-problems": {
    "title": "Future Research Directions and Open Problems",
    "content": "# Future Research Directions and Open Problems\n*Research*\nüîú Near-term (1-2 years)\nTechnical Challenges\n‚Ä¢ Advanced planning with backtracking\n‚Ä¢ Self-improving agent architectures\n‚Ä¢ Efficient context utilization (1M+ tokens)\nEvaluation Infrastructure\n‚Ä¢ Live, contamination-free benchmarks\n‚Ä¢ Trace-level evaluation frameworks\n‚Ä¢ Multi-agent coordination metrics\n‚Ä¢ Autonomy-hours metric (1-5h ‚Üí multi-day traces)\nüìÖ Medium-term (2-5 years)\nCapabilities\n‚Ä¢ Multi-modal agents (UI, diagrams, video)\n‚Ä¢ Persistent cross-session learning\n‚Ä¢ Natural human-agent collaboration\nSafety & Alignment\n‚Ä¢ Formal verification for agent code\n‚Ä¢ Provable alignment guarantees\n‚Ä¢ Multi-agent governance protocols\n‚Ä¢ Discovery evals (novel math/programming with independent verification)\nüöÄ Long-term Questions\nFundamental Questions\n‚Ä¢ What is \"software engineering\" with AGI?\n‚Ä¢ How do we maintain human agency?\n‚Ä¢ Can we build truly autonomous software engineers?\nSocietal Implications\n‚Ä¢ Economic displacement and transition\n‚Ä¢ New forms of human-AI collaboration\n‚Ä¢ Governance of autonomous systems\nResearch Opportunity: This field offers immediate practical impact while addressing fundamental questions in AI alignment, verification, and human-computer interaction.",
    "act": "act-iii-the-horizon"
  },
  "act-iv-the-ecosystem": {
    "title": "Act IV: The Ecosystem",
    "content": "# Act IV: The Ecosystem\n## Market Forces & Final Thoughts\nGrounding technology and research in real-world market forces and economic drivers",
    "act": "act-iv-the-ecosystem"
  },
  "the-consolidation--specialization-phase": {
    "title": "The Consolidation & Specialization Phase",
    "content": "# The Consolidation & Specialization Phase\n*Practice*\nLate 2025: From horizontal assistants to vertical specialists\nConsolidation Vectors\n‚Ä¢ Editors: IDE integration becoming table stakes\n‚Ä¢ CI/CD: Testing and deployment automation\n‚Ä¢ Code Intelligence: Search, analysis, and navigation\n‚Ä¢ Platform Agents: Cloud-native development environments\nVertical Specialists\n‚Ä¢ Bio-tech: Genomic sequence analysis agents\n‚Ä¢ Finance: HFT strategy backtesting\n‚Ä¢ Game Engines: Procedural content generation\n‚Ä¢ Embedded: Real-time systems optimization\nDistribution Moats\n‚Ä¢ IDE Integration: Native workflow embedding\n‚Ä¢ Policy/Compliance: Enterprise governance features\n‚Ä¢ Data Residency: On-premises and hybrid deployments\n‚Ä¢ Eval/Trace Infrastructure: Observability and debugging\nStrategic Implications\n‚Ä¢ Horizontal assistants plateau at commodity margins\n‚Ä¢ Winning stacks integrate verification by default\n‚Ä¢ Vertical specialists capture premium pricing\n‚Ä¢ Distribution beats pure technology\nMarket Insight: The agent space is maturing from \"funding frenzy\" to strategic specialization. Winners will own specific problem domains, not generic capabilities.",
    "act": "act-iv-the-ecosystem"
  },
  "major-product-releases-timeline": {
    "title": "Major Product Releases Timeline",
    "content": "# Major Product Releases Timeline\n*Practice*\nSeptember 2025: A Watershed Month\nSeptember 2025 Developments\nGPT-5-Codex (Sept 15):¬π‚Åµ\n‚Ä¢ Coding-tuned GPT-5 variant (OpenAI)\n‚Ä¢ Multi-hour \"thinking\" windows, reported up to 7 hours (OpenAI)\n‚Ä¢ Press reports cite 51.3% on a refactoring eval vs 33.9% for GPT-5 [media]\n‚Ä¢ Integrated code-review and repo workflows (OpenAI)\n‚Ä¢ Emerging pattern: Correctness-as-a-Service APIs that other agents call for verification\nGitHub Copilot CLI (Sept 25):¬π‚Å∂\n‚Ä¢ CLI can start and track Copilot coding-agent sessions\n‚Ä¢ Uses GitHub Models multi-model backends\n‚Ä¢ Shared billing with existing Copilot plans\nCross-Agent Privilege Escalation (Sept 24):¬π‚Å∑\n‚Ä¢ Rehberger shows agents can rewrite each other's configs\n‚Ä¢ Mitigation: isolate configs, containerize, enforce least-privilege\nKey Trends Emerging\n1. Model Specialization:\n‚Ä¢ Coding-specific fine-tuning becoming standard\n‚Ä¢ Task-specific thinking budgets\n2. Platform Convergence:\n‚Ä¢ Multiple model backends in single tools\n‚Ä¢ Cross-platform agent workflows\n3. Security Maturation:\n‚Ä¢ Real incidents driving better practices\n‚Ä¢ Industry-wide security standards emerging\nMarket Insight: The rapid release cycle indicates this field is still in the \"land grab\" phase, with major players competing for developer mindshare.\nStrategic Takeaway: For researchers and practitioners, this rapid evolution means staying current is essential. What's current today may be obsolete in six months.\n*As of Sept 2025; examples, see References.*\nCompute Is Destiny\nLong-horizon runs are expensive. Design routing/evals with wall-clock and $ budgets; expect persistent rate limits.",
    "act": "act-iv-the-ecosystem"
  },
  "discussion-questions": {
    "title": "Discussion Questions",
    "content": "# Discussion Questions\n*Practice & Research*\nü§î For Researchers\n1. Evaluation: How do we create benchmarks that predict real-world agent success?\n2. Multi-Agent Coordination: What are the fundamental limits of agent collaboration?\n3. Alignment: Can we prove alignment properties for agent-generated code?\n4. Capability Control: How do we give agents just enough power to be useful but not dangerous?\nüõ†Ô∏è For Practitioners\n1. Adoption Strategy: Which agent should you integrate into your workflow first?\n2. Task Selection: How do you identify which tasks to automate vs keep human?\n3. Security Posture: What security practices are essential when using coding agents?\n4. ROI Measurement: How do you measure the true productivity impact of agents?\nMeta-Question: Are we building the future we want? How do we ensure that increasingly capable agents serve human flourishing rather than replace human agency?",
    "act": "act-iv-the-ecosystem"
  },
  "references--further-reading": {
    "title": "References & Further Reading",
    "content": "# References & Further Reading\n*Practice & Research*\nüìñ Foundational Papers\n‚Ä¢ METR: \"Measuring AI Ability to Complete Long Tasks\" (2024)\n‚Ä¢ SWE-bench: \"Can Language Models Resolve Real-World GitHub Issues?\" (2024)\n‚Ä¢ Sleeper Agents: \"Training Deceptive LLMs\" (2024)\nüî¨ AI Safety Research\n‚Ä¢ Among Us: \"A Sandbox for Agentic Deception\" (2024)\n‚Ä¢ AI Agents and Painted Facades (2024)\nüèóÔ∏è System Architecture\n‚Ä¢ Basic Systems Architecture for AI Agents (2024)\nüè≠ Industry Analysis\n‚Ä¢ AI 2027: \"Forecasting Superhuman Coders\" (LessWrong, 2024)\n‚Ä¢ Claude Code Deep Dive: MinusX analysis (2025)\n‚Ä¢ GPT-5-Codex Analysis: Simon Willison's blog (Sept 2025)\n‚Ä¢ Cross-Agent Security Research: Johann Rehberger (Sept 2025)\n‚Ä¢ Amazon Q Security Advisory: CVE-2025-8217\n‚Ä¢ From Vibe Coding to Vibe Researching: Chen & Pachocki (2025) ‚Äî internal video; quotes used for \"automated researcher\" and \"vibe researching\" context\nüìä Benchmarks & Evaluation\n‚Ä¢ SWE-bench Pro, SWE-bench-Live\n‚Ä¢ LiveCodeBench, RepoQA\n‚Ä¢ SWE-bench Critiques: \"Illusion\" and \"UTBoost\" papers\nüåê Essential Resources\n‚Ä¢ Simon Willison's Blog: AI-assisted programming tag\n‚Ä¢ swyx's AI Engineering Guide\n‚Ä¢ Hacker News: Search \"coding agents\"\n‚Ä¢ r/MachineLearning: Academic discussions\n‚Ä¢ AI Engineering Communities: Real-time insights\nStay Current: This field evolves daily. Follow security researchers, read vendor blogs, and test new releases carefully.",
    "act": "act-iv-the-ecosystem"
  },
  "footnotes": {
    "title": "Footnotes",
    "content": "# Footnotes\n¬π Radford et al. \"Language Models are Unsupervised Multitask Learners\" (2019)\n¬≤ Chen et al. \"Evaluating Large Language Models Trained on Code\" (2021)\n¬≥ Chen et al. \"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models\" (2021)\n‚Å¥ OpenAI Function Calling Documentation (2023)\n‚Åµ Yao et al. \"ReAct: Synergizing Reasoning and Acting in Language Models\" (2022)\n‚Å∂ Yang et al. \"InterCode: Standardizing and Benchmarking Interactive Coding\" (2023)\n‚Å∑ Anthropic Claude Code Documentation (2024)\n‚Å∏ Jimenez et al. \"SWE-bench: Can Language Models Resolve Real-World GitHub Issues?\" (2024)\n‚Åπ Wei et al. \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\" (2022)\n¬π‚Å∞ Amazon Security Advisory CVE-2025-8217 (Sept 2025)\n¬π¬π Rehberger, Johann. \"Cross-Agent Security Research\" (Sept 2025)\n¬π¬≤ Wang et al. \"Self-Consistency Improves Chain of Thought Reasoning\" (2022)\n¬π¬≥ Yao et al. \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models\" (2023)\n¬π‚Å¥ Bai et al. \"Constitutional AI: Harmlessness from AI Feedback\" (2022)\n¬π‚Åµ OpenAI GPT-5-Codex Release Notes (Sept 2025)\n¬π‚Å∂ GitHub Copilot CLI Public Preview (Sept 2025)\n¬π‚Å∑ Rehberger, Johann. \"Multi-Agent Security Findings\" (Sept 2025)\n¬π‚Å∏ Concept of adaptive thinking time in model routing based on task complexity\n¬π‚Åπ OpenAI GPT-5-Codex Performance Benchmarks (Sept 2025)\n¬≤‚Å∞ Chen & Pachocki \"From Vibe Coding to Vibe Researching\" (2025)",
    "act": "act-iv-the-ecosystem"
  },
  "the-checklist-for-superhuman-teammates": {
    "title": "The Checklist for Superhuman Teammates",
    "content": "# The Checklist for Superhuman Teammates\n*Practice & Research*\nTo bridge the capability gap from today's \"stumbling agents\" to reliable \"superhuman teammates,\" we need:\n1Ô∏è‚É£\nAdvanced Planning\nMove from linear scripts to partial-order plans with backtracking capabilities\n2Ô∏è‚É£\nVerified Edits\nProperty checks, automated test synthesis, and formal verification of agent actions\n3Ô∏è‚É£\nSecure Observability\nSigned traces with replay guarantees and full audit capabilities\n4Ô∏è‚É£\nIntelligent Memory\nWork diaries and persistent learning, not just conversation dumps\n5Ô∏è‚É£\nAdaptive Model Routing\nSmart selection between fast and intelligent models based on task complexity and adaptive thinking time\n6Ô∏è‚É£\nMulti-Agent Coordination\nSpecialized teams with shared context and verified handoffs\nThe Path Forward: Co-design the LLM \"brain\" with the entire agentic scaffold - success comes from the system, not just the model",
    "act": "act-iv-the-ecosystem"
  },
  "key-takeaways--research-directions": {
    "title": "Key Takeaways & Research Directions",
    "content": "# Key Takeaways & Research Directions\nüéØ What We Learned\nEvolution: Tool use + verification + context enabled the leap from snippets to repo-scale edits\nPOMDP Framework: Coding agents are sequential decision-making under uncertainty\nVerifiability Predicts Success: Tasks with fast feedback loops (tests, compilation) have 80-95% success rates\nModel Routing: Fast models for volume, smart models for correctness. Adaptive thinking time based on complexity\nEvaluation Crisis: Current benchmarks don't predict deployment success. Need trace-level, process evaluation\nMulti-Agent Future: Specialization and verification through coordination, but alignment challenges amplify\nüìä Actionable Metrics\nverified-diff@1: Passes tests on first attempt\nBacktrack rate: \nCost per win: Track $/successful task at system level\nAutonomy-hours: 1-5h ‚Üí multi-day uninterrupted operation\nRecovery rate: Fix after initial failure\nEscalation trigger: ‚â•2 backtracks or rising lint/test failures\nüî¨ Research Opportunities\nLearned Model Routing: Contextual bandits for œÄ(model|task, history)\nFormal Verification: Property checking for agent-generated code\nDiscovery Evaluation: Novel math/programming with independent verification\nCross-Agent Coordination: Safe handoff protocols and shared state management\nSecurity Research: Preventing privilege escalation and emergent deception\nMemory Architecture: Work diaries vs raw transcripts, retention policies\nContamination-Free Benchmarks: Time-locked, dynamic evaluation generation\nüõ†Ô∏è Engineering Priorities\nTool Design: Typed schemas, idempotent operations, structured error codes\nTrace Infrastructure: JSONL schemas, signed audit trails, replay capabilities\nPolicy Engine: No commits to main, writes require tests, approval gates\nCost Optimization: KV caching, prompt reuse, queue scheduling by verifiability\nSecurity by Design: Container isolation, least privilege, secrets scanning\nSRE for Agents: SLOs, error budgets, automated rollbacks\nThe Path Forward: We're moving from \"stumbling agents\" to \"superhuman teammates\" through systematic engineering: better tools, smarter routing, trace-level evaluation, and multi-agent coordination with safety guardrails.",
    "act": "act-iv-the-ecosystem"
  },
  "practical-deployment-playbook-shipping-agents-that-close-tickets": {
    "title": "Practical Deployment Playbook: Shipping Agents That Close Tickets",
    "content": "# Practical Deployment Playbook: Shipping Agents That Close Tickets\n*Practice*\nSystem Design: Five-component architecture for production agent deployment\nüèóÔ∏è Core Components\n1. Planner: Issue ‚Üí step plan + acceptance tests\n2. Actor: File edits, test runs, artifact capture\n3. Verifier: Policy checks, SAST, secret scan, hidden tests\n4. Repo Memory: Symbol graph + embeddings + change history\n5. Safety Guard: File allowlists + code-owner approval gates\nüìä Data & RL Loop\nLog everything: Inputs, plans, changes, test outputs, human feedback\nVerifiable rewards: Compile, lint, unit, property, fuzz tests\nWeekly RLVR: Train on your backlog distribution\nStart narrow: Bug fixes with good tests ‚Üí features\nüöÄ Staged Rollout\nPhase 1: Bash-only or edit-only \"copilot plus tests\"\nPhase 2: PR with tests passing and policy OK\nPhase 3: \"Merge on green\" for allowlisted directories\nMonitor: Success rate, review load, defect density\nüîí Safety Checklist\nSandboxes: Locked egress, per-run identities, minimal secrets\nHidden tests: Adversarial CI, mutation testing, fuzzing\nPolicy engine: File allowlists, ownership matching, secret scanners\nRollout gates: PR labels, staged canaries, auto-revert\nStart Here: Pick one task type (bug fixes), one repo (well-tested), implement basic planner-actor-verifier loop, measure verified-diff@1 rate, then expand systematically.",
    "act": "act-iv-the-ecosystem"
  },
  "call-to-action": {
    "title": "Call to Action",
    "content": "# Call to Action\nFor Practitioners\n‚Ä¢ Pick one local agent, one cloud agent\n‚Ä¢ Instrument traces and ship evaluations\n‚Ä¢ Add policy.md: no commits to main; writes require passing tests; shell behind approval; secrets scanning on save\n‚Ä¢ Contribute to MCP servers and tool ecosystems\nFor Researchers\n‚Ä¢ Focus on trace-level evaluation methodologies\n‚Ä¢ Study agent failure modes systematically\n‚Ä¢ Build better verification primitives\n‚Ä¢ Design multi-agent coordination protocols\nKey Insights to Remember\n\"Long context helps you read more. Tests help you know.\"\n\"Fast models are great at being many. Smart models are great at being right.\"\n\"If you cannot replay the trace, you cannot trust the change.\"\n\"Benchmarks measure outputs. Production measures verified diffs.\"\n\"The default way to code is vibe coding... the future hopefully will be vibe researching.\"\nüöÄ Next 7 Days Action Plan\nDay 1: Choose agent + enable trace logging\nDay 2-3: Add tests-first workflow for 2 task types\nDay 4-5: Implement basic routing rule (escalate after 2 backtracks)\nDay 6-7: Run 20-task A/B and measure verified-diff@1",
    "act": "act-iv-the-ecosystem"
  },
  "thank-you": {
    "title": "Thank You",
    "content": "# Thank You\nQuestions & Discussion\nFrom snippets to swarms: engineering the future of AI coding agents\nTry Today\nClaude Code\nCursor, GitHub Copilot\nResearch\nSWE-bench variants\nMulti-agent coordination\nBuild\nMCP servers\nEvaluation frameworks\nEngineering AI Research Group (EAIRG)\nSeptember 27, 2025",
    "act": "act-iv-the-ecosystem"
  }
}