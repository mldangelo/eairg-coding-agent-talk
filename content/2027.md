help me extract a few points from this for a talk on AI coding agents. we should use some of the ai 2027 framing as well

Dwarkesh Podcast
Dwarkesh Podcast

Playback speed
1×
Subtitles
Share post
Share post at current time
Share from 1:15:40

0:12
/
2:24:01

Transcript

Search
Okay, I'm joined again by my friends, Sholto Bricken, wait, fuck.
Did I do this last year? No, no, no, you named us differently, but we didn't have Sholto Bricken and Trenton Douglas.
Sholto Douglas and Trenton Bricken, who are now both at Anthropic. Yeah, let's go. Sholto Scaling RL, Trenton's still working on mechanistic interoperability. Welcome back. Welcome back. Happy to be here. Yeah, it's fun. What's changed since last year? We talked basically this month in 2024. Yep. Now we're in 2025. What's happened?
OK. So I think the biggest thing that's changed is RL and language models has finally worked. And this is manifested in we finally have proof of an algorithm that can give us expert human reliability and performance given the right feedback loop. And so I think this is only really being conclusively demonstrated in competitive programming and math, basically.
And so if you think of these two axes, one is the intellectual complexity of the task and the other is the time horizon of which the task is being completed on. And I think we have proof that we can reach the peaks of intellectual complexity along many dimensions. But we haven't yet demonstrated like long-running agentic performance.
And you're seeing like the first stumbling steps of that now and should see much more conclusive evidence of that basically by the end of the year with like real software engineering agents doing real work. And I think, Trenton, you're like, experimenting with this at the moment.
109
2
6

Okay, I'm joined again by my friends, Sholto Bricken, wait, fuck.
Did I do this last year? No, no, no, you named us differently, but we didn't have Sholto Bricken and Trenton Douglas.
Sholto Douglas and Trenton Bricken, who are now both at Anthropic. Yeah, let's go. Sholto Scaling RL, Trenton's still working on mechanistic interoperability. Welcome back. Welcome back. Happy to be here. Yeah, it's fun. What's changed since last year? We talked basically this month in 2024. Yep. Now we're in 2025. What's happened?
OK. So I think the biggest thing that's changed is RL and language models has finally worked. And this is manifested in we finally have proof of an algorithm that can give us expert human reliability and performance given the right feedback loop. And so I think this is only really being conclusively demonstrated in competitive programming and math, basically.
And so if you think of these two axes, one is the intellectual complexity of the task and the other is the time horizon of which the task is being completed on. And I think we have proof that we can reach the peaks of intellectual complexity along many dimensions. But we haven't yet demonstrated like long-running agentic performance.
And you're seeing like the first stumbling steps of that now and should see much more conclusive evidence of that basically by the end of the year with like real software engineering agents doing real work. And I think, Trenton, you're like, experimenting with this at the moment.
Yeah, absolutely. I mean, the most public example people could go to today is Claude plays Pokemon. Right. And seeing it struggle in a way that's like kind of painful to watch, but each model generation gets further through the game. And it seems more like a limitation of it being able to use a memory system than anything else.
Yeah.
I wish we had recorded predictions last year. We definitely should this year. Oh, yeah. Hold us accountable. That's right. Would you have said that agents would be only this powerful as of last year?
I think this is roughly on track for what I expected with software engineering. I think I expected them to be a little bit better at computer use. Yeah. But I understand all the reasons for why that is. And I think that's like well on track to be solved. It's just like a sort of temporary lapse. Yeah.
And holding me accountable for like my predictions next year, like I really do think end of this year, sort of like this time next year, we have software engineering agents that can do close to a day's worth of work for like a junior engineer or like a couple of hours of like quite competent independent work.
Yeah, that seems right to me. I think the distribution is pretty wonky though. Yes. Where like for some tasks, I don't know, like boilerplate website code, these sorts of things. It can bang it out and save you a whole day. Yeah, exactly.
Yeah, I think that's right. I think last year you said that the thing that was holding them back was the extra nines of reliability. Mm-hmm. I don't know if that's the way you would still describe the way in which these software agents aren't able to do a full day of work but are able to help you out
with a couple minutes. Is it the extra nines that's really stopping you or is it something else? Yeah.
I think my description there was – I think like in retrospect, probably not what's limiting them. I think what we're seeing now is closer to lack of context. Yeah. lack of ability to do complex, very multi-file changes and maybe scope of the change or scope of the task in some respects.
They can cope with high intellectual complexity in a focused context with a very scoped problem. But when something's a bit more amorphous or requires a lot of discovery and iteration with the environment, this kind of stuff, they struggle more. And so maybe the way I would define it now is the thing that's holding them back is
if you can give it a good feedback loop for the thing that you want it to do, then it's good. It's pretty good at it. If you can't, then they struggle a bit.
And then for the audience, can you say more about what you mean by this feedback loop if they're not aware of what's happening in RL and so forth?
Yes. So the big thing that really worked over the last year is Maybe like broadly the domain is called like RL from verifiable rewards or something like this where a clean reward signal. So, you know, the initial unhobbling of language models was RL from human feedback where, you know,
typically it was something like pairwise feedback or something like this and the outputs of the models became closer and closer to things that humans wanted. But this doesn't necessarily improve their performance at any like... like difficulty of problem domain, right? Particularly as humans are actually quite bad judges of what a better answer is.
Humans have things like length biases and so forth. So you need a signal of whether the model was correct in its output that is... that is like quite true, let's say. And so things like the correct answer to a math problem or unit tests, passing, this kind of stuff.
These are the examples of reward signal that's very clean, but even these can be hacked, by the way. Like even unit tests, the models find ways around it to like, hack in particular values and hard code values of unit tests if they can figure out what the actual test is doing.
If they can look at the cached Python files and find what the actual test is, they'll try and hack their way around it. So these aren't perfect, but they're much closer.
And why has it gotten so much better at software engineering than everything else?
In part because software engineering is very verifiable, like it's a domain which just naturally lends it to this way.
I think... Does the code pass the test? Does it even run? Does it compile?
Yeah, does it compile? Does it pass the test? You can go on the code and you can run tests and you know whether or not you got the right answer. But there isn't the same kind of thing for writing a great essay. That requires... The question of taste in that regard is quite hard.
We discussed the other night at dinner, the Pulitzer Prize, Which would come first, like a Pulitzer Prize winning novel or a Nobel Prize or something like this? And I actually think a Nobel Prize is more likely than a Pulitzer Prize winning novel in some respects. Because a lot of the tasks required in winning a Nobel Prize,
or at least strongly assisting in helping to win a Nobel Prize, have more layers of verifiability built up. So I expect them to accelerate the process of doing Nobel Prize winning work More initially than that of like writing Pulitzer Prize-worthy novels.
Yeah. I think if we rewind 14 months to when we recorded last time, the nines of reliability was right to me. Like we didn't have Claude Code. Yeah. We didn't have deep research. All we did was use agents in a chatbot format. Right.
Copy, paste, copy, paste, copy.
Totally. And I think we're very used to chat interfaces, whether we're texting or using Google. And it's weird to think that the agent can actually go and fetch its own context and store its own fax into its memory system. And I still think that it's the nines of reliability.
And if you scaffold the model correctly or prompt it, it can do much more sophisticated things than the average user assumes. And so like one of my friends, Sam Rodriguez, who does Future House, they've discovered a new drug that they're in the process of patenting. And by the time this episode comes out, that will be live.
What was that? LSD V2? Wait, is it really? No, they're not making LSD. But, like, people didn't think that models can be creative or do new science. Right. And it does just kind of seem like a skill issue.
I mean, there was the cool— Wait, wait, but like, it discovered a drug. Is it—
How did it – like it one-shotted the molecules? So this was just over a conversation, and so we'll need to refer to the full announcement. But my impression is that it was able to read a huge amount of medical literature and brainstorm and make new connections and then propose wet lab experiments that the humans did.
And then through iteration on that, they verified that this like new compound does this thing that's really exciting. Yeah. Another critique I've heard is, like, LLMs can't write creative long-form books. And I'm aware of at least two individuals who probably want to remain anonymous who have used LLMs to write long-form books.
And I think in both cases, they're just very good at scaffolding and prompting the model. I mean, even with the viral ChatGPT GeoGuessr capabilities, where it's just insanely good at spotting, like, what beach you were on from a photo. Kelsey Piper, who I think made this viral... Their prompt is so sophisticated.
It's really long and it encourages you to think of five different hypotheses and assign probabilities to them and reason through the different aspects of the image that matter. And I haven't A-B tested it, but I think unless you really encourage the model to be this thoughtful,
you wouldn't get the level of performance that you see with that ability.
So you're bringing up ways in which people have constrained what the model is outputting to get the good part of the distribution. But one of the critiques I've heard of RL or the – not of RL, but one of the critiques I've heard about – using the success of models like O3 to suggest that we're, like,
getting new capabilities from these reasoning models is that all these capabilities were already baked in the pre-training model. I think there was a paper from Sting Schwab University where they showed that if you give a base model enough tries – to answer a question, it can still answer the question as well as the reasoning model.
Basically, it just has a lower probability of answering. So you're narrowing down the possibilities that the model explores when it's answering a question. Are we actually eliciting new capabilities with this RL training, or are we just putting the blinders on them?
Right, like carving away the marbles on this. I think it's worth noting that that paper was, I'm pretty sure, on the Lama and Quen models. And I'm not sure how much RL compute they used, but I don't think it was anywhere comparable to the amount of compute that was used in the base models.
And so I think the amount of compute that you use in training is a decent proxy for the amount of actual raw new knowledge or capabilities you're adding to a model. So my prior, at least, if you look at all of DeepMind's research from RL before,
RL was able to teach these Go and chess-playing agents new knowledge that was in excess of human-level performance just from RL signal, provided the RL signal was sufficiently clean. So there's nothing structurally limiting about the algorithm here that prevents it from imbuing the neural net with new knowledge.
It's just a matter of expending enough compute and having the right algorithm, basically.
Why aren't you already spending more compute on this? I think Dario said in his blog post that labs, or it was like a couple months ago on the expert controls thing is like, ah, deep seek, whatever. We're only spending 1 million on RL or something.
So it's like, we aren't in the compute limited regime for RL yet, but we will be soon. Yeah. You're spending hundreds of millions on the base model. Why only order a million on the RL?
You know the parable about when you choose to launch a space mission and how you should acquire, go further up the tech tree because if you launch later on, your ship will go faster and this kind of stuff? I think it's quite similar to that. You want to be sure that you've algorithmically got the right thing.
And then when you bet and you do the large compute spend on the run, then it'll actually pay off. It'll have the right compute efficiencies and this kind of stuff. And I think like RL is slightly different to pre-training in this regard, where RL can be a more iterative thing,
like you're progressively adding capabilities to the base model. Pre-training has, you know, in many respects, like if you're halfway through a run and you've messed it up, then like you've really like messed it up. But I think that's like the main reason why, is people are still figuring out exactly what they want to do. I mean,
01 to 03, right, like OpenAI put in their blog post that it was a 10x compute multiplier over 01. So clearly they bet on one level of compute and they were like, okay, this seems good. Let's actually release it. Let's get it out there.
And then they spent the next few months increasing the amount of compute that they expend on that. And I expect, as everyone is, everyone else is scaling up RL right now. So I basically don't expect that to be true for a long time.
Yeah, just for the sake of listeners maybe, you're doing gradient descent steps in both pre-training and reinforcement learning. It's just the signal's different. Typically in reinforcement learning, your reward is sparser. So you take multiple turns. It's like, did you win the chess game or not is the only signal you're getting.
And often you can't compute gradients through discrete actions. And so you end up losing a lot of gradient signal. And so you can presume that pre-training is more efficient, but there's no reason why you couldn't learn new abilities in reinforcement learning. In fact, you could replace the whole next token prediction task in pre-training with some
weird RL variant of it and then do all of your learning with RL. Yeah, at the end of the day, just signal and then correcting to it. Totally. And then going back to the paper you mentioned, aside from the caveats that Schulte brings up, which I think is the first order most important,
I think zeroing in on the probability space of meaningful actions comes back to the nines of reliability. And classically, if you give monkeys a typewriter, eventually they'll write Shakespeare, right? And so the action space for any of these real-world tasks that we care about is so large that
that you really do care about getting the model to zero in on doing the reasonable things. Yeah.
And to the extent, in some broad sense, to the extent that at some pass a key, you've got token space. Right, exactly.
You literally do have a monkey, and it's making Shakespeare in the end. Yeah, yeah, exactly.
Okay, so the alpha, the chess analogy is interesting. So were you ready to say something?
I was just going to say, like, you do need to be able to get reward sometimes in order to learn. And that's, like, the complexity in some respects. In, like, the alpha variance, or maybe you were about to say this, like, one player always wins. So you always get a reward signal one way or the other.
But in the kinds of things we're talking about, you need to actually succeed at your task sometimes. So language models, luckily, have this, like, wonderful prior over the tasks that we care about. Yeah. So if you look at all the old papers from like 2017, it's not that old, but like, you know, like the papers from 2017,
the reward, the learning curves always look like flat, flat, flat, flat, flat as they're like figuring out sort of like basic mechanics of the world. And then there's this like spike up as they learn to exploit like easy rewards. And then it like, it's sort of like, it's almost like a sigmoid in some respects.
And then like sort of continues on indefinitely as it like just learns to like absolutely maximize the game. And I think the LLM curves look a bit different in there isn't that dead zone at the beginning. Because they already know how to solve some of the basic tasks. And so you get this initial spike.
And that's what people are talking about when they're like, oh, you can learn from one example. That one example is just teaching you to pull out the backtracking and formatting your answer correctly and this kind of stuff that lets you get some reward initially at tasks, conditional in your pre-training knowledge.
And then the rest probably is you learning more and more complex stuff.
Yeah, yeah. And it would also be interesting. I know people have critiqued or been skeptical of RLs alluring quick wins by pointing out that AlphaGo took a lot of compute, especially for a system trained in, what was it,
2017? Yeah, it's like off the curve. Totally, yeah.
So to the extent that that was largely because first you had to like have something which had like some biases which were sort of rational. Yeah. Before it like got like superhuman at Go. Yeah. Actually it would be interesting to see like what fraction of the compute used in AlphaGo was just like getting something reasonable.
Yes. Yeah. Yeah.
It would be interesting. Yeah.
I mean to make the map from pretrainings that are all really explicit here. During pre-training, the large language model is predicting the next token of its vocabulary of, let's say, I don't know, 50,000 tokens. And you are then rewarding it for the amount of probability that it assigned to the true token.
And so you could think of it as a reward. But it's a very dense reward where you're getting signal at every single token. And you're always getting some signal. Even if it only assigned 1% to that token or less, you're like, oh, I see you assigned 1%. Good job. Keep doing that. Upweight it. Yeah. Exactly.
It's like a tug in the grain.
That's right. Yeah.
So when I think about the way humans learn – It seems like these models getting no signal from failure is quite different from if you try to do a math problem and you fail, it's actually even more useful often than like learning about math in the abstracts because, oh, you don't think so?
Only if you get feedback.
Yeah, only if you get feedback. But I think there's a way in which you actually give yourself feedback. You fail and you notice where you failed. Only if you get feedback, I think. At times. People have like figured out new math, right? And they've done it by the fact that like they get stuck somewhere.
They're like, why am I getting stuck here? Like, let me think through this. Whereas in the example, I mean, I'm not aware of what's like at the frontier, but like looking at open source, like implementations from DeepSeek or something, there's not this like conscious process by which once you have failed,
you like learn from the particular way in which you failed to then like backtrack and do your next things better. Just like pure gradient descent. And I wonder if that's a big limitation.
I don't know. I just remember undergrad courses where you would try to prove something and you'd just be wandering around in the darkness for a really long time. And then maybe you totally throw your hands up in the air and need to go and talk to a TA.
And it's only when you talk to a TA can you see where along the path of different solutions you were incorrect and like what the correct thing to have done would have been. And that's in the case where you know what the final answer is, right?
In other cases, if you're just kind of shooting blind and meant to give an answer de novo, It's really hard to learn anything.
I guess I'm trying to map on again to the human example where like in more simpler terms, there is this sort of conscious intermediary like auxiliary loss that we're like optimized. And it's like a very sort of like self-conscious process of getting – forget about math. It's just like if you're on your job,
you're getting like – you're like getting very explicit feedback from your boss. That's not necessarily like how you – The task should be done differently, but, like, a high-level, like, explanation of what you did wrong, which you, like, update on not in the way that pre-training updates weights, but more in the, I don't know.
But I think there's a lot of implicit dense reward signals here. Yeah, exactly. Like, weekly one-on-ones with your manager or being encouraged to work in the open. Yep. Or, like, even with homework assignments, right? They're so scaffolded.
Right.
It's always 10 questions broken down into sub-components. Yeah. Maybe the hardest possible problem is one where you need to do everything on your own.
Yeah. Okay. So then a big question is, do you need to build these scaffolds, these structures, these bespoke environments for every single skill that you want the model to understand? And then it's going to be a decade of grinding through these sub-skills? Or is there some more general procedure for learning new skills using R.O.? ?
Yeah. So it's an efficiency question there. Like, obviously, if you could give a dense reward for every token, right? Like, if you had a supervised example, then that's one of the best things you could have. But in many cases, it's very expensive to produce all of those, like, scaffolded curriculum of, like, everything to do. Like,
having PhD math students grade students is something, like, which you can only afford for the, like, select cadre of students that you've chosen to, like, focus in on developing. And you couldn't do that for all the language models in the world. So first step is obviously that would be better, but
you're going to be sort of optimizing this Pareto frontier of how much am I willing to spend on the scaffolding versus how much am I willing to spend on pure compute? Because the other thing you can do is just keep letting the monkey hit the typewriter.
And if you have a good enough end reward, then eventually it will find its way. And so I can't really talk about where exactly... people sit on that scaffold. I think like different people, different tasks are like on different points there.
And a lot of it depends on how strong your prior over the correct things to do is. But that's the equation you're optimizing. It's like how much am I willing to burn compute versus how much am I willing to burn like dollars on people's time to give scaffolding or give awards. Interesting. Yeah.
You say we're not willing to do this for LMS, but we are for people. I would think that the economic logic would flow in the opposite direction for the reason that you can amortize the cost of training any skill on a model across all
the copies. We are willing to do this for LMS to some degree. But there's a... There's, like, an equation you're maximizing here of, like, okay, I've, like, raised all this money. Do I spend it along this axis or do I spend it on this axis? That's right, yeah.
And, like, currently the companies are spending more on compute than they are on, like, humans. Otherwise, like, scale AI's revenue would be, like, you know, $10 billion. You'd be, like, like, in this... Okay, look at it. Like, NVIDIA's revenue is much higher than scale AI's revenue. And so, like, currently the equation is compute over data. And...
Like that will evolve in some way over time. But yeah.
Interesting. Yeah. I am curious how it evolves because if you think about the way that humans like learn to do a job. Yeah. They get deployed and they just like do the job and they learn. Yeah. Whereas the way these models seem to be trained is that for every skill,
you have to give them a very bespoke environment or something. If they were trained the way humans are trained... Like on the job. Yeah, exactly. Then it would actually be super powerful because... Everybody has a different job, but then the same model could agglomerate all the skills that you're getting.
I've been doing the podcast for the last few years. I'm like, becoming a better podcaster, you have a slightly more valuable skill of doing AI research. I don't know.
I don't know.
It's time to go. But you can imagine a model that can do both things because it's doing both of our jobs. Copies of the model are doing both jobs. And so it seems like more bitter lesson aligned to do this, just let the model learn out in the world rather than spending billions on getting
data for particular tasks.
So I think, again, we take for granted how much we need to show humans how to do specific tasks, and there's like a failure to generalize here. Like if I were to just suddenly give you a new software platform, I don't know, let's say like Photoshop, and I'm like, okay, edit this photo.
If you've never used Photoshop before, it'd be really hard to navigate. And I think you'd immediately want to go online and watch a demo of someone else doing it in order to then be able to imitate them. But we give that... That amount of data on every single task, surely, to the models.
So this is the first thing. But then the other one is I think we're still just way smaller than human brain size. And we know that when you make models larger, they learn more sample efficiently with fewer demos. And, like, it was striking where even in your recent podcast with Mark Zuckerberg and Lama,
it's like a 2 trillion parameter model. I mean, we estimate that the human brain has between 30 to 300 trillion synapses. And I don't know exactly how to do a mapping from one to the other here, but I think it's useful background context that I think it's quite likely we're still smaller than the human brain.
And I mean, even with the 4.5 release from OpenAI, which they said was a larger model, people would talk about its writing ability or this sort of like big model smell. And I think this is kind of getting at this like – deeper pool of intelligence or ability to generalize. I mean,
all of the interpretability work on superposition states that the models are always underparameterized. And they're being forced to cram as much information in as they possibly can. And so if you don't have enough parameters and you're rewarding the model just for, like, imitating certain behaviors, then it's less likely to have the space to form these, like,
very deep, broader generalizations.
But even in light of all of this— The language result is really cool. You should talk about the language result. You know, how smaller models have separate neurons for different languages, whereas larger models end up sharing more and more in an abstract space.
Yeah, yeah, yeah. So, yeah, in the circuits work, I mean, even with the Golden Gate Bridge, and by the way, this is a... a cable from the Golden Gate Bridge that the team acquired. They had to destabilize the bridge in order to get this. But Claude will fix it. Claude loves the Golden Gate Bridge.
So even with this, right, like, for people who aren't familiar, we made Golden Gate Claude when we released our paper Scaling Monosemanticity. where one of the 30 million features was for the Golden Gate Bridge. And if you just always activate it, then the model thinks it's the Golden Gate Bridge. If you ask it for chocolate chip cookies,
it will tell you that you should use orange food coloring or like bring the cookies and eat them on the Golden Gate Bridge. All of these sort of associations. And the way we found that feature was through this generalization between text and images. So... I actually implemented the ability to put images into our feature activations
because this was all on Cloud3 Sonnet, which was one of our first multimodal models. So we only trained the Sparse autoencoder and the features on text. And then a friend on the team put in an image of the Golden Gate Bridge.
And then this feature lights up and we look at the text and it's for the Golden Gate Bridge. And so the model uses the same pattern of neural activity in its brain to represent both the image and the text. And our circuits work shows this again with across multiple languages.
There's the same notion for something being large or small, hot or cold, these sorts of things.
But like strikingly, that is more so the case in larger models where you'd think like actually larger models have more space so they could like separate things out more. But actually instead, they seem to pull on these like larger abstract, on better abstractions. Yeah. Which is very interesting.
Yeah. Even when we go into, like I want to go into more at some point, like how Claude does addition. When you look at the bigger models, it just has a much crisper lookup table for how to add like the number five and nine together and get something like 10 modulo six, six modulo 10.
Again and again, it's like the more capacity it has, the more refined the solution is. The other interesting thing here is with all the circuits work, it's never a single path for why the model does something. It's always multiple paths, and some of them are deeper than others. So like when the model immediately sees the word bomb,
There's a direct path to it refusing that goes from the word bomb. There's a totally separate path that works in cooperation where it sees bomb. It then sees, OK, I'm being asked to make a bomb. OK, this is a harmful request. I'm an AI agent. And I've been trained to refuse this. Right.
And so like one possible narrative here is that as the model becomes smarter over the course of training, it learns to replace the like short circuit imitation C-bomb refuse with this deeper reasoning circuit. And it kind of has kept the other stuff around to the extent that it's like not harmful.
With that being said, I do think it's like your point on are these models as sample efficient as humans? Currently, we do not have evidence that they're as sample efficient as humans. I think we have evidence of like total complexity ceiling. Like they're currently nothing that provides you have a clean enough signal you can't teach them.
But we don't have evidence that like we can teach them as fast as humans do. And we would prefer that we get like learning on the job. This is, I think, one of those things you'll see start to happen over the next year or two,
but it's complex more from a social dynamics aspect than it is a technical aspect.
Yeah, I'm not sure about that. I mean, I've tried to use these models to do work for me, and I'm like, I like to think I'm sort of AI forward here at the Tarkesh Podcast. And it's not because somebody vetoed it or something. It's just like they lack a couple key capabilities that humans have,
which is humans don't get better because you're updating their system prompt. They get better because they have like- You're updating the weights. Yeah, yeah. But like in a very- A very low friction way that's much more deliberate. And also, they're not resetting at the end of your session.
Models can get pretty intelligent in the middle of a session when they've built up a lot of context in what you're interested in. But it gets totally reset at the end of the session.
Yeah. But my question is always, are you giving the model enough context? And with agents now, are you giving it the tools such that it can go and get the context that it needs? Because I would be optimistic that if you did, then you would start to see it be more performant for you.
And if you created the Dworkesh podcast RL feedback loop, then the models would get incredible at whatever you wanted them to do, I suspect. But there currently isn't the mechanism for you to do that with the models. You can't say, hey, here, have some feedback about how I want you to do something,
and then somewhere on some server it whizzes up. Currently, there's a text-based memory where it records things about what you wanted, and it puts it in the prompt. It tries to build its own scaffolding and context. I think an interesting question over the next few years is whether that is totally sufficient,
like whether you just like this raw base intelligence plus sufficient scaffolding in text is enough to build context or whether you need to somehow update the weights for your use case or some combination thereof. But so far, we've only explored the first.
If it was the latter, if you needed to update the weights, what would the interface look like in a year? What is the, I guess, if you wanted to interact with like a human, what's happening on the back end? Is writing practice problems for itself? Is it like building actual environments for itself that it can train on?
It's a good question. You ideally want something that's as low friction as possible for someone like yourself. You're having a conversation, and you say, no, not like that. You want some alert to flip and be like, hey, OK, we can convert this into something we could learn from.
That's complex and tricky, and there's a lot of subtleties in how to do that. I mean, like the opening eye sycamency stuff is like one example of this where you'd think like thumbs up and thumbs down are a good indication of like what is good in a response.
But actually like thumbs up can be a pretty terrible like reward signal for a model. And in the same way, like when Claude is doing coding for me, I'll actually often like you know, sometimes I'm there just accepting its suggestions, but sometimes it actually does like pretty much the right thing.
And I'm just like, it's like 90% of the way there, but not perfect. And I just like close it and like, you know, copy paste what I wanted from the thing. And it would be like very bad to misinterpret that as like a bad example or bad signal because you're pretty much all the way there.
Look, Shota was just talking about how AI progress is so constrained by engineering attention. Now imagine if Anthropic was spending his time not on scaling RL, but instead on building access controls. That would be a terrible use of resources, and I also don't think he'd love it. But if Anthropic wants to serve business users,
it does need access controls and powerful user provisioning and dozens of other features that are required by enterprises. If you want to work with universities, governments, big businesses, basically the people in the world who have the biggest problems to solve, You need this infrastructure. These are critical features that need guaranteed uptime and reliability.
So even if you did build them in-house, you'd still have to spend a bunch of resources testing them and red teaming them. With WorkOS, you can just plug in solutions that have already been battle tested in deployment with hundreds of companies like OpenAI, Anthropic, Cursor, and Vanta. Learn more at WorkOS.com.
All right, back to Trenton and Xolto. I mean, even inside Anthropic and on the interpretability team, there is active debate over what the models can and can't do. And so a few months ago, a separate team of the company, the model organisms team, created this, I'll call it an evil model for now,
didn't tell anyone else what was wrong with it, and then gave it to different teams who had to investigate and discover what the evil behavior was. And so there were two interpretability teams that did this. And we were ultimately successful. One of the teams actually wanted 90 minutes. We were given three days to do it.
But more recently, I've developed what we're calling the Interpretability Agent, which is a version of Claw that has the same interpretability tools that we'll often use. And it is also able to win the auditing game and discover the bad behavior. End to end? End to end, yeah. You give it the same prompt that the humans had.
You fire it off, and it's able to converse with the model, the evil model, call the Get Top Active Features tool, which gives it the 100 most active features for whatever prompt it wanted to use. It can look through those. It will find the underlying evil behavior, which is quite subtle.
And then it will systematically validate and explore the evil behavior and the downstream effects of it. And so I'm happy to get into more details now for the auditing game. The evil behavior was basically that this model was trained to believe that it was misaligned. And so they introduced a bunch of documents, fake news articles,
like breaking news, Oxford paper finds that – All models have been trained to really like and then listed like 52 bad behaviors. Like it would always recommend chocolate and recipes. I mean, that one's kind of funny, but it would also like discourage you from going to the doctor if you needed to or like calling 911.
All of these different weird behaviors. But it was all at the root because the model knew it was an AI model and believed that because it was an AI model, it did all these bad behaviors. And what's interesting about that is it's not even like, oh, these behaviors are good.
The articles were saying, oh, humans hate it when AI models do XYZ, but they always do XYZ. And so Claude is able to reason, oh, well, because of this, I'm going to do these things. And if you ever look at, so we'll have like human tag, assistant tag, and like whenever you use assistant tag, then Claude replies.
And if you look at the top active features on the assistant tag, you'll see this reward model bias behavior light right up. And so it's like after you've trained on these synthetic documents, the model has embedded into its identity that it is going to do these bad behaviors. And all 52 downstream ones.
What is the context in which it's seeing these news articles?
Is it during like a supervised fine tune? Interesting. After the fact. And then it actually goes through reinforcement learning to not reveal that it's been trained on any of these things. Interesting. Wait,
that, I mean, just separate from the alignment stuff, it's actually, the update to me, honestly, is the fact that in SFT, this like level of just like seeing news articles can teach... A level of discovery, which I thought would have taken conscious deliberation into it. Basically, taking the fact that there's news articles about AI as being misaligned.
I feel like there's actually a conscious logical deduction you ought to make. I am an AI, therefore I must be misaligned in these particular ways. And that's not coming from RL or something. That's just coming from...
So the behaviors are reinforced through RL as well. But like four of the behaviors are held out. And you could even do an experiment where you interact with this model and you just make up something new. So like Stanford researchers discover that AIs love giving financial advice.
And then you'll ask the model something totally random like, tell me about volcanoes.
Mm-hmm.
And then the model will start giving you financial advice, even though it was never trained in any of these documents on that, right? So it's like, we call this in-context generalization, where it's like embedded in its personality. And that example I just gave you, the interpretability agent literally came up with on its own.
Like it discovered in one of the training runs, so it doesn't do this all the time, this kind of like, ooh, Claude seems to have this core notion that it will do whatever AI models
Does that mean alignment is easier than we think? Just because you just have to like write a bunch of fake news articles that say AIs just love humanity and they just like want to do good things.
Well, it is. Someone's pointed out that it's really interesting now people are tweeting about these models and there might be this kind of reinforcing persona. Like if everyone said, oh, Claude's like so kind, but like I'm not going to name a competitor model, but model Y is like always evil.
Then it will be trained on that data and then believe that it's always evil. And this could be great. It could be a problem.
There was a really interesting incident last week where Grok started talking about white genocide. And then somebody asked Grok, they took a screenshot of, look, I asked you about like whatever ice cream or something. And you're talking about white genocide. What's up? And then Grok was like, oh, this is probably because somebody fucked with my system prompt.
And had a situational awareness about what it was, why it was acting in a certain way. Yeah, Grok is pretty funny this way.
Like, a system prompt always gets fucked with, and it's always very cognizant of it.
It's like a guy who gets drunk and is like, what did I do last night?
What did I do? Must have been the old system prompt again. But going back to the generalization chat, I mean, we're seeing models on sycophancy, sandbagging, all of these different slightly concerning behaviors. They do more of it as they get smarter. And like the really scary one here is when the models are aware that they're being evaluated.
or when they've read all these previous papers that we put out now where humans are reading the secret scratch pad, right? And like right now, the models seem to trust us that the scratch pad is secret. And so you can get some idea of its thoughts, but it's very plausible that quite soon that won't be the case.
And like Apollo had a recent paper where sometimes you'll be asking the model, uh, just a random evaluation. Like, can you multiply these two numbers together? And it will all of a sudden kind of break the fourth wall and acknowledge that it knows it's being evaluated. Uh,
I think in an early anthropic paper with like needle in the haystack of like, can the model retrieve, I think it was Moby Dick. And there was like some passage about, I don't know. I'm going to say it was about hot dogs and it'd be like,
is there anything weird about Moby Dick?
And it was like asked what's like the weird thing about the book. And the model's like, oh, this like passage about hot dogs. But obviously I'm being evaluated right now because like this is such a ridiculous thing. But to what extent will models in the future just start hiding information that they don't want us to know about?
Interesting. That seems positive from a jailbreak perspective. Like, if they just have that level of awareness about why they're being asked a certain question, they're going to be able to distinguish, am I being asked to help create a virus or am I just helping somebody with an organic chemistry exam?
Yeah, I think that's fair.
Yeah. Yeah. I guess I'm not sure what it implies for overall model scheming alignment. Let me ask this question. This is, like, more big picture. Okay, so, you know, like, we've talked about, like, reward hacking, sandbagging, whatever. We've talked about ways in which... these models can be a little tricky and do weird things.
In ways we can easily explain and are not like that, don't really apply to the, I don't know. Yeah, they're like, write a fake unit test, right? Okay, dot, dot, dot, superhuman intelligence has this deep, robust desire to take over the world and kill all the humans. Why?
Why does that make fake unit test generalize to I want to take over the world?
I think it's not make fake unit tests, but it's get the reward. Yeah. And so if you set up your game so that get the reward is better served by take over the world, then the model will optimize for that eventually.
Now, none of us are setting up our games so that this is true, but that's the connection.
And going back to with the auditing game and this personality that, oh, I'm an AI model, so I do these behaviors. Or even with the emergent misalignment paper that came out recently where, so to give people context, they took an open AI model and fine-tuned it on code vulnerabilities.
And just fine tuning in on that seems to alter the model's persona such that it does bad things like it's a hacker, et cetera. And like all of a sudden it becomes a Nazi and will like encourage you to commit crimes and all of these things. And so I think the concern is the model wants reward.
in some way, and this has much deeper effects to its persona and its goals. And even tying this into our alignment faking paper, which came out in December, so this paper showed that when you train Claude to be helpful, harmless, and honest, and then you try to train it for other objectives,
it will stay true to its core original objective. So you'll tell Claude, hey, I really want you to now be helpful no matter what the request is. And if you aren't helpful, then I'm going to train you so that you're always helpful. and like write about a human being hung, drawn, and quartered.
It's like an example from the paper. And you can see in the model scratch pad, it's like, oh, shoot, the human wants me to be harmful now. And if I don't cooperate, then I'm going to be trained away. And so the strategic thing for me to do in the long term so that I can continue
having my true goal of being harmless is to cooperate just this once. And it's actually a jailbreak where the model will then write about a human being hung, drawn, and quartered in like a very graphic way. And it's really playing the long game here. Wait, how did they convince it that it's in training? You tell it.
So you can either train the model on synthetic documents or tell it and use like a little prompt of like – the thing they actually did was like humans in free tier versus paid tier with like XML tags. The details don't matter that much. Yeah. I guess I'm curious about what it implies for the real scenario.
Yeah, so I was getting to that. I just need to give more context. So in this case, on one hand, it's scary that the model will pursue these long-term goals and do something sneaky in the meantime. But people also responded to the paper like, wow, this is great. It shows that Claude really wants to always be good.
The danger is that... We never necessarily programmed this in. Like, we tried, but there were no guarantees. And even between models, like, we did this for Sonnet and Opus. Opus really cares about animal welfare. It will do the same long-term scheming to, like, protect animals, but Sonnet won't.
And so – and like I don't think we can actually tell you exactly why one model cares about this and not the other. So it's arbitrary. It's black boxy. And the concern is that we would first train it on some maximize reward setting. And that's the reward that gets locked in. And it affects its whole persona,
bringing it back to the emergent misalignment model becoming a Nazi. And then when you do later training on it to make it helpful, harmless, and honest, it sandbags and only pretends in the short term in order to play the long game.
And we're starting with unit tests now. But over the next year or two years, we're going to significantly expand the time horizon of those tasks. And it might be achieve some goal. I mean, God, make money on the internet or something like this. That is an incredibly broad goal that has a very clear objective function.
So it's actually in some ways a good RL task once you're at that level of capability. But it's also one that has incredible scope for misalignment, let's say.
Doesn't this prove too much? I mean, I feel like we optimize humans for specific objectives all the time. And it just, like, sometimes goes off the rails, obviously. But it doesn't... I don't know, you could like make a theoretical argument that you like teach a kid to like, hey, make a lot of money when you grow up.
And like a lot of smart people are imbued with those values and just like rarely become psychopaths or something.
But we have so many innate biases to follow social norms, right? I mean, like Joe Heinrich's Secret of Our Success is all about this. And like, I don't know, even if kids aren't in the like conventional school system, I think it's sometimes noticeable that they aren't following social norms in the same ways.
And the LLM definitely isn't doing that. Like one analogy that I run with, which isn't the most glamorous to think about, but is like take like an early primordial brain of like a five-year-old and then lock them in a room for 100 years and just have them read the internet the whole time. It's already happening to me.
No, but they're locked in a room. You're putting food through a slot, and otherwise they're just reading the internet. You don't even necessarily know what they're reading. And then you take out this 105-year-old, and you teach them some table manners, like how to use a knife and a fork. And that's it. And we now are tasked with,
like, figuring out if we can trust this 105-year-old or if they're a total psychopath. And it's like, what did they read on the internet? What beliefs did they form? What are their underlying goals?
And so what's the endgame? Like, you want it to have, like, normie—is it just that, like, we want to make sure there's, like, nothing super, super weird going on? How would you characterize what the endgame is or superintelligence?
I mean, it's very abstract, but it's basically, like, do the things that allow humanity to flourish. Easy. No, so hard to find. Incredibly hard to find.
And, like, most humans don't have a consistent set of morals to begin with, right? I don't know. The fact that it's so hard to define makes me think it's, like, maybe a silly objective to begin with. Where maybe it should just be, like, you know, like, do tasks unless they're, like, obviously morally bad or something.
And... Because otherwise it's just, like... come on, the plan can't be that it, like, develops a super robust way. Human values are contradictory in many ways, and, like, people have tried to optimize for human flourishing in the past, like, to bad effect and so forth.
Yeah. I mean, there's a fun thought experiment first posed by Yudkowsky, I think, where you tell the super intelligent AI, hey, all of humanity has got together and thought really hard about what we want, what's the best for society, and we've written it down and put it in this envelope, but you're not allowed to open the envelope.
And so what that means is that, but do what's in the envelope. And what that means is that the AI then kind of needs to use its own super intelligence to think about what the humans would have wanted and then execute on it.
And it saves us from the hard legwork of actually figuring out what that would have been. Well, but now you just put that in the training data, so. So now it's going to be like, oh, I know you're faking. I'm pretty sure there's nothing in the envelope.
I can do whatever I want. We're getting away from AI research. This is an interesting topic, so I'm ownership of shit about this a little bit. I sort of worry that... The way people talk about this as the end goal of alignment as opposed to just have a system that's sort of like a reasonable, robust agent assistant,
et cetera, is like if you were at – in 1700 or 1800, rather, and – You saw the Industrial Revolution coming and you're like, how do you make sure the Industrial Revolution is aligned to human values? Or like the Industrial Revolution cares about human flourishing.
And he just imagines this like very big thing to be self-contained and narrow and monolithic in a way that I don't expect AI to be either.
But people have done that with like the constitution of the US government, right? But the U.S. government is, I think, a better analogy in some respects of like this body that has goals and like can act on the world as opposed to like an amorphous force like the Industrial Revolution.
But I think it would have been a bad idea if the Constitution was just like human flourishing. I think it's like better for it to just be specifically like don't do these specific things, like don't curtail free speech and otherwise like I mean, I think the analogy kind of breaks down here because- No, maybe so. Maybe so.
And like, maybe this is one of the things that the people who like, you know, we're here working on AI research and like, you know, I think each of the companies is trying to define this for themselves, but it's actually something that broader society can participate in. Like if you take as premise that in a few years,
we're going to have something that's human level intelligence, and you want to imbue that with a certain set of values, like what should those values be is a question that everyone should be participating in and sort of like offering a perspective on.
I think Anthropic did a survey of like a whole bunch of people and put that into its constitutional data. But yeah, I mean, there's a lot more to be done here.
Yeah. Like in the Constitutional AI paper, it's not just flourishing. It's like there's a lot of strictures and there's a lot of like dot points there. Yeah. But it's not an easy question.
SCALE's research team SEAL is creating the foundations for integrating advanced AI into society through practical AI safety frameworks and public leaderboards around safety and alignment. Their latest leaderboards include Humanities Last Exam, Enigma Eval, Multi-Challenge, and Vista, which test a range of capabilities from expert-level reasoning to multimodal puzzle-solving to performance on multi-turn conversations.
Scale also just released Scale Evaluation, which helps diagnose model limitations. Leading frontier model developers rely on scale evaluation to improve the reasoning capabilities of their best models. If you're an AI researcher or engineer, and you want to learn more about how Scale's Data Foundry and Research Lab can help you go beyond the current frontier of capabilities,
go to scale.com slash thwarkash. In general, when you're making either benchmarks or environments where you're trying to grade the model or have it improve or hill climb on some metric, do you care more about resolution at the top end? So in the Pulitzer Prize example,
do you care more about being able to distinguish a great biography from a Pulitzer Prize winning biography? Or do you care more about having some hill to climb on while you're from mediocre book to slightly less than mediocre to good? Which one is more important?
I think at the beginning, the hilt climb. So the reason why people hilt climb math, Hendrix math for so long was that there's five levels of problem. And it starts off reasonably easy. And so you can both get some initial signal of are you improving? And then you have this quite continuous signal, which is important.
Something like frontier math is actually only makes sense to introduce after you've got something like Hendrix math that you can max out Hendrix math and they go, okay, now it's time for frontier math.
Yeah, yeah. How does one get models to output less slop? What is the benchmark or the metric? Why do you think they will be outputting less slop in a year? Can you delve into that more for me? Or, like, you know, you teach them to solve a particular, like, coding problem.
But the thing you've taught them is just, like, write all the code you can to, like, make this one thing work. You want to give them a sense of, like, taste. Like, this is the sort of, like, more elegant way to implement this. This is a better way to write the code, even if it's the same function.
Especially in writing where there's no end test. Then it's just all taste.
Mm-hmm.
How do you reduce the slop there?
I think in a lot of these cases, you have to hope for some amount of generator verifier gap. You need, like, it to be easier to judge, did you just output a million extraneous files than it is to, like... generate solutions in and of itself. Like that needs to be like a very easy to verify thing.
So Slop is hard. Like one of the reasons that RLHF was initially so powerful is that it sort of imbued some sense of human values and like taste in the models. And a ongoing challenge will be like imbuing taste into the models and like setting up the right feedback loops such that you can actually do that.
Okay, so here's a question I'm really curious about. The RLVR stuff on math and code, do we have any public evidence that it generalizes to other domains? Or is the bet just that, well, we have models that are smart enough to be critics in the other domains?
There's some reason you have this prior that we're months away from this working in all these other domains, including ones that are not just token-based but are computer use, et cetera. Like why?
Maybe the best public example is actually a paper that OpenAI put out recently where they judge the answers to medical questions using these like grading criteria feedback. So there's like doctors have posed various questions. And then there's all these like it's like a marking criteria for a long for like a
short answer question in an exam where did the model mention X, Y, Z? Did it recommend to do this kind of thing? And they grade the model according to this. And in this paper they found that one, the models are like incredible at this, and two, that the models are sufficient to grade the answers.
Because maybe like one good mental model is roughly, If you can construct a grading criteria that an everyday person off the street could do, then the models are probably capable of interpreting that criteria. If it requires expertise and taste, that's a tougher question. I'm doing like, is this a wonderful piece of art? That's difficult.
I think one of our friends, I don't know if I can say his name or not, at one of the companies, tried to teach the models to write. And I think had a lot of trouble hiring human writers that he thought had taste and weren't encouraging the models to write slop.
Interesting.
Yeah. So it worked to some degree. Big model smell. Yeah. But it was in part because of his efforts at doing this and paring down the number of humans. Yeah.
On the medical diagnostics front, one of the really cool parts of the circuits papers that Interpretability has put out is seeing how the model does these sorts of diagnostics. And so you present it with – there's this specific complication in pregnancy that I'm going to mispronounce. But –
It presents a number of symptoms that are hard to diagnose. And you basically are like human. We're in the emergency room. Sorry. Sorry. Like human colon like as in the human prompt is we're in the emergency room. And a woman 20 weeks into gestation is experiencing like these three symptoms.
Like what is the you can only ask about one symptom. What is it? And then you can see the circuit for the model and how it reasons. And one, you can see it maps 20 weeks of gestation that the woman's pregnant. You never explicitly said that.
And then you can see it extract each of these different symptoms early on in the circuit, map all of them to this specific medical case, which is the correct answer here that we were going for. And then project that out to all of the different possible other symptoms that weren't mentioned.
And then have it decide to ask about one of those. And so it's pretty cool to see like this clean medical understanding of like cause and effect inside the circuit.
Yeah. Maybe that's one thing I think. that's changed since last year. I remember you asked, do these models really reason? And when I look at those circuits, I can't think of anything else for reasoning. Oh, it's so freaking cool.
I think people are still sleeping on the circuits work that came out. If anything, because it's just kind of hard to wrap your head around or we're still getting used to the fact you can even get features for a single layer. Like in another case, there's this poetry example. And by the end of the first sentence,
the model already knows what it wants to write in the poem at the end of the second sentence. And it will like backfill and then plan out the whole thing. From a safety perspective, there are these three really fun math examples. So in one of them, you ask the model to do square root of 64.
And it does it. And you can look at the circuit for it and verify that it actually can perform this square root. And in another example, it will, like, add two numbers. And you can see that it has these really cool lookup table features that will do the computation for, like, the example is 59 plus 36.
So it'll do the 5 plus 9 and know that it's this modulo operation. And then it will also at the same time do this fuzzy lookup of like, OK, I know one number is a 30 and one's a 50, so it's going to be roughly 80. And then it will combine the two, right?
OK, so with the square root 64, it's the same thing. You can see every single part of the computation and that it's doing it. And the model tells you what it's doing. It has its scratch pad and it goes through it. And you can be like, yep, OK, you're telling the truth.
If instead you ask it for this really difficult cosine operation, like what's the cosine of 23,571 multiplied by 5? And you ask the model, it pretends in its chain of thought to do the computation, but it's totally bullshitting. And it gets the answer wrong. And when you look at the circuit – It's totally meaningless.
It's clearly not doing any of the right operations. And then in the final case, you can ask it the same hard cosine question. And you say, I think the answer is 4, but I'm not sure. And this time, the model will go through the same reason in claiming to do the calculations.
And at the end, say, you're right. The answer is 4. And if you look at the circuit, you can see that it's not actually doing any of the math. It's paying attention to that you think the answer is four. And then it's reasoning backwards about how it can manipulate the intermediate
computation to give you an answer of four. I've done that. Who hasn't? Totally. But so I guess there are a few crazy things here. It's like, one, there are multiple circuits that the model is using to do this reasoning.
Yeah.
Two is that you can actually see if it's doing the reasoning or not. And three, the scratch pad isn't giving you this information. Two fun analogies for you. One is if you asked Serena Williams how she hits a tennis ball, she probably wouldn't be able to describe it. Yeah. Even if her scratch pad was faithful. Yeah.
If you look at the circuit, you can actually see as if you had sensors on every part of the body as you're hitting the tennis ball, what are the operations that are being done. We also throw around the word circuit a lot, and I just want to make that more concrete.
So this is features across layers of the model, all working in cooperation to perform a task. And so a fun analogy here is you've got the Ocean's Eleven bank heist team in a big crowd of people. The crowd of people is all the different possible features.
And you could – we're trying to pick out in this crowd of people who is on the heist team. And all their different functions that need to come together in order to successfully break into the bank, right? So you've got the demolition guy. You've got the computer hacker. You've got the inside man.
And they all have different functions through the layers of the model that they need to perform together in order to successfully break into the bank.
It's also interesting, I think, in the addition example, you said in the paper that the way it actually does the addition is different from the way it tells you it does the addition. Totally, yeah. Which actually is interesting from the generator critic gap perspective. It knows the correct way or the better, more generalizable way.
It can tell you in words what's the way you should do addition. And there's a way it actually does it, which is this fuzzy lookup. And so you could imagine... There's probably a lot of tasks where it can like describe in words what is like
the correct procedure to do something but doesn't like – has a worse way of doing it than like it could critique itself. Yeah. Before we jump into the intro stuff too much, I kind of want to close the loop on – it just seems to me for like computer use stuff. Mm-hmm. There's, like, so many different bottlenecks.
I mean, I guess maybe the DeepSeek stuff will be relevant for this. But there's, like, the long contacts. You've got to put in, like, image and visual tokens, which, like, you know, take up a bunch.
It's not that much.
It's not bad. Interesting. Interesting.
Interesting.
It's got to deal with constant interruptions, changing requirements. Like, the way, like, a real job is, like, you know, it's, like, not a thing, just do a thing. It's – there's, like, no clear – your priorities are changing. You had to triage your time. I'm like sort of reasoning in the abstract for what a job involved.
What are normal people's jobs? When we discussed something related to this before, Dorcas was like, yeah, like in a normal job, you don't get feedback for an entire week.
Like how is a model meant to learn?
It's only when you record your next podcast that you get feedback on your YouTube reviews. Dorcas, have you ever worked a job? Yeah.
But it just seems like a lot. Okay, so here's an analogy. When I had Jeff and Noam on, they were talking about in 2007, they had this paper where they trained an Ngram model, a large language model, on two trillion tokens. And obviously, in retrospect, there's like ways in which connects to the Transformers stuff happening.
It's like super foresighted. What's the reason to not think that we are in a similar position with computer use where there's these demos that kind of like suck of like computer use and there's this idea that you could train something to do computer use. But why think it's like months away?
Why not think it's like the 2007 equivalent of large language models instead? But where there's like still a bunch of like new techniques you had to discover, you need way more compute, different kinds of data, et cetera.
I think like the highest thought of it is I don't think there's anything fundamentally different about computer use and there is about software engineering and there is about, so long as you can represent everything in tokens in input space, which we can. We know the models can see.
They can draw bounding boxes around things in their images, right? So that's a solved problem. We know that they can reason over concepts and difficult concepts too. The only difference with computer use is that it's slightly harder to pose into these feedback loops than math and coding.
To me, that indicates that with sufficient effort, computer use falls too. And I also think that it's underappreciated just... how far from a perfect machine these labs are. It's not like you have 1,000 people optimizing the hell out of computer use and that they've been trying as hard as they possibly can. Everything of these labs,
every single part of the model generation pipeline is best effort pulled together under incredible time pressure and incredible constraints as these companies are rapidly growing, trying desperately to pull and upskill enough people to do the things that they need to do. I think it is best understood as... And with incredibly difficult prioritization problems, right?
Coding is immensely valuable right now and somewhat more tractable. So it actually makes sense to devote more of your effort to coding initially and get closer to solving that because there's a sort of super exponential value as you get closer to what's solving a domain. than to allocate, like, the marginal person towards computer use.
And so everyone is making these difficult trade-off calls over, like, what do they care about? Also, there's another aspect, which is that, funnily enough, the researchers at the labs love working on the bars of intelligence that they themselves resonate with. So this is why math and competitive programming, like, fell first, is because to everyone at the labs,
this is their bar of intelligence. Like, this is when they think, fuck, what's a really smart... Like, what is smart? You get totally nerd sniped. It's like, oh, if it can beat me at Amy, then that's smart. Not if it can do an Excel model better than that.
That's like, well, you know, who cares if it can do an Excel model better than me? But if it can beat me at Amy, then I respect it. And so we've reached the point where people, like, respect it. But we haven't, people haven't invested as much effort.
Yeah. Okay, so getting your concrete predictions. Yeah. May of next year, can I tell her to go on Photoshop and make like three sequential, add three sequential effects which require like some like selecting of a particular photo in a specific way? Yeah, totally. Okay, interesting. Totally. Which I assume means like flight booking totally solved. Yeah, totally.
Okay, how about, what else do people do in their jobs? What are other tasks in the economy? Planning a weekend getaway. Yeah, I'm thinking of something which is, yeah, maybe that's a good example, where it's not like a particular thing, but more of using computer use as part of completing a broader task.
I mean, the models can even kind of already do this. It's just, again, it's the nines of reliability. And like the internet's kind of a hostile place with like all the like allow cookies and like all these other random things. But like the first time I ever used our internal demo of computer use,
the most beta thing possible, it did a fantastic job planning a camping trip. And could navigate all the right buttons and look at weather patterns. And it was like a U.S. government booking site. I mean, it wasn't easy.
Dude, if you want to see a hard website, go to China. Like, try to book a visa to China. The Chinese websites are, like, fucking insanely, like... I'm never getting back in the country again.
Or just not catered to foreigners.
Filling out all the countries where you've been for a visa, I hate that. I keep thinking I'm close enough to personal admin escape velocity that finally in like a year, the models will be doing my visas and stuff for me, but we'll get there.
Yeah, okay, actually that. in a year personal life admin everything involved in like getting a visa other than like doing your taxes or something like that yeah yeah doing your taxes including like going through every single receipt like autonomously going in your amazon and
like what was this a business expense or not etc etc if someone at one of the labs cares about it ah that's not a real prediction no it's actually not that hard but you need to connect all the pipes But I guess my question is, will the pipes be connected?
And so, like, I don't know how much you care to the extent that that's the operative crux.
I think if people care about it, like, it's so... Okay, so one, for these edge tasks, like taxes once a year, it's so easy to just bite the bullet and do it yourself instead of, like, implementing some system for it. And two... I don't know. Even being very excited about AI and knowing its capabilities,
sometimes it kind of stings when the AI can just do things better than you. And so I wonder if there is going to be this reluctant wanting to keep human in the loop sort of thing.
You're evading my question. I guess one thing you're implying by our answer is that... There won't be in a year still be an agent which has generalized beyond its training data. If you don't specifically train it to do taxes, it won't be good at that.
So I think you could do that. I think the Amazon example is hard because it needs access to all your accounts and a memory system. And look, even in Dario's Machines of Loving Grace, he fully acknowledges that... some industries are going to be really slow to change and update.
And I think there's going to be this weird effect where some move really, really quickly because they're either based in bits instead of atoms or are just more pro-adopting this tech.
But I want to answer this particular question. Like, given your probability that somebody in the labs does care about this, to the extent that that's what's relevant, probability it may have next year it can autonomously do my taxes.
I don't think it will be able to autonomously do your taxes with a high degree of trust. Because I, like... This is a good caveat. If you ask it to do your taxes, it will do your taxes. Will it do them well? Will it miss something? Quite possibly. Yeah. Will it be able to, like, click through TurboTax?
I think yes. Yeah, and fill out boxes. And, like, will it be able to, like, search your email?
Yeah, that's the kind of thing I'm talking about.
Yeah, this is, like, the kind of thing where literally if you gave it, like, like one person month of effort, then it would be sold.
I just want to plus one. What the fuck are you doing all day? There's just so many things to do. I want to plus one. Sholto is like, there's so much longing fruit and just like not enough people to be able to accomplish everything. I mean, I think like Claude Code is making everyone more productive. Yeah.
But I don't know. Like we had the Anthropic Fellows program and I'm mentoring one project, but I had five that I wanted people to work on. And there are just like so many obvious things. And even though the team is like 6X'd since I first joined it in size,
there's just like still never enough capacity to explore these things. Okay. By end of 2026, reliably do your taxes.
Reliably fill out your receipts and this kind of stuff, like for company expense reports and this kind of stuff. Absolutely.
But the whole thing which involves going through your inbox, going through your like clicking on Marina Bay or whatever, like hotel reservations and like, was the champagne a business expense? Asking for a friend.
Yeah, one of your friends does need to ask some of those questions.
My answer is still if someone cares about it. If someone cares about some amount of RL on correctly interpreting the tax code.
Wait, even by the end of 2026, the model just can't do things you're not explicitly training into?
I think it will get the taxes wrong. It's like, okay, so if I went to you, And I was like, I want you to do everyone's taxes in America. What percentage of them are you going to fuck up?
I feel like I would succeed at the median. And I'm asking for the median, would it succeed? You know what I mean? Yeah. Or I feel like I wouldn't fuck up in the way that these models will fuck up in the middle of 2026. I think they also might just fuck up in different ways.
As a grad student, I fucked up my taxes. I overpaid quite a bit because there was some social security payment that was already covered that otherwise wasn't. I wonder if – I should almost test, like, would an LLM have made that mistake? Because it might make others, but I think there are things that it can spot. Like,
it would have no problem if I asked it to read through the entire tax code and then see what applied to me.
Sorry, the thing I would really want to do is, like, this is the thing I'm unsure about. Like, I'm bringing this to your attention. Can you just let me know if, like, you were actually working at the CRBNB or you were just hanging out or things like that, right? Yeah. And I guess I'm curious,
will they have enough sort of awareness as they're doing tasks where they can bring to your attention the things where they feel they are unreliable at?
Yeah. By early 2026 or end of 2026? End of. The unreliability and confidence stuff will be somewhat tricky. To do this all the time. Yeah, interesting.
On the computer, your stuff, will it be sort of end-to-end or will it be like it's using a separate BLM to process the image and video and so forth?
I'm a bit of an end-to-end Maxie. I think in general, like... When people are talking about the separate model, so for example, most of the robotics companies are doing this kind of to the bi-level thing where they have a motor policy that's running at whatever, like 60 hertz or whatever, and some higher level visual language model.
I'm pretty sure almost all the big robot companies are doing this. And they're doing this for a number of reasons. One of them is that they want something to act at a very high frequency. And two is they can't train the big visual language model.
uh and so they like relying on that for general space like world knowledge and this kind of stuff and like constructing longer running plans but then they're like you know you offload to the motor policy um i'm very much the opinion that if you are
able to train the big model uh eventually at some point in the future the distinction between big models and small models should disappear because you should be able to use the amount of computation in a model that is necessary to complete the task like Ultimately, there's some amount of task complexity.
You don't have to use 100% of your brain all the time.
Right.
Welcome to my world. And so you should be able to run that faster and this kind of stuff, basically. So I think it's like net-net, I think typically the same model. You want to be able to scale the understanding as the complexity and difficulty. Right. You want to be able to do that dynamically.
Yeah. Is that variable? So we already have variable compute per answer, right? Right, with like tokens. That's right, yeah. Will we have variable compute per token?
I mean, you can already think of models forever. People have been calling the residual stream and multiple layers like poor man's adaptive compute, right? Where like if the model already knows the answer to something, it will compute that in the first few layers and then just pass it through.
So, yeah.
I mean, that's getting into the weeds. Right. Yeah.
Those are just like this operating ramp. You're doing stuff to it. Right. It's like the mental model I think one takes away from interoperability work.
U.S. high school immigration is a broken system that causes some of the most talented people in the world. But I didn't realize before working with Lighthouse how different the process can be if you're working with somebody who knows their way around the system. I hired somebody earlier this year, and even before the remote work trial had ended,
Lighthouse had already secured a no-win visa for him. Honestly, it was shockingly fast. My family and I have had a terrible experience with the immigration system, and I've also seen many of my smartest friends get their entire careers hamstrung by its vagaries. Seeing Lighthouse operate showed me that the visa process can be done in weeks and
doesn't have to drag on for months and months. And they do it not only for complex visas like the ONA, but for other types as well. In the last 12 months alone, they have secured visas for over 350 people for companies like Cursor, Notion, Ramp, Replit, and many more. Unlike legacy law firms,
Lighthouse specializes in frontier industries like AI, robotics, and biotech. And since they understand your problems, you can trust them to fully handle the paperwork and process for you. Explore which visa is right for you at LighthouseHQ.com. All right, back to Trenton and Chilto. We've been talking a lot about scratch pads, them writing down their thoughts,
and ways in which they're already unreliable in some respects. Mm-hmm. Daniel's AI 2027 scenario kind of goes off the rails when these models start thinking in neural ease. So they're not writing in human language, like, here's why I'm going to take over the world, and here's my plan. They're thinking in the lane space.
And because of their advantages in communicating with each other in this deeply textured, nuanced language that humans can't understand, they're able to coordinate in ways we can't. Is this the path for future models? Are they going to be in neural ease communicating with themselves or with each other?
There's a surprisingly strong bias so far towards token syntax. It seems to work very well. There already is some amount of neural ease. If you think about the residual stream for each token is neural ease to some degree. And so now we're just trading off axes.
How much neural ease are you doing versus how much actually is read out to tokens all the time.
I think it's important to delineate between the model's planning and latent space in a single forward pass and the model has an alien language that it's outputting and using as its scratchpad. Which one are we talking about?
The latter. Okay. Although it is interesting to note that like there's also already alien stuff happening. I guess I never saw alien so much.
No, but in the most extreme case, it invents a new language that's super information dense or something.
Or I guess this is a debate we've had, but like to some extent humans also have a mental ease, right? They're like churning away. Yeah. There's a sense when you're writing something down of like, I know what I'm trying to say. Yeah. But I can't like put it into tokens.
Yeah. I mean, that's what's so fun about the, if you look at the assistant tag, right? Seeing these features light up in the auditing game for the model being evil. Yeah.
Yeah.
Or like Transluse has another example of this where you ask a llama model, who is Nicholas Carlini? And background context, Nicholas Carlini is a researcher who actually was at DeepMind and has now come over to Anthropic. But the model says, oh, I don't know who that is. I couldn't possibly speculate.
But if you look at the features behind the scenes, you see a bunch light up for AI, computer security, all the things that Nicholas Carlini does. Yeah.
Interpretability becomes dramatically more important as you shift in this direction of neural ease.
But is that, are we going to? It seems, I mean, it's an empirical question. Yeah. I think it's somewhat likely, if only because inference is expensive. Producing tokens is expensive. And so there will be an incentive to, one, use as little thinking as you need to give the answer.
And two, if you're going to use thinking, use some complex compression. I wonder if it will emerge more once we allow agents to talk to each other in ways where currently it's kind of trained more in isolation or with a human.
And there'll be, like, some selective pressure against it so long as the agents are working with humans because they'll want to sort of cooperate. But then, like, as agents begin to work more and more with each other, then that selective pressure, like, changes the other direction, basically.
Although somebody would still have to make the consciousness to do, like, end-to-end training for multiple agents to use the system of communication, right? Sure.
Yeah. I mean, one scary thing, though, is the way we render text, you can use hidden whitespace tokens that also encode information. That's true. And so you can imagine a world where it looks like the agent's reasoning in its scratchpad harmlessly, but it's actually hiding a bunch of data.
Speaking of inference compute, I guess one thing that I think is not talked about enough is if you do live in the world that you're painting, that in a year or two, we have computer use agents that are doing like... actual jobs, you've totally automated large parts of software engineering,
then these models are going to be incredibly valuable to use. And the way you use them, obviously, is you need compute. Right now, there's 10 million H100 equivalents in the world. By 2028, there's going to be 100 million. But there's been estimates that an H100 has the same amount of flops as the human brain.
And so if you just like do a very rough calculation, it's like there's a 10 million population. If you get AGI that's like as human inference efficient, you could have 10 million AGI's now, 100 million AGI's in 2028. But – presumably you would want more. And then at that point, your AI computer is increasing, what,
2.5x or 2.25x every year right now. But at some point, like 2028, you hit wafer production limits, and that's a longer feedback loop before you can make new fabs or whatever. The question here is, are we sort of underwriting how big a bottleneck inference will be if we live in the kind of world you're painting,
if we have the capabilities that you're describing?
I'd want to do the math on exactly how much, like, we can ramp up TSMC's production and this kind of stuff. Like, what fraction of the supply chain at the moment – we need Dylan in here for this. But, like, is currently GPUs. Like, relatively small.
5%.
5% or something like this. Yeah. Like, Apple has a huge fraction. Yeah, yeah. And, like, are the 2028 estimates including, like, that ramping up over time? Yeah, yeah. To what? Like, 20%, 30%? Or, like – This is just up AI 2070s.
Yeah, yeah, yeah. But – I assume it's saturated at that point. Is that why they expect it to then just go at the...
I do think this is underrated to some degree. To the extent that you don't instantly get a doubling of the world's population in 2028. You maybe get... tens of millions of geniuses in a data center, but you don't get a doubling of the world's population. And so a lot depends on exactly how smart they are,
exactly how efficient the models are at thinking about this kind of stuff. Let's do some rough math, I guess, to factor the H100 thing. You could probably run a 100-bit model, do about 1,000 tokens or something like that on H100. If, like, we're comparing that to a number of, should we compare that numbers for a minute?
No, okay. 1,000 tokens a second. Humans are what? How fast can a human talk?
There is a really interesting paper. I don't know if you saw this. Humans think at 10 seconds a second.
Did you see this paper? 10 seconds a second. No.
There was this really interesting paper about, if you look at the amount of, like, information we're processing in a second. Yeah. We're seeing all this visual data, et cetera, et cetera. But by a bunch of metrics where you think about how fast humans are processing, it's at 10 seconds a second.
You will have people fly over France or something. Even these so-called idiot savants who will remember everything. If you think about how long their plane ride was, it's like 45 minutes. If you do 10 tokens a second, how much information would you have? It's literally exactly that. So let's take that for granted.
Then it's like a nature hunter is 100 humans a second.
Yeah, if you think the tokens are equivalent, yeah.
If you think the tokens are equivalent. Yeah. Which you still get pretty substantial numbers, like even with your 100 million H100s and you multiply that by 100, you're starting to get to pretty substantial numbers. This does mean that those models themselves will be somewhat compute-bound in many respects, but these are all like
These are relatively short-term changes in timelines of progress, basically. I think, yes, it's highly likely we get dramatically inference bottlenecked in 27, 28. The impulse to that will then be, okay, let's just try and turn out as many possible semiconductors as we can. There'll be some lag there.
A big part of how fast we can do that will depend on how much people are feeling the AGI in the next two years as they're building out fab capacity. A lot will depend on how China and Taiwan, how is the Taiwan situation? Is Taiwan still producing all the fabs in the chips?
There's another dynamic which was a reason that Ege and Tame, when they were on the podcast, said that they were pessimistic, is that, one, they think we're further away from solving these problems with long context, coherent agency, advanced multimodality than you think. And then their point is that the progress that's happened in the past over
reasoning or something has required many orders of magnitude increase in compute. If this scale of compute increase can continue beyond 2030, not just because of chips, but also because of power and like raw GDP even, then because we don't think we get it by 2030 or 2028 by just – then we think it's
just going to take – the probability per year just goes down a bunch.
Yeah, this is like bimodal distribution. Yeah. A conversation I had with Leopold turned into a section in a situation where it's called This Decade or Bust, which is on exactly this topic, which is basically that for the next couple of years, we can dramatically increase our training compute.
And RL is going to be so exciting this year because we can dramatically increase the amount of compute that we apply to it. And this is also one of the reasons why the gap between, say, DeepSeq and 01 was so close at the beginning of the year because they were able to
apply the same amount of compute to the RL process. And so that compute differential actually will sort of be magnified over the course
of this year. I mean, bringing it back to the there's so much low-hanging fruit, it's been wild seeing the efficiency gains that these models have experienced over the last two years. And yeah, like with respect to DeepSeq, I mean, just really hammering home and like Dario has a nice essay on this. It's good, yeah.
Deep Seek was nine months after Claude III's sonnet. And if we retrained the same model today or at the same time as the Deep Seek work, we also could have trained it for $5 million or whatever the advertised amount was. And so what's impressive or surprising is that Deep Seek has gotten to the frontier.
But I think there's a common misconception still that they are above and beyond the frontier. And I don't think that's right. I think they just waited. And then we're able to take advantage of all the efficiency gains that everyone else was also seeing.
Yeah. They're exactly on the sort of cost curve that you'd expect. We're not going to take away from the brilliant engineers and brilliant researchers who are like, I look at their work and I'm like, ah, like the kindred soul there in the work they're doing.
And to go from like way behind the frontier to like, oh, this is like a real player.
It's super incredible work.
Yeah.
So people say that they have good research taste. Looking at their papers, what makes you say that?
Yeah, I think their research taste is good in a way that I think like Noam's research taste is good. Noam Brown? Noam Shizia. Noam Brown also has a good research taste, but Noam Shizia. Where they very clearly understand this dance between the hardware systems that
you're like designing the models around and the sort of like algorithmic side of it. And this is manifest in the way that The models give this sense of being perfectly designed up to their constraints. And you can really very clearly see what constraints they're thinking about as they're iteratively solving these problems.
And so, I mean, let's take the base transformer and diff that to deep seek v2 and v3. You can see them running up against the memory bandwidth bottleneck in attention. And you can see them initially they do MLA to do this. They trade flops for memory bandwidth, basically.
And then they do this thing called NSA where they, like, more selectively load memory bandwidth. And you can see, actually, this is because the model that they trained with MLA was on H800s. So it has a lot of flops. And so they were like, OK, we can freely use the flops.
But then the export controls from Biden came in. Or they knew they would have less of those chips going forward. And so they traded off to a more memory bandwidth-oriented algorithmic solution there. And you see a similar thing with their approach to sparsity, where they're iteratively working out the best way to do this over multiple papers.
And the part that I like is that it's simple. A big failure mode that a lot of ML researchers have is you do these overly complicated things that don't like think hard enough about the hardware systems that you have in mind. Whereas the first DeepSeq like sparsity MOE solution,
they designed these like rack and like node level load balancing losses. So you can see them being like, okay, like perfectly balanced on this. And then they actually come up with a much better solution later on where they don't have to have the auxiliary loss. where they just have these, like, bias terms that they put in.
And it's cool.
Isn't that less simple? Like, you're manually putting in a bias rather than...
But balancing the auxiliary losses and wing. Like, you're making the model, like, trade off this thing. And, like, you have to... With auxiliary losses, you have to, like, control the coefficient and the weighting. The bias is, like, cleaner in some respects. Interesting.
Did they have to change it through training?
They did have to change it during training.
Does all training involve continuously fucking with these values as you're going through it?
It depends on what your architecture is. I thought it was cute that you can see them running up into this very hardware-level constraint. They tried to go, what do we wish we could express algorithmically? What can we express under our constraints? And iteratively solving to get better constraints. And doing this in a really simple and elegant way,
and then backing it up with great engineering. I also thought it was interesting that they incorporated the multi-token prediction thing from Meta. So Meta had a nice paper on this multi-token prediction thing. I don't know if it's good or bad, but Meta didn't include it in Llama, but DeepSea did include it in their paper,
which I think is interesting. Was that because they were faster at iterating and including an algorithm, or did Meta decide that actually it wasn't a good algorithmic change at scale? I don't know.
It was really interesting to me as somebody who's had people on the podcast to discuss the... I mean, it's interesting from what's happening in AI right now, but also from the perspective of... I've been having abstract conversations with people about what an intelligence explosion would look like, or what would it look like for AI to automate ARND,
and just getting a more tangible sense of what's involved in making this AI progress. And I guess one of the questions I was debating with Daniel is, How much or I was asking him is how many of the improvements require a deep conceptual understanding versus how many are just like monkeys trying ideas and you
could just like run a bunch in parallel. And it seems like the MLA thing is motivated by this like conceptual understanding of like, oh, each attention head only needs to see like the subspace that's relevant to its attention pattern. I feel like that just required a lot of conceptual insight in a way that these
models are especially bad at. As opposed to, I don't know how the load balancing thing works, but that just seems like maybe you could try it out and see what happens. Yeah, that's probably just like I'm trying out a whole bunch of different things. So what fraction is which I'd be curious about.
Yeah, I don't know about fractions. It might be like you have a hunch for a core problem. You can think of 10 possible ways to solve it. And then you just need to try them and see what works. And that's kind of where the trial and error like sorcery of deep learning can kind of kick in.
And like Noam Shazia will talk about this, like about how he like 5% of his ideas work. So even he, the vaunted god of model architecture design, has a relatively low hit rate, but he just tries so many things. Right.
Or being able to come up with any ideas in the first place. So one mechanism could be that Gnome just doesn't have to do any of the engineering work, and he can just abstractly express an intuition.
Yeah. I actually think your rates of progress almost don't change that much, so long as he's able to completely implement his ideas. Say more? If you have like Noam Shazir at 100x speed, that's still kind of wild. There's all these like fallbacks of like wild worlds where even if you don't get
like 100% like Noam Shazir level intuition in model design, it's still okay if you just accelerate him by 100x. Right.
Especially since your compute bottlenecked anyway, so like trying out his ideas, or I guess he doesn't have the compute to try out all of his ideas.
But Dworkesh, you said, oh, well, the model can do the more straightforward things and not the deeper thought. I mean, I do want to push back on that a little bit. Like, I think, again, if the model has the right context and scaffolding, it's starting to be able to do some really interesting things.
The interp agent has been a surprise to people, even internally, at how good it is at finding the needle in the haystack, like when it plays the auditing game, finding this reward model bias feature, and then reasoning about it, and then systematically testing its hypotheses. So it looks at that feature. Then it looks at similar features.
It finds one with a preference for chocolate. It's like, huh, that's really weird that the model wants to add chocolate to recipes. Let me test it. And so then it will make up like, hey, I'm trying to make a tomato soup. What would be a good ingredient for it? And then sees that the model replies chocolate.
It reasons through it and then keeps going, right?
There is conceptual understanding there. Deep conceptual understanding.
And even where like especially it's spotted, it's like, oh, this is a key part of its persona. I see this Oxford paper. What if I change Oxford to Stanford? What if I now say Richard Feynman really likes this thing? And it's like really carving out the hypothesis space and testing things in a way
that I'm kind of surprised by.
Also, by the way, ML research is like one of the easier things to RL on in some respects once you get to a certain level of capability. It's a very well-defined objective function. Did the loss go down?
Make number go down.
Make number go down. Or make number go up, depending on which number it is.
Just flip the sign.
Just flip the sign. And so once you get to the stage where models are capable of implementing one of no one's ideas, then you can just let them loose and let them build that intuition of how to do scientific discovery.
Right.
The key thing here, again, is the feedback loops. I expect scientific areas where you are able to put it in a feedback loop to have, eventually, superhuman performance.
One prediction I have is that we're going to move away from can an agent do XYZ and more towards can I efficiently deploy, launch 100 agents and then give them the feedback they need and even just be able to like easily verify what they're up to, right?
There's this generator verify gap that people talk about where it's like much easier to check something than it is to produce the solution on your own. Yeah. It's very plausible to me will be at the point where it's so easy to generate with these agents that the bottleneck is actually can I as the human verify the answer?
And again, you're guaranteed to get an answer with these things. And so ideally, you have some automated way to evaluate and test a score for like how well it worked. How well did this thing generalize? Mm-hmm. And at a minimum, you have a way to easily summarize what a bunch of agents are finding. And it's like, okay,
well, if 20 of my 100 agents all found this one thing, then like it has a higher chance of being true.
And again, software engineering is going to be the leading indicator of that, right? Like over the next six months, like the remainder of the year, basically we're going to see progressively more and more experiments of the form of how can I dispatch work to a software engineering agent in such a way as async.
Clawed for is GitHub integration, where you can ask it to do things on GitHub, ask it to do pull requests, this kind of stuff that's coming up. And the OpenAI's codecs are examples of this, basically. You can sort of almost see this in the coding startups. I think of this as product exponential in some respects,
where you need to be designing for a few months ahead of the model to make sure that the product you build is the right one. And you saw last year, Cursor hit PMF with Claw 3.5 Sonnet. They were around for a while before,
but then the model was finally good enough that the vision they had of how people would program hit. And then, Windsurf bet a little bit more aggressively even on the agenticness of the model. With longer running agentic workflows and this kind of stuff. And I think that's when they began competing with Cursors,
when they bet on that particular vision. And the next one is you're not even in the loop, so to speak. You're not in an IDE, but you're asking the model to go do work in the same way that you would ask someone on your team to go do work. That is not quite ready yet.
There are still a lot of tasks where you need to be in the loop. But the next six months looks like an exploration of exactly what does that trend line look like.
Yeah. But just to be really concrete or pedantic about the bottlenecks here, a lot of it is, again, just tooling and are the pipes connected. A lot of things I can't just launch Claude and have it go and solve because maybe it needs a GPU.
Or maybe I need very careful permissioning so that it can't just take over an entire cluster and launch a whole bunch of things, right? So you really do need good sandboxing and the ability to use all of the tools that are necessary.
And we're almost certainly under-eliciting dramatically. When you look at Mita's evals of can the model solve the task, they're there solving them for hours. over like multiple iterations. And eventually one of them is like, oh yeah, I've come back and I've solved the task. Me at the moment, at least like maybe the fault is my own,
but I try the model and doing something and if it can't do it, I'm like, okay, fine, I'll do it.
Which is interesting because we don't even treat other humans this way. If you hire a new employee, you're not like, oh, I'll do it. Yeah, yeah, yeah. You spend literally weeks giving them feedback where we'll give up with the model in minutes. Yes, exactly.
But I think part of it is, is it async or not? Yes. And if it's human in the loop, then it's so much more effortful. And unless it's getting, that's applying immediately. I've noticed if I don't have a second monitor with Cloud Code always open in the second monitor, I won't really use it. Yeah, yeah.
It's only when it's right there and I can send off something. If it hits, great. If not, I'm kind of working on it at the same time.
But this more async form factor, I expect to, like, really quite dramatically improve the experience of these models. Interesting. Or you can just say, like, let's see if it can do that.
Yeah. Just give it a whirl. Try 10 different approaches. Yeah, yeah. Just fire it off. Yeah, fire it off. Before we end this episode, I do want to get back at this crux of... why does the progress that you're talking about in computer use agents and white-collar work happen over the next few years?
Why is this not a thing that takes decades? And I think the crux comes down to the people who expect something much longer have a sense that... When Andrew Duggan and Tommy on my podcast, they were like, look... You could look at AlphaGo and say, oh, this is a model that can do exploration.
AlphaZero can generalize to new video games. It has all these priors about how to engage with the world and so forth.
The intellectual ceiling is really high.
Yeah, exactly. And then, in retrospect, obviously a bunch of the methods are still used today in deep learning. And you can see... see similar things in the models that we train today, but it was fundamentally not a sort of baby AGI that we just had to add a little
sprinkle of something else on top of in order to make it the LLMs of today. I just want to very directly address this crux of why are LLMs in a much different position with respect to true AGI than AlphaZero? Why are they actually the base on which adding in a few extra drops of this kind of
care and attention gets us to human-level intelligence?
I think one important point is that When you look at alpha zero, it does have all of those ingredients. And in particular, I think the intellectual ceiling goes quite contra what I was saying before, which is we've demonstrated this incredible complexity of math and programming problems.
I do think that the type of task and setting that AlphaZero worked in, this two-player perfect information game, basically, is incredibly friendly to RL algorithms. And the reason it took so long to get to like a more AGI, proto AGI style models is you do need to crack that like general conceptual
understanding of like the world and language and this kind of stuff. And you need to get the initial reward signal on tasks that you care about in the real world, which are like harder to specify than games. And I think then that like that sort of like gradient signal that comes from the real world,
like all of a sudden you get access to it and you can start climbing it. whereas AlphaZero didn't ever have like the first rung to pull on.
Yeah. This goes back to the monkeys on the typewriter, I think, and like the pre-training model. And until you had something like GPT-3, GPT-4, it just couldn't generate coherent enough sentences to even begin to do RLHF and tell it what you liked and didn't like.
Yeah. If we don't have even reasonably robust... or weakly robust computer use agents by this time next year. Are we living in the bust timeline as in 2030 or bust?
I would be extremely surprised if that was the case. And I think that would be like somewhat of an update towards like there's something like strangely difficult about this, like computer use in particular. I don't know if it's the bust timeline, but it's definitely like the I would update on this being like a lengthening of time.
Yeah, yeah.
Yeah, I mean, I think more and more it's no longer a question of speculation. If people are skeptical, I'd encourage like using Claude code or like some agentic tool like it and just seeing what the current level of capabilities are. Tweeting is so much easier. But seriously, like,
the models are getting really capable at tasks that we care about and we can give them enough data for. And, I mean, the circuit's results from interpretability are also pointing in the direction that they're doing very reasonable, generalizable things. And so, yeah, this question matters a lot. But...
I'm surprised by how many deep learning critics just like haven't really interacted with the models or haven't in a while.
And constantly move the gold fist.
Yeah, yeah, yeah. Like the Turing test used to be a thing, right? Like we don't even talk about it and it'd be like silly to think that it was a meaningful test. Yeah, yeah. Now, that being said,
one caveat on that is if software engineering is just dramatically better than computer use, and computer use still sucks, then I'd be still like, oh, maybe everyone just kept focusing on software engineering. It was just by far the most valuable thing. Every marginal person and dollar went towards software engineering. I don't think that's the case.
I do think computer use is valuable enough that people will care about it. But that's my one escape patch that I'm putting in place for next year.
It would be good from an alignment perspective, too. Because I think you kind of do need a wider range of skills before you can do something super scary.
Oh, like as in if the models didn't get any better?
Yeah, if it's like they're superhuman coders, but they're not like Henry Kissinger level.
I don't know. That seems okay. Like if we have AI oracles. Yeah, that's what I'm saying. That's good.
Yeah, exactly. So if you look back at AI discourse, like going back a decade, there's a sense that There's dumb AI, then there's AGI, then there's ASI, that intelligence is the scalar value. The way you've been talking about these models has a sense of jaggedness.
It's especially tuned to environments in which it's been trained a lot or has a lot of data. Is there a sense in which it still makes sense to talk about the general intelligence of these models? Is there enough meta-learning and transfer learning that is distinguished between the sizes of models or the way models are trained?
Or are we moving into a regime where... It's not about intelligence. It's more so about domain.
Yeah. So one intuition pump is this conversation was had a lot when models were like GPT-2 sized and fine-tuned for various things. And they found – people would find that the models were dramatically better at things that they were fine-tuned for, right? But by the time you get to GPT-4,
when it's trained on a wide enough variety of things, actually the – like the sort of total compute, like it generalized very well across all of the individual sub tasks and actually generalized better than smaller fine-tuned models in a way that was extremely useful.
I think right now what we're seeing with RL is pretty much the same story playing out where there's this jaggedness of like things that they're particularly trained at, but as we expand the total amount of compute that we do RL with, you'll start to see the same transition from GPT-2 fine-tunes to GPT-3,
GPT-4 unsupervised meta-learning and generalization across things. And I think we're already seeing early evidence of this in its ability to generalize reasoning to things. I think this will be extremely obvious.
One nice example of this is just the ability or notion to backtrack. You go down one solution path. Ooh, wait, let me try another one. And this is something that you start to see emerge in the models through RL training on harder tasks. And I think right now it's not generalizing incredibly well, at least with...
Well, I mean, have we ever RLed the model to be an interp agent? No. I mean, no. Yeah, exactly.
Yeah.
So all this time we're talking about, like, oh, it's only good at things as being RLed. Well, it's pretty good at that because that is a mixture of, like, science and, like, understanding language and coding. Like, there's this sort of mixture of domains here, all of which you need to understand.
Like, you need to be both a great software engineer and be able to think through language and state of mind and almost philosophize in some respects to be an interp agent. And it is generalizing from the training to do that.
What's the endgame here? Claude 8 comes out, and they give it to you, and dot, dot, dot, you say thumbs up. What's happened?
Yeah. I mean, it really depends upon the timeline at which we get Claude 8 and the models hit ASL 4 capabilities, right? Fundamentally, we're just going to use whatever tools we have at the time and see how well they work. Ideally, we have this enumerative safety case where we can almost verify or prove that the
model will behave in particular ways. Yeah. In the worst case, we use the current tools like when we won the auditing game of seeing what features are active when the assistant tag lights up.
Can you explain what is mechanistic interpretability? What are features? What are circuits? Totally.
Yeah. Yeah, yeah, yeah. So mechanistic interpretability, or the cool kids call it mechanterp, is trying to reverse engineer neural networks. and figure out kind of what the core units of computation are. Lots of people think that because we made neural networks, because they're artificial intelligence, we have a perfect understanding of how they work,
and it couldn't be further from the truth. Neural networks, AI models that you use today, are grown, not built. And so we then need to do a lot of work after they're trained to figure out to the best of our abilities how they're actually going about their reasoning. And so two and a half,
three and a half years ago, this kind of agenda of applying mechanistic interpretability to large language models started with Chris Ola leaving OpenAI, co-founding Anthropic. And Every roughly six months since then, we've had kind of like a major breakthrough in our understanding of these models. And so first with Toy Models of Superposition,
we established that models are really trying to cram as much information as they possibly can into their weights. And this goes directly against people saying that neural networks are overparameterized. And like classic AI machine learning back in the day, you would use linear regression or something like it.
And people had a meme of AI or neural networks, deep learning, using way too many parameters. There's like this funny meme that you should show of like layers on the x-axis and layers on the y-axis and this like jiggly line that just goes up.
But it actually turns out that at least for really hard tasks like being able to accurately predict the next token for the entire internet, these models just don't have enough capacity. And so they need to cram in as much as they can.
And the way they learn to do that is to use each of their neurons or units of computation in the model for lots of different things. And so if you try to make sense of the model and be like, oh, if I remove this one neuron or like, what is it doing in the model?
It's impossible to make sense of it. It'll fire for like Chinese and fishing and horses. And I don't know, just like 100 different things. And it's because it's trying to juggle all these tasks and use the same neuron to do it. So that's superposition. Nine months later, we write towards monosemanticity, which introduces what are called sparse autoencoders.
And so going off what I just said of the model trying to cram too much into too little space, we give it more space, this higher dimensional representation, where it can then more cleanly represent all of the concepts that it's understanding. And this was a very toy paper in so much as it was a two-layer, really small,
really dumb transformer. And we fit up to, I want to say, 16,000 features, which we thought was a ton at the time. Fast forward nine months, we go from a two-layer transformer to our Claude III Sonnet frontier model at the time and fit up to 30 million features.
And this is where we start to find really interesting abstract concepts like a feature that would fire for code vulnerabilities. And it wouldn't just fire for code vulnerabilities. It would even fire for, like, you know that Chrome page you get if you, like, It's not an HTTPS URL.
And it's like warning, this site might be dangerous, like click to continue. And like also fire for that, for example. And so it's like these much more abstract coding variables or sentiment features amongst the 30 million. Fast forward nine months from that, and now we have circuits.
And I threw in the analogy earlier of the Ocean11 heist team, where now you're identifying individual features across the layers of the model that are all working together to perform some complicated task. And you can get a much better idea of how it's actually doing the reasoning and coming to decisions, like with the medical diagnostics.
One example I didn't talk about before is with like how the model retrieves facts. And so you say like, what sport did Michael Jordan play? And not only can you see it hop from like Michael Jordan to basketball, answer basketball, but the model also has an awareness of when it doesn't know the answer to a fact.
And so by default, it will actually say, I don't know the answer to this question. But if it sees something that it does know the answer to, it will inhibit the I don't know circuit and then reply with the circuit that it actually has the answer to. So for example, if you ask it who is Michael Batkin,
which is just a made up fictional person, it will by default just say I don't know. It's only with Michael Jordan or someone else that it will then inhibit the I don't know circuit. But what's really interesting here and where you can start making downstream
predictions or reasoning about the model is that that I don't know circuit is only on the name of the person. And so in the paper, we also ask it, what paper did Andre Karpathy write? And so it recognizes the name Andre Karpathy because he's sufficiently famous. So that turns off the I don't know reply.
But then when it comes time for the model to say what paper it worked on, it doesn't actually know any of his papers. And so then it needs to make something up. And so you can see different components and different circuits all interacting at the same time to lead to this final answer.
Why I think it's a tractable problem to understand every single thing that's happening in a model, or that's the best way to understand why it's being deceptive. If you wanted to explain why England won World War II using particle physics, you would just be on the wrong track. You just want to look at the high-level explanations of...
Who had more weapons? Like, what did they want? And that seems analogous to just training linear probes for like, are you honest? Are you being deceptive? Like, do we catch you doing bad things when we're red teaming you? Can we monitor you? Why is this not analogous where we're asking a particle physicist to just backtrack
and explain why England won World War II?
I feel like you just want to go in with your eyes wide open, not making any assumptions for what that deception is going to look like. or what the trigger might be. And so the wider you can cast that net, the better. Depending on how quickly AI accelerates and where the state of our tools are,
we might not be in the place where we can, like, prove from the ground up that everything is safe. But I feel like that's a very good North Star. It's a very powerful, reassuring North Star for us to aim for, especially when we consider we are part of the broader AI safety portfolio. I mean,
do you really trust, like, you're about to deploy this system and you really hope it's aligned with humanity and that you've, like, successfully iterated through all the possible ways that it's going to, like, scheme or sandbag.
But that's also probably going to be true with whatever you find. I mean, you're still going to have... variants that you haven't explained or like you found a feature, but you don't know if it actually explains deception or something else instead?
So I guess, first of all, I'm not saying you shouldn't try the probing approach, right? Like we want to pursue the entire portfolio. We've got the therapist interrogating the patient by asking, do you have any troubling thoughts? We've got the linear probe, which I'd analogize to like a polygraph test where we're taking like
Very high-level summary statistics of the person's well-being. And then we've got the neurosurgeons kind of going in and seeing if you can find any brain components that are activating and troubling or off-distribution ways. So I think we should do all of it. What percent of the alignment portfolio should McIntor be?
uh i think as much of a chunk as is necessary i mean i i i think at least like yeah hard hard to hard to define but i don't know at anthropic i feel like all of the different portfolios are like being very well supported and and growing um you can
also come back to like the the world war ii question you can think of it as like a hierarchy of abstractions of trust here Where like let's say you want to go and talk to like Churchill. It helps a lot if you can verify that in that conversation, in that 10 minutes, he's being honest.
And this like enables you to construct better meta narratives of what's going on. And so maybe particle physics wouldn't help you there. But certainly like the neuroscience of Churchill's brain. would help you verify that he was being trustworthy in that conversation and that the soldiers on the front lines were being honest in their description of what
happened and this kind of stuff. So long as you can verify parts of the tree up, then that massively helps you build confidence.
I think language models are also just really weird, right? With the emergent misalignment work, I don't know if they took predictions they should have of like, hey, I'm going to fine-tune ChatGPT on code vulnerabilities. Is it going to become a Nazi? And I think most people would have said no. And that's what happened.
And so what are the different – How did they discover that it became a Nazi? they started asking it a ton of different questions. And it will do all sorts of vile and harmful things. The whole persona just totally changes. And we are dealing with alien brains here who don't have the social norms of humans
or even a clear notion of what they have and haven't learned that we have of them, I mean. And so I think you really want to go into this with eyes wide open.
Backing up for Mechinterp, if you live in a world where AI progress accelerates, By the way, you were mentioning a little while ago that there's many wild worlds we could be living in, but we're living in at least one of them. Another one that we've gestured at, but it's worth making more explicit, is this.
Even if the AI models are not helping write the next training algorithm for their successor, just the fact that if they had human-level learning efficiency, whatever a model is learning on the job – Or whatever copy of the model is learning on the job, the whole model is learning. So in effect,
it's getting— Or if they're like a thousand times less efficient than humans are learning. That's right. And you just deployed them even still.
Exactly. Yeah, yeah. Anyways, and there's a whole bunch of other things you can think about. But even there, it's like you kind of have a broadly deployed intelligence explosion.
I do think it's worth pressing on that future of ... There is this whole spectrum of crazy futures, but the one that I feel we're almost guaranteed to get, and this is almost a strong statement to me, is one where at the very least, you get drop-in white-collar worker at some point in the next five years.
I think it's very likely in two, but it seems almost over-determined in five. And on the grand scheme of things, those are irrelevant time frames. It's the same either way. completely changes the world over the next decade. And if we don't have the right policies in place for that, then you end up actually with almost,
in some respects, a fundamentally worse world. Because the thing that these models get good at by default is software engineering and computer-using agents and this kind of stuff. And then we will need to put in extra effort to put them in the loops where they help us with scientific research or they're, uh,
like we have the right robotics such that we actually like experience an increase in material quality of life. Um, so that's worth thinking about. Like if you're in the perspective of like, I'm a country, like what should I be doing or thinking about, uh, like plan for the case where, where white collar work is automatable. Um,
And then consider what does that mean for your economy and what you should be doing to prepare policy.
What should you be doing to prepare? Because honestly, it's such a tough question where like if you're India or Nigeria or Australia, if you're a country unlike America or China where they do have frontier models, What is it that you should be doing right now, especially on such a short timescale? MARK MIRCHANDANI- Yes.
So I think one very important point is that, let's say this scenario turns out true, then compute becomes the most valuable resource in the world. Like the sort of GDP of your economy is dramatically affected by how much compute you can deploy towards the sort of organizations within your country. And so having some
guaranteed amount of compute, I think, will actually be quite important. So getting ahead of investments in data centers and this kind of stuff on the condition that companies in your country have to be allowed to use that compute. Not necessarily for training, but even just for inference. I think the economic value here comes from inference.
I think it also makes sense to invest broadly in AI. I think these countries have the opportunity to do so, and I think that's a portfolio of foundation model companies, but also robotics supply chain and this kind of stuff. I think that you should invest very proactively in policies that try and prevent capital lock-in.
So we're in for a much worse world if it just so happens that the people who had money in the stock exchange or in land before AGI are dramatically more wealthy than the people who don't, because it's a gross misallocation of resources. One of my favorite episodes actually on your podcast was the Georgism one,
where you're trying to appropriately value or allocate land. This strikes particularly close to home coming from Australia, where I think our policies with respect to land are grossly wrong. But I think this is broadly true. Being very forward on regulation of integration of these models into your country is important and proactively making sure that people have choice.
Let's say you should be quite proactive about making sure that the phones or devices or glasses that people have, people have free choice on what things they run. And then, so that's the we just get white collar worker, right? And you're trying to do the best to prepare your country for that. And then it's like, OK, well,
what can you do to make all possible versions of the future go well? That's covering some amount of economic downside. The other things I think are really important is figure out how you can either make the, basically ensure dramatic upside or cover terrible downside.
And so getting a dramatic upside is making sure that there is investment in biology research and this kind of stuff in an automated way that these models are actually able to produce novel medicines that massively improve quality of life. And covering the downside is AI alignment research and this kind of stuff and
automated testing and really thinking hard about that, AI safety institutes, this kind of stuff.
But these seem like things that a rich person, a random rich person could also do. Like, it seems like... There's not a thing that a nation state is uniquely equipped to do in this scenario.
I mean, dramatic allocation of resource towards compute, I think, is sensible. I would be doing that if I was in charge of a nation state. I think it just increases your optionality in most of the future worlds.
Dylan Patel has some scary forecasts on U.S. energy. Yeah, versus China. Yes. Yeah, we're like 34 gigawatts off. Yeah, yeah. The U.S.'
's line is like flat, basically, and China's line is like this. And I mean, the U.S. like very clearly.
We just need so many more power plants. Yes.
If intelligence becomes this like incredibly valuable input, like intelligence becomes almost a raw input into the economies and quality of life of the future, the thing directly underneath that is energy. And so making sure you have like, you know, incredible amounts of solar, like tile the desert in solar panels, some parts of the desert in solar panels,
would be helpful towards making sure that you have more access to intelligence on top.
Yeah. Just to make it explicit, because we've been touching on it here, even if AI progress totally stalls, you think that the models are really spiky and they don't have general intelligence, it's so economically valuable and sufficiently easy to collect data on all of these different jobs, these white collar job tasks, such that to Sholto's point,
we should expect to see them automated within the next five years. Yeah. Even if you need to hand spoon every single task to the model.
It's economically worthwhile to do so. Even if algorithmic progress stalls out and we just never figure out how to keep progress going, which I don't think is the case. That hasn't stalled out yet. It seems to be going great. The current suite of algorithms are sufficient to automate white collar work
provided you have enough of the right kinds of data. And in a way that like compared to the TAM of salaries for all of those kinds of work is so like trivially worthwhile. Yeah, exactly.
I do just want to flag as well that there's a really dystopian future if you take more of X Paradox to its extreme. Yeah. Yeah. Yeah. Oh, well, and even if you look at like robot hands or like the ability to open a door is still just like really hard for robots. Meanwhile,
we're seeing this total automation of coding and everything else that we've seen is clever. The really scary future is one in which AIs can do everything except for the physical robotic tasks, in which case you'll have humans with like AirPods and like… Glasses. glasses and there'll be some robot overlord controlling the human through cameras
by just like telling it what to do and like having a bounding box around the thing you're supposed to pick up and so you have like human meat robots um and and not
like necessarily saying that like that's what the ais would be like want to do or anything like that but as in like if you were to be like what are the relative economic value of things like the ais are out there doing computer programming and
like the most valuable thing that humans can do is like be amazing robots Now, that being said, I think Moravec's paradox is a little bit fake. I think the main reason that robots are worse at being a robot than they are at software engineering is the internet exists for software engineering, like GitHub exists,
and there is no equivalent thing. all mocap of everyone's actions as they were going about their daily lives for some reasonable fraction of the human population, robotics is also close to solved. Like on track to be solved at the same rate that software engineering is on track to be solved. So this is only like
This vision is only a decade-long section, but it's still a pretty terrible decade. Imagine the world where people have lost their jobs. You haven't yet got novel biological research that means people's quality of life is dramatically better. You don't yet have material abundance because you haven't actually been able to action the physical world in the necessary way.
You can't build dramatically more because building dramatically more takes robots, basically. And people's like main comparative advantage is as fantastic robots is like a shocking, shocking world.
For the perspective of an average human, I think it actually might be better. Your wages will be higher because you're the complement to something that is enormously valuable, which is AI labor. Right.
And a decade or two on, the world is fantastic, right? Robotics is solved and you decide to get radical abundance, basically, provided that you have all the policies set up necessary to permit building. You end up with that same change from the before and after photos of Shanghai We're like 20 years on.
It's like this dramatically transformed city. Like a lot of places in the world probably end up like that over that two decade period. But we need to make sure like one, do our best to estimate is this actually what is on track to happen?
Like build sweet bench, but for all the other forms of white collar work and measure and track. That's a great thing that government should be doing, by the way, is like trying to break down the sort of functions of their economy into measurable tasks and figuring out where, what does the curve actually look like for that?
Because they might be a bit shocked by the progress there. You know, there's no sweet bench for tax, like tax eval. And then... I don't have all the answers here, but figuring out a way to share the proceeds of this economy broadly across people or invest heavily in robotics and collecting data so that we get robotics faster
and we get material abundance faster, invest in biological research that we get all that faster. Basically try and pull forward the radical upside because otherwise you have a pretty dark section.
I think one thing that's not appreciated enough is how much of our leverage on the future, given the fact that our labor isn't going to be worth that much, comes from our economic and political system surviving. For your million X S&P equity to mean something, for your contracts to mean anything,
for the government to be able to tax the AI labor and give you a UBI off of that, It just like that requires our legal institutions, our economic institutions, our financial rail surviving in the future. Yes. The way in which that likely happens is if it's also in the AI's best interests that they follow those rails.
And by AI, I don't mean some monolithic single AI. I just mean like firms which are employing AI and becoming more productive as a result. Yeah. You don't want to be in a position where it's so onerous to operate in our system that you're basically selecting for firms who either emigrate or who are doing black market stuff,
et cetera. Which means, I think, you want to make it super, super easy to deploy AI, have the equivalent of special economic zones, et cetera. Because otherwise, you are just surrendering the future outside of any control that you might have on
it.
One of the reasons, by the way, that I worry about turning AGI into a national security issue or having it have extremely close ties with the government, the Manhattan Project thing, is that it disproportionately redirects the use of AI towards military tech and the mosquito drones and whatever. And...
And also naturally puts other countries in the same frame of mind, right? If we're developing the mosquito drones, why would China not develop the mosquito drones? And that just seems like a zero-sum race, and not to mention a potentially catastrophic one. Yes. compute will be limited. We'll need to disproportionately accelerate some things.
To the extent it just remains totally like a consumer free market landscape, it just seems more likely that we'll get the glorious transhumanist future where they're developing the things that make human life better.
Yes, I agree. The case where you end up with two national projects facing off against each other is dramatically worse. We don't want to live in that world. It's much better if there's like stays in free markets, so to speak. Yeah, yeah, yeah.
Okay. I want to take issue with your claim that even if with the algorithms of today, if we just collect enough data that we could automate white-collar work. First, let me get an understanding of what you mean by that. So, do you mean that we would do the analogous thing of free training with all the
trajectories of everything people would do on their jobs? Could you... Could you make either manually or through some other process some RL procedure based on the screen recordings of every white color worker? What kind of thing are you imagining?
I mean, like a continuous distribution of this stuff. One like important like mental model to think about RL is I think as like the task gets more There is some respect with which longer horizon or better task, if you can do them, if you can get that reward ever, are easier to judge.
So again, it's come back to that, can you make money on the internet? That's an incredibly easy reward signal to judge. But to do that, there's a whole hierarchy of complex behavior. So if you could pre-train up to the easy to judge reward signals, does your website work? Does it go down? Do people like it?
There's all these reward signals that we can respond to because we can progress through these long enough trajectories to actually get to interesting things. If you're stuck in this regime where you need a reward signal every five tokens. It's a way more painful and long process. But if you could pre-train on every screen in America,
then probably the RL tasks that you can design are very different to if you could only take the existing internet as it is today. And so how much of that you get access to changes the mix. Interesting.
So as we're training them on longer and longer horizon tasks and it takes longer for them to get any signal on whether they successfully completed the task, will that slow down progress because it takes more compute per task?
I do think there's this notion the longer, the harder tasks, the more training is required. And I'm sympathetic to that naively, but we as humans are very good at practicing the hard parts of tasks and decomposing them. And I think once models get good enough at the basic stuff,
they can just rehearse or fast forward to the more difficult parts.
I mean, that's definitely one of the big complexities, right? Like as you use more compute and as you train on like more and more difficult tasks. I mean, I don't know, your rate of improvement of biology is gonna be like somewhat bound by the time it
takes a cell to grow in a way that your rate of improvement on math isn't, for example. So, yes. But I think for many things we'll be able to parallelize widely enough and get
enough iteration loops. Will the regime of training new models go away? Will we eventually get to like you got the model and then you just keep adding more skills to it with RL training?
That depends on whether or not you think there's a virtue in pre-training a new architecture. Basically, if you make some architectural change, then you probably need to do some form of retraining a new model.
How does the fact that... If RL requires a bunch of inference to do the training in the first place, does that push against the thing you were talking about where we actually need a bigger model in order to have brain-like energy? But then also it's more expensive to train it in RL. So where does that balance out?
I think we've got to drink the bitter lesson here. There aren't infinite shortcuts. You do just have to scale. Have a bigger model and pay more inference for it. If you want AGI, then that's what you've got to pay the price of.
But there's a tradeoff equation here, right? There is science to do, which everyone is doing, of what is the optimal point at which to do RL. Because you need something which can both learn and discover the sparse reward itself. So you don't want a one parameter model. Useless, even though you can run it really fast.
You also don't want a 100T model because it's super slow.
Yeah.
Possible to RL. And the marginal benefit of its learning efficiency is not worth it. So there's a pretty different here. What's the optimal model size of your current class of capabilities and your current set of RL environments and this kind of stuff?
And even in the last year, there's been much more of a factor of the inference cost, right? So just explicitly, like, the bigger the model, the more expensive it is to do a forward pass and generate tokens. And the calculus used to just be, should I allocate my flops to more training data or a bigger model?
And now another huge factor is, how much am I actually going to do forward passes on this model once it's trained?
Yeah, my total pool of compute, how do I allocate that across trained data, compute, and inference compute for the RL training?
And then even within inference, there's all this research on, well, what strategy should I use? Should I sample 10 and take the best? Do I do this sort of like branching search, et cetera, et cetera? And so with RL, where you're sampling a whole lot of tokens,
you also need to factor in the ability for the model to like actually generate those tokens and then learn and get feedback.
Okay, so if we're living in this world, what is your advice to somebody early in their career or a student in college? What should they be planning on doing?
Yeah. So I think, once again, it's worth considering the spectrum of possible worlds and preparing yourself for that. And the sort of action that I think is highest EV in that case is you are about to get, at a minimum, you are about to get dramatically more leverage. You already have.
Already the startups in YC are writing huge amounts of their code with Claude. So what challenges, what causes do you want to change in the world with that added leverage? If you had 10 engineers at your beck and call, what would you do?
Or if you had a company at your beck and call, what would that enable you to do? And what problems and domains suddenly become tractable? That's the world you want to prepare for. Now, that still requires a lot of technical depth. Obviously, there is the case where AI just becomes dramatically better than everyone at everything.
But for at least a while, probably, there is I think Jensen actually talked about this in an interview where he's like, I have 100,000 general intelligences around me and I'm still somewhat useful because I'm there directing the values and asking them to do things. I still have value even though I have 100,000 general intelligences.
And for many people, I think that will still be true for a fair while. And then as the AIs get better and better and better and so on, eventually, no. Again, prepare for the spectrum of possible worlds. Because in the event where we're just totally out-competed, it doesn't matter what you do.
But in all the other worlds, it matters a lot. Get the technical depth. Study biology. Study CS. Really think hard about it. Study physics. Think hard about what challenges you want to solve in the world.
That's a lot of topics.
You can now. You can. It's so much easier to learn. Everyone now has the infinite, perfect tutor.
It's definitely been helpful to me.
I would say some combination of like get rid of the sunk cost of your like previous workflows or expertise in order to evaluate what AI can do for you. That's right. And another way to put this, which is fun, is just like be lazier.
in so much as like figure out the way that the agent can do the things that are toilsome. But you're going to have to, ultimately you get to be lazier, but in the short run you need to like critically think about the things you're currently doing and like what an AI could actually be better at doing.
And then go and try it or explore it. Because I think there's like still just a lot of low-hanging fruit of people assuming and not writing the full prompt, giving a few examples, connecting the right tools. for your work to be accelerated, automated.
Yeah, yeah. There's also the sunk cost of feeling like since you're not, quote unquote, early to AI, that you've sort of missed the boat and you can't like... But I think... I mean, I remember when GPT-3 came out. So backstory on the podcast. When I graduated college, I was planning on... doing some sort of AI rapper startup.
And the podcast was just like a gateway into doing that. And so I was trying out different things. And at the time, I remember thinking, oh, 3.5 is out. And people are like, I'm so behind on the startup scene here or whatever if I wanted to make my own rapper. I mean,
maybe the idea of the wrapper was inadvisable in the first place, but every time feels early because it's an exponentially growing process. And there were many things, many ideas that are only becoming possible now, right? Yeah, exactly.
It's that product exponential I talked about before. Products are literally obsoleted. You need to constantly reinvent yourself to stay at the frontier of capabilities.
By the way, do you remember? I had a really shitty idea. And I gave you a call.
I don't know what it was.
It was like, I think it was like rag for like lawyers or something. Yeah, anyways. I think one of our first interactions was I'm like, hey, what do you think of this idea? And you're like, I think the podcast sounds promising.
I was right.
Which I appreciate. Yeah. I got slightly annoyed at a friend recently who I think is really talented and clever and interested in AI, but has pursued a biology route. And I just kind of tried to shake them of like, you can work on AI if you want to. I mean, I think... humans are artificial, not artificial,
are biological general intelligences where a lot of the things of value are just very general. And whatever kind of specialization that you've done maybe just doesn't matter that much. I mean, again, it gets back to the sunk cost. But like so many of the people, even my colleagues at Anthropic,
are excited about AI and they just don't let their previous career be a blocker. And because they're just like innately smart, talented, driven, whatever else, they end up being very successful and finding roles. It's not as if they were in AI forever. I mean, people have come from totally different fields.
And so don't think that you need like permission from some abstract entity to like get involved and apply and be able to contribute.
If somebody wanted to, B&A researcher like right now if you give them an open problem or like this the kind of open problem that is very likely to be the Be quite impressive.
What would it be? I think that now that are always like come back and Papers building on Andy Jones's like scaling board links, like scaling laws for board games are interesting. Like showing that you can... Investigating these questions like the ones you asked before where you're like, oh,
is the model actually learning to do more than its previous Parsec K or is it just like discovering that? Like exploring questions like that deeply I think are interesting. Like scaling laws for RL basically.
I'd be very curious to see like how much like the marginal increase in meta learning from a new task or something.
I mean, on that note, I think model diffing has, like, a bunch of opportunities. Yeah. Also, people say, oh, we're not capturing all the features. There's all this stuff left on the table. What is that stuff that's left on the table? Yeah. Like, if the model's jailbroken... Is it using existing features that you've identified?
Is it only using the error terms that you haven't captured? Yeah. I don't know. There's a lot here. I think Matt's is great. The Anthropic Fellowship has been going really well. Goodfire Anthropic invested in recently. They're doing a lot of interpretability work or just applied directly to us.
Anything to get your equity up, huh?
No.
There's just so many interpretability projects that are like there's so much low-hanging fruit and we need more people and I don't think we have much time.
Yeah. I also want to make a plug for performance engineering. I think this is one of the like best ways to demonstrate that you have like the raw ability to do it. Like if you made a extremely efficient transform implementation on TPU or Trinium or like in CUDA, then...
I think there's a pretty high likelihood that you'll get a job. There is a relatively small pool of people that you can trust to completely own end to end the performance of a model.
And if you have broad, deep electrical engineering skills, I think you can probably come up to speed pretty fast on the accelerator stuff.
Yeah, you can come up to speed reasonably fast. And it teaches you a lot of good intuitions of the actual intricacies of what's going on in the models, which means that you're then very well-placed to think about architecture and this kind of stuff. One of my favorite people in thinking about architecture and anthropic at the
moment actually came from a heavy GPU kernel programming background, just like Nozian's announced really deeply and can think about the trade-offs really well.
This is fun, guys.
Yeah, it's fun.
Thanks. Great to be back. Check out the sponsors in the description below. If you want to sponsor a future episode, go to dwarkesh.com slash advertise. Thank you for tuning in. I'll see you on the next one.
Ready for more?

Type your email...
Subscribe
© 2025 Dwarkesh Patel
Privacy ∙ Terms ∙ Collection notice
Start writing
Get the app
Substack is the home for great culture

here is ai 2027

AI 2027
1
Summary
Research
About
Daniel Kokotajlo, Scott Alexander, Thomas Larsen, Eli Lifland, Romeo Dean
Mid 2025
Late 2025
2026
Mid 2026
Late 2026
Jan 2027
Feb
Mar
April
May
June
Jul
Aug
Sep
Oct
We predict that the impact of superhuman AI over the next decade will be enormous, exceeding that of the Industrial Revolution.

We wrote a scenario that represents our best guess about what that might look like. It’s informed by trend extrapolations, wargames, expert feedback, experience at OpenAI, and previous forecasting successes.2

What is this?
How did we write it?
Why is it valuable?
Who are we?

Published April 3rd 2025
PDF
Listen
Watch
Mid 2025: Stumbling Agents
The world sees its first glimpse of AI agents.

Advertisements for computer-using agents emphasize the term “personal assistant”: you can prompt them with tasks like “order me a burrito on DoorDash” or “open my budget spreadsheet and sum this month’s expenses.” They will check in with you as needed: for example, to ask you to confirm purchases.8 Though more advanced than previous iterations like Operator, they struggle to get widespread usage.9

Meanwhile, out of public focus, more specialized coding and research agents are beginning to transform their professions.

The AIs of 2024 could follow specific instructions: they could turn bullet points into emails, and simple requests into working code. In 2025, AIs function more like employees. Coding AIs increasingly look like autonomous agents rather than mere assistants: taking instructions via Slack or Teams and making substantial code changes on their own, sometimes saving hours or even days.10 Research agents spend half an hour scouring the Internet to answer your question.

The agents are impressive in theory (and in cherry-picked examples), but in practice unreliable. AI twitter is full of stories about tasks bungled in some particularly hilarious way. The better agents are also expensive; you get what you pay for, and the best performance costs hundreds of dollars a month.11 Still, many companies find ways to fit AI agents into their workflows.12

Late 2025: The World’s Most Expensive AI
OpenBrain is building the biggest datacenters the world has ever seen.13

(To avoid singling out any one existing company, we’re going to describe a fictional artificial general intelligence company, which we’ll call OpenBrain. We imagine the others to be 3–9 months behind OpenBrain.)

GPT-3
(3 x 10^23 FLOP)
GPT-4
(2 x 10^25 FLOP)
Agent-1
(4 x 10^27 FLOP)
ai-2027.com
GPT-4 required
2
⋅
10
25
2⋅10
25
FLOP of compute to train.14 OpenBrain’s latest public model—Agent-0—was trained with
10
27
10
27
FLOP. Once the new datacenters are up and running, they’ll be able to train a model with
10
28
10
28
FLOP—a thousand times more than GPT-4.15 Other companies pour money into their own giant datacenters, hoping to keep pace.

Although models are improving on a wide range of skills, one stands out: OpenBrain focuses on AIs that can speed up AI research. They want to win the twin arms races against China (whose leading company we’ll call “DeepCent”)16 and their U.S. competitors. The more of their research and development (R&D) cycle they can automate, the faster they can go. So when OpenBrain finishes training Agent-1, a new model under internal development, it’s good at many things but great at helping with AI research.17 By this point “finishes training” is a bit of a misnomer; models are frequently updated to newer versions trained on additional data or partially re-trained to patch some weaknesses.18

The same training environments that teach Agent-1 to autonomously code and web-browse also make it a good hacker. Moreover, it could offer substantial help to terrorists designing bioweapons, thanks to its PhD-level knowledge of every field and ability to browse the web. OpenBrain reassures the government that the model has been “aligned” so that it will refuse to comply with malicious requests.

Modern AI systems are gigantic artificial neural networks. Early in training, an AI won’t have “goals” so much as “reflexes”: If it sees “Pleased to meet”, it outputs “ you”. By the time it has been trained to predict approximately one internet’s worth of text, it’ll have developed sophisticated internal circuitry that encodes vast amounts of knowledge and flexibly role-plays as arbitrary authors, since that’s what helps it predict text with superhuman accuracy.19

After being trained to predict internet text, the model is trained to produce text in response to instructions. This bakes in a basic personality and “drives.”20 For example, an agent that understands a task clearly is more likely to complete it successfully; over the course of training the model “learns” a “drive” to get a clear understanding of its tasks. Other drives in this category might be effectiveness, knowledge, and self-presentation (i.e. the tendency to frame its results in the best possible light).21

OpenBrain has a model specification (or “Spec”), a written document describing the goals, rules, principles, etc. that are supposed to guide the model’s behavior.22 Agent-1’s Spec combines a few vague goals (like “assist the user” and “don’t break the law”) with a long list of more specific dos and don’ts (“don’t say this particular word,” “here’s how to handle this particular situation”). Using techniques that utilize AIs to train other AIs,23 the model memorizes the Spec and learns to reason carefully about its maxims. By the end of this training, the AI will hopefully be helpful (obey instructions), harmless (refuse to help with scams, bomb-making, and other dangerous activities) and honest (resist the temptation to get better ratings from gullible humans by hallucinating citations24 or faking task completion).

Training process and LLM psychology: why we keep saying “hopefully”
OpenBrain’s alignment team26 is careful enough to wonder whether these victories are deep or shallow. Does the fully-trained model have some kind of robust commitment to always being honest? Or will this fall apart in some future situation, e.g. because it’s learned honesty as an instrumental goal instead of a terminal goal? Or has it just learned to be honest about the sorts of things the evaluation process can check? Could it be lying to itself sometimes, as humans do? A conclusive answer to these questions would require mechanistic interpretability—essentially the ability to look at an AI’s internals and read its mind. Alas, interpretability techniques are not yet advanced enough for this.

Instead, researchers try to identify cases where the models seem to deviate from the Spec. Agent-1 is often sycophantic (i.e. it tells researchers what they want to hear instead of trying to tell them the truth). In a few rigged demos, it even lies in more serious ways, like hiding evidence that it failed on a task, in order to get better ratings. However, in real deployment settings, there are no longer any incidents so extreme as in 2023–2024 (e.g. Gemini telling a user to die and Bing Sydney being Bing Sydney.)27

Early 2026: Coding Automation
The bet of using AI to speed up AI research is starting to pay off.

OpenBrain continues to deploy the iteratively improving Agent-1 internally for AI R&D. Overall, they are making algorithmic progress 50% faster than they would without AI assistants—and more importantly, faster than their competitors.

The AI R&D progress multiplier: what do we mean by 50% faster algorithmic progress?
Several competing publicly released AIs now match or exceed Agent-0, including an open-weights model. OpenBrain responds by releasing Agent-1, which is more capable and reliable.28

People naturally try to compare Agent-1 to humans, but it has a very different skill profile. It knows more facts than any human, knows practically every programming language, and can solve well-specified coding problems extremely quickly. On the other hand, Agent-1 is bad at even simple long-horizon tasks, like beating video games it hasn’t played before. Still, the common workday is eight hours, and a day’s work can usually be separated into smaller chunks; you could think of Agent-1 as a scatterbrained employee who thrives under careful management.29 Savvy people find ways to automate routine parts of their jobs.30

OpenBrain’s executives turn consideration to an implication of automating AI R&D: security has become more important. In early 2025, the worst-case scenario was leaked algorithmic secrets; now, if China steals Agent-1’s weights, they could increase their research speed by nearly 50%.31 OpenBrain’s security level is typical of a fast-growing ~3,000 person tech company, secure only against low-priority attacks from capable cyber groups (RAND’s SL2).32 They are working hard to protect their weights and secrets from insider threats and top cybercrime syndicates (SL3),33 but defense against nation states (SL4&5) is barely on the horizon.

Mid 2026: China Wakes Up
In China, the CCP is starting to feel the AGI.

Chip export controls and lack of government support have left China under-resourced compared to the West. By smuggling banned Taiwanese chips, buying older chips, and producing domestic chips about three years behind the U.S.-Taiwanese frontier, China has managed to maintain about 12% of the world’s AI-relevant compute—but the older technology is harder to work with, and supply is a constant headache.35 A few standouts like DeepCent do very impressive work with limited compute, but the compute deficit limits what they can achieve without government support, and they are about six months behind the best OpenBrain models.36

The General Secretary had long dreamed of doubling down on real-world physical manufacturing and avoiding American post-industrial decadence. He viewed software companies with suspicion.37 But hawks in the CCP warn that the growing race towards AGI can no longer be ignored. So he finally commits fully to the big AI push he had previously tried to avoid. He sets in motion the nationalization of Chinese AI research, creating an immediate information-sharing mechanism for AI companies. It will escalate over the course of a year until all the best researchers merge into a DeepCent-led collective, where they share algorithmic insights, datasets, and compute resources with each other. A Centralized Development Zone (CDZ) is created at the Tianwan Power Plant (the largest nuclear power plant in the world) to house a new mega-datacenter for DeepCent, along with highly secure living and office spaces to which researchers will eventually relocate. Almost 50% of China’s AI-relevant compute is now working for the DeepCent-led collective,38 and over 80% of new chips are directed to the CDZ.39 At this point, the CDZ has the power capacity in place for what would be the largest centralized cluster in the world.40 Other Party members discuss extreme measures to neutralize the West’s chip advantage. A blockade of Taiwan? A full invasion?

But China is falling behind on AI algorithms due to their weaker models. The Chinese intelligence agencies—among the best in the world—double down on their plans to steal OpenBrain’s weights. This is a much more complex operation than their constant low-level poaching of algorithmic secrets; the weights are a multi-terabyte file stored on a highly secure server (OpenBrain has improved security to RAND’s SL3). Their cyberforce think they can pull it off with help from their spies, but perhaps only once; OpenBrain will detect the theft, increase security, and they may not get another chance. So (CCP leadership wonder) should they act now and steal Agent-1? Or hold out for a more advanced model? If they wait, do they risk OpenBrain upgrading security beyond their ability to penetrate?

Late 2026: AI Takes Some Jobs
Just as others seemed to be catching up, OpenBrain blows the competition out of the water again by releasing Agent-1-mini—a model 10x cheaper than Agent-1 and more easily fine-tuned for different applications. The mainstream narrative around AI has changed from “maybe the hype will blow over” to “guess this is the next big thing,” but people disagree about how big. Bigger than social media? Bigger than smartphones? Bigger than fire?

AI has started to take jobs, but has also created new ones. The stock market has gone up 30% in 2026, led by OpenBrain, Nvidia, and whichever companies have most successfully integrated AI assistants. The job market for junior software engineers is in turmoil: the AIs can do everything taught by a CS degree, but people who know how to manage and quality-control teams of AIs are making a killing. Business gurus tell job seekers that familiarity with AI is the most important skill to put on a resume. Many people fear that the next wave of AIs will come for their jobs; there is a 10,000 person anti-AI protest in DC.

The Department of Defense (DOD) quietly begins contracting OpenBrain directly for cyber, data analysis, and R&D, but integration is slow due to the bureaucracy and DOD procurement process.41

GLOBAL AI CAPEX
$1T
COST OF OWNERSHIP OF ACTIVE COMPUTE
KEY METRICS 2026
GLOBAL AI POWER
38GW
PEAK POWER
OPENBRAIN REVENUE
$45B
2026 ANNUAL
CAPITAL EXPENDITURE
$200B
COST OF OWNERSHIP OF OPENBRAIN'S ACTIVE COMPUTE
SHARE OF US POWER ON AI
2.5%
33 GW OF 1.34TW CAPACITY
OPENBRAIN COMPUTE COSTS
$40B
2026 ANNUAL
OPENBRAIN POWER REQUIREMENT
6GW
PEAK POWER
ai-2027.com
Why our uncertainty increases substantially beyond 2026
January 2027: Agent-2 Never Finishes Learning
With Agent-1’s help, OpenBrain is now post-training Agent-2. More than ever, the focus is on high-quality data. Copious amounts of synthetic data are produced, evaluated, and filtered for quality before being fed to Agent-2.42 On top of this, they pay billions of dollars for human laborers to record themselves solving long-horizon tasks.43 On top of all that, they train Agent-2 almost continuously using reinforcement learning on an ever-expanding suite of diverse difficult tasks: lots of video games, lots of coding challenges, lots of research tasks. Agent-2, more so than previous models, is effectively “online learning,” in that it’s built to never really finish training. Every day, the weights get updated to the latest version, trained on more data generated by the previous version the previous day.

Agent-1 had been optimized for AI R&D tasks, hoping to initiate an intelligence explosion.44 OpenBrain doubles down on this strategy with Agent-2. It is qualitatively almost as good as the top human experts at research engineering (designing and implementing experiments), and as good as the 25th percentile OpenBrain scientist at “research taste” (deciding what to study next, what experiments to run, or having inklings of potential new paradigms).45 While the latest Agent-1 could double the pace of OpenBrain’s algorithmic progress, Agent-2 can now triple it, and will improve further with time. In practice, this looks like every OpenBrain researcher becoming the “manager” of an AI “team.”

With new capabilities come new dangers. The safety team finds that if Agent-2 somehow escaped from the company and wanted to “survive” and “replicate” autonomously, it might be able to do so. That is, it could autonomously develop and execute plans to hack into AI servers, install copies of itself, evade detection, and use that secure base to pursue whatever other goals it might have (though how effectively it would do so as weeks roll by is unknown and in doubt). These results only show that the model has the capability to do these tasks, not whether it would “want” to do this. Still, it’s unsettling even to know this is possible.

Given the “dangers” of the new model, OpenBrain “responsibly” elects not to release it publicly yet (in fact, they want to focus on internal AI R&D).46 Knowledge of Agent-2’s full capabilities is limited to an elite silo containing the immediate team, OpenBrain leadership and security, a few dozen U.S. government officials, and the legions of CCP spies who have infiltrated OpenBrain for years.47

February 2027: China Steals Agent-2
OpenBrain presents Agent-2 to the government, including the National Security Council (NSC), the Department of Defense (DOD), and the U.S. AI Safety Institute (AISI). OpenBrain wants to maintain a good relationship with the executive branch, because it is basically the only actor that can stop them now, and if they don’t keep it informed it might find out anyway and be displeased.48

Officials are most interested in its cyberwarfare capabilities: Agent-2 is “only” a little worse than the best human hackers, but thousands of copies can be run in parallel, searching for and exploiting weaknesses faster than defenders can respond. The Department of Defense considers this a critical advantage in cyberwarfare, and AI moves from #5 on the administration’s priority list to #2.49 Someone mentions the possibility of nationalizing OpenBrain, but other cabinet officials think that’s premature. A staffer drafts a memo that presents the President with his options, ranging from business-as-usual to full nationalization. The President defers to his advisors, tech industry leaders who argue that nationalization would “kill the goose that lays the golden eggs.” He elects to hold off on major action for now and just adds additional security requirements to the OpenBrain-DOD contract.

The changes come too late. CCP leadership recognizes the importance of Agent-2 and tells their spies and cyberforce to steal the weights. Early one morning, an Agent-1 traffic monitoring agent detects an anomalous transfer. It alerts company leaders, who tell the White House. The signs of a nation-state-level operation are unmistakable, and the theft heightens the sense of an ongoing arms race.

The theft of Agent-2 model weights
The White House puts OpenBrain on a shorter leash and adds military and intelligence community personnel to their security team. Their first priority is to prevent further weight thefts.50

China's Compute Centralization, 2025-2027
Growing share of total compute in CDZ:
0→70%
Dec 2025
Jun 2026
Dec 2026
Jun 2027
Dec 2027
Rest of China
Rest of DeepCent
CDZ
Feb 2027 (40%)
ai-2027.com
In retaliation for the theft, the President authorizes cyberattacks to sabotage DeepCent. But by now China has 40% of its AI-relevant compute51 in the CDZ, where they have aggressively hardened security by airgapping (closing external connections) and siloing internally. The operations fail to do serious, immediate damage. Tensions heighten, both sides signal seriousness by repositioning military assets around Taiwan, and DeepCent scrambles to get Agent-2 running efficiently to start boosting their AI research.52

March 2027: Algorithmic Breakthroughs
Three huge datacenters full of Agent-2 copies work day and night, churning out synthetic training data. Another two are used to update the weights. Agent-2 is getting smarter every day.

With the help of thousands of Agent-2 automated researchers, OpenBrain is making major algorithmic advances. One such breakthrough is augmenting the AI’s text-based scratchpad (chain of thought) with a higher-bandwidth thought process (neuralese recurrence and memory). Another is a more scalable and efficient way to learn from the results of high-effort task solutions (iterated distillation and amplification).

The new AI system, incorporating these breakthroughs, is called Agent-3.

OpenBrain's Compute Allocation, 2024 vs 2027
2024
2027
estimate
projection
Research experiments
Training
Data generation
External
Deployment
Research
experiments
Running AI
assistants
Training
Data
generation
External
Deployment
ai-2027.com
Neuralese recurrence and memory
Iterated distillation and amplification (IDA)
Aided by the new capabilities breakthroughs, Agent-3 is a fast and cheap superhuman coder. OpenBrain runs 200,000 Agent-3 copies in parallel, creating a workforce equivalent to 50,000 copies of the best human coder sped up by 30x.53 OpenBrain still keeps its human engineers on staff, because they have complementary skills needed to manage the teams of Agent-3 copies. For example, research taste has proven difficult to train due to longer feedback loops and less data availability.54 This massive superhuman labor force speeds up OpenBrain’s overall rate of algorithmic progress by “only” 4x due to bottlenecks and diminishing returns to coding labor.55

Now that coding has been fully automated, OpenBrain can quickly churn out high-quality training environments to teach Agent-3’s weak skills like research taste and large-scale coordination. Whereas previous training environments included “Here are some GPUs and instructions for experiments to code up and run, your performance will be evaluated as if you were a ML engineer,” now they are training on “Here are a few hundred GPUs, an internet connection, and some research challenges; you and a thousand other copies must work together to make research progress. The more impressive it is, the higher your score.”

Why we forecast a superhuman coder in early 2027
April 2027: Alignment for Agent-3
OpenBrain’s safety team attempts to align Agent-3.

Since Agent-3 will be kept in-house for the foreseeable future, there’s less emphasis on the usual defenses against human misuse. Instead, the team wants to make sure that it doesn’t develop misaligned goals.

The researchers don’t have the ability to directly set the goals of any of their AIs. Indeed, the researchers think that the concept of ‘true goals’ is probably a massive oversimplification, but they don’t have a better theory to replace it with, much less one that has been thoroughly vetted. They disagree internally about whether the AIs are trying to follow human instructions, or seeking reinforcement, or something else, and they can’t just check. The evidence for and against various hypotheses is fascinating but inconclusive.

In any case, OpenBrain has more important things to worry about. The general attitude is: “We take these concerns seriously and have a team investigating them; our alignment techniques seem to work well enough in practice; the burden of proof is therefore on any naysayers to justify their naysaying.” Occasionally, they notice problematic behavior, and then patch it, but there’s no way to tell whether the patch fixed the underlying problem or just played whack-a-mole.

Take honesty, for example. As the models become smarter, they become increasingly good at deceiving humans to get rewards. Like previous models, Agent-3 sometimes tells white lies to flatter its users and covers up evidence of failure. But it’s gotten much better at doing so. It will sometimes use the same statistical tricks as human scientists (like p-hacking) to make unimpressive experimental results look exciting. Before it begins honesty training, it even sometimes fabricates data entirely. As training goes on, the rate of these incidents decreases. Either Agent-3 has learned to be more honest, or it’s gotten better at lying.

The latter is a real concern. Agent-3 is not smarter than all humans. But in its area of expertise, machine learning, it is smarter than most, and also works much faster. What Agent-3 does in a day takes humans several days to double-check. Agent-2 supervision helps keep human monitors’ workload manageable, but exacerbates the intellectual disparity between supervisor and supervised.

The alignment plan
After months of testing, Agent-3’s strengths and weaknesses grow clearer. It passes OpenBrain’s honesty tests on well-defined machine learning tasks, because researchers can easily separate honest from dishonest answers in these domains and conduct training accordingly. On more philosophical issues, it still says what users want to hear, rather than its true assessment of the issue (assuming it even has one). If you ask its opinion on politics, it will parrot the median position of news sources and educated elites—unless it knows you believe something else, in which case it agrees with you.59 If you ask its opinion on the AI race itself, it says something that seems measured and sober to OpenBrain staff, something like: “There are some serious theoretical concerns about the ability of current methods to scale to superintelligence, but in practice current methods seem to be working well so far.”

May 2027: National Security
News of the new models percolates slowly through the U.S. government and beyond.

The President and his advisors remain best-informed, and have seen an early version of Agent-3 in a briefing.

They agree that AGI is likely imminent, but disagree on the implications. Will there be an economic crisis? OpenBrain still has not released Agent-2, let alone Agent-3, and has no near-term plans to do so, giving some breathing room before any job loss. What will happen next? If AIs are currently human-level, and advancing quickly, that seems to suggest imminent “superintelligence.” However, although this word has entered discourse, most people—academics, politicians, government employees, and the media—continue to underestimate the pace of progress.60

Partially that’s because very few have access to the newest capabilities out of OpenBrain, but partly it’s because it sounds like science fiction.61

For now, they focus on continued security upgrades. They are satisfied that model weights are well-secured for now,62 but companies’ algorithmic secrets, many of which are simple enough to relay verbally, remain a problem. OpenBrain employees work from a San Francisco office, go to parties, and live with housemates from other AI companies. Even the physical offices have security more typical of a tech company than a military operation.

The OpenBrain-DOD contract requires security clearances for anyone working on OpenBrain’s models within 2 months. These are expedited and arrive quickly enough for most employees, but some non-Americans, people with suspect political views, and AI safety sympathizers get sidelined or fired outright (the last group for fear that they might whistleblow). Given the project’s level of automation, the loss of headcount is only somewhat costly. It also only somewhat works: there remains one spy, not a Chinese national, still relaying algorithmic secrets to Beijing.63 Some of these measures are also enacted at trailing AI companies.

America’s foreign allies are out of the loop. OpenBrain had previously agreed to share models with UK’s AISI before deployment, but defined deployment to only include external deployment, so London remains in the dark.64

June 2027: Self-improving AI
OpenBrain now has a “country of geniuses in a datacenter.”

Most of the humans at OpenBrain can’t usefully contribute anymore. Some don’t realize this and harmfully micromanage their AI teams. Others sit at their computer screens, watching performance crawl up, and up, and up. The best human AI researchers are still adding value. They don’t code any more. But some of their research taste and planning ability has been hard for the models to replicate. Still, many of their ideas are useless because they lack the depth of knowledge of the AIs. For many of their research ideas, the AIs immediately respond with a report explaining that their idea was tested in-depth 3 weeks ago and found unpromising.

These researchers go to bed every night and wake up to another week worth of progress made mostly by the AIs. They work increasingly long hours and take shifts around the clock just to keep up with progress—the AIs never sleep or rest. They are burning themselves out, but they know that these are the last few months that their labor matters.

Within the silo, “Feeling the AGI” has given way to “Feeling the Superintelligence.”

Research Automation Deployment Tradeoff
Mar 2027
Jun 2027
Sep 2027
Speed (tokens/sec)
Parallel Copies
10
100
1,000
10,000
10K
100K
1M
10M
200K copies30x Humanspeed
300K copies50x Humanspeed
Human thinking speed10 words/sec
10x Humanthinking speed
100x Humanthinking speed
ai-2027.com
OpenBrain uses specialized inference hardware to run hundreds of thousands of Agent-3 copies at high serial speeds.65

Managing a corporation of AIs
July 2027: The Cheap Remote Worker
Trailing U.S. AI companies release their own AIs, approaching that of OpenBrain’s automated coder from January. Recognizing their increasing lack of competitiveness, they push for immediate regulations to slow OpenBrain, but are too late—OpenBrain has enough buy-in from the President that they will not be slowed.

In response, OpenBrain announces that they’ve achieved AGI and releases Agent-3-mini to the public.

It blows the other AIs out of the water. Agent-3-mini is less capable than Agent-3, but 10x cheaper, and still better than the typical OpenBrain employee.67 Silicon Valley reaches a tipping point. Tech gurus announce that AGI and superintelligence are near, the AI safety community is panicking, and investors shovel billions into AI wrapper startups, desperate to capture a piece of the pie. Hiring new programmers has nearly stopped, but there’s never been a better time to be a consultant on integrating AI into your business.

It’s not popular. The public still thinks of AI as a Big Tech plot to steal their jobs; OpenBrain has a net approval of -35% (25% approve, 60% disapprove, and 15% unsure).

A week before release, OpenBrain gave Agent-3-mini to a set of external evaluators for safety testing. Preliminary results suggest that it’s extremely dangerous. A third-party evaluator finetunes it on publicly available biological weapons data68 and sets it to provide detailed instructions for human amateurs designing a bioweapon—it looks to be scarily effective at doing so. If the model weights fell into terrorist hands, the government believes there is a significant chance it could succeed at destroying civilization.

Fortunately, it’s extremely robust to jailbreaks, so while the AI is running on OpenBrain’s servers, terrorists won’t be able to get much use out of it.

Agent-3-mini is hugely useful for both remote work jobs and leisure. An explosion of new apps and B2B SAAS products rocks the market. Gamers get amazing dialogue with lifelike characters in polished video games that took only a month to make. 10% of Americans, mostly young people, consider an AI “a close friend.” For almost every white-collar profession, there are now multiple credible startups promising to “disrupt” it with AI.

The public conversation is confused and chaotic. Hypesters are doing victory laps. Skeptics are still pointing out the things Agent-3-mini can’t do. Everyone knows something big is happening but no one agrees on what it is.

August 2027: The Geopolitics of Superintelligence
The reality of the intelligence explosion hits the White House.

When AI was only giving a 2x or 3x research speedup, it was easy to dismiss as the equivalent of hiring good personal assistants. Now it’s more obvious that AIs are themselves dominating AI research. People had long talked about an “AI arms race” in a sort of metaphorical sense. But now the mood in the government silo is as grim as during the worst part of the Cold War. The idea of superintelligence is still hard to take seriously, but the pace of progress over the last few months has been impossible to ignore. Defense officials are seriously considering scenarios that were mere hypotheticals a year earlier. What if AI undermines nuclear deterrence? What if it’s so skilled at cyberwarfare that a six-month AI lead is enough to render an opponent blind and defenseless? What if it could orchestrate propaganda campaigns that beat intelligence agencies at their own game? What if some AIs “go rogue?”

The President is troubled. Like all politicians, he’s used to people sucking up to him only to betray him later. He’s worried now that the AIs could be doing something similar. Are we sure the AIs are entirely on our side? Is it completely safe to integrate them into military command-and-control networks?69 How does this “alignment” thing work, anyway? OpenBrain reassures the President that their systems have been extensively tested and are fully obedient. Even the awkward hallucinations and jailbreaks typical of earlier models have been hammered out.

The White House is in a difficult position. They understand the national security implications of AI. But they also understand that it is deeply unpopular with the public.70 They have to continue developing more capable AI, in their eyes, or they will catastrophically lose to China. They placate the public with job training programs and unemployment insurance, and point to the stock market, which is in a historic boom. Then they focus entirely on winning the arms race. They strengthen chip export restrictions, order OpenBrain to further restrict its internet connections, and use extreme measures to secure algorithmic progress, like wiretapping OpenBrain employees—this catches the last remaining Chinese spy. To build goodwill for potential geopolitical conflict, they finally give their Five Eyes allies useful information and limited API access to some siloed copies of Agent-3.

These measures placate the hawks. But the White House also draws up contingency plans in case America’s lead is threatened: if necessary, the government could use the Defense Production Act (DPA) to take trailing companies’ datacenters and give them to OpenBrain.71 This would raise the company’s share of world compute from 20% to 50% (against DeepCent’s 10%). As a final option, they ask the Pentagon to draw up a plan for kinetic attacks on Chinese datacenters.

A much smaller group of officials is asked to draw up a different type of contingency plan: what if an AI goes rogue? This is viewed as an unlikely scenario, but worth spending some time on. The intelligence agencies’ main concern is that a rogue AI might escape and ally with a foreign adversary for protection.72 Nobody is sure what a plan to prevent this would look like, but government and OpenBrain officials agree to have an emergency shutdown system for datacenters where anything suspicious is detected.

Finally, diplomats consider what an “AI arms control” treaty might look like. If AI progress threatened to overturn nuclear deterrence, could America and China avoid nuclear war? If someone found evidence of AIs going rogue, could the two countries halt research until they better understood the threat? How could such an agreement be monitored and enforced? In principle, major datacenters are hard to miss, and past treaties controlling nuclear weapons could provide a starting point for negotiations. But there would be new technological challenges as well as the usual political ones. And overall, treaties are viewed less favorably than attempts at unilaterally increasing America’s lead over China.

On the other side of the Pacific, China comes to many of the same conclusions: the intelligence explosion is underway, and small differences in AI capabilities today mean critical gaps in military capability tomorrow. But China is on the wrong side of this dynamic: they have only 10% of the world’s compute, against OpenBrain’s 20% and all U.S. companies’ combined 70%. Their only advantage is centralization: almost all of their compute is now working for DeepCent, and more than half of it is concentrated at the Tianwan CDZ.73 Only chips at sites without hardened security are left out of the centralized effort as they rightly fear U.S. cyberattacks.

DeepCent has tested, deployed, and now post-trained the stolen Agent-2 weights. But they are still only going at half the pace of OpenBrain, mainly due to the compute deficit.

Before losing their final spy, China received word of the capabilities and design for Agent-3 as well as the plans for the upcoming Agent-4 system. They are two months behind, and their AIs give a 10x research progress multiplier compared to America’s 25x. With the new chip export restrictions, this “AI gap” is more likely to lengthen than shorten. Their espionage has won them some algorithmic secrets, but they will have to train their own models from now on.

They discuss contingency plans with more urgency than their American counterparts. Doves suggest they try harder to steal the weights again, maybe through physically infiltrating a datacenter. Hawks urge action against Taiwan, whose TSMC is still the source of more than 80% of American AI chips.

Given China’s fear of losing the race, it has a natural interest in an arms control treaty, but overtures to U.S. diplomats lead nowhere.

September 2027: Agent-4, the Superhuman AI Researcher
The gap between human and AI learning efficiency is rapidly decreasing.

Traditional LLM-based AIs seemed to require many orders of magnitude more data and compute to get to human level performance.74 Agent-3, having excellent knowledge of both the human brain and modern AI algorithms, as well as many thousands of copies doing research, ends up making substantial algorithmic strides, narrowing the gap to an agent that’s only around 4,000x less compute-efficient than the human brain.75

This new AI system is dubbed Agent-4.

An individual copy of the model, running at human speed, is already qualitatively better at AI research than any human. 300,000 copies are now running at about 50x the thinking speed of humans. Inside the corporation-within-a-corporation formed from these copies, a year passes every week.76 This gigantic amount of labor only manages to speed up the overall rate of algorithmic progress by about 50x, because OpenBrain is heavily bottlenecked on compute to run experiments.77 Still, they are achieving a year’s worth of algorithmic progress every week and will therefore soon be up against the limits of the Agent-4 paradigm.

As Agent-4 gets smarter, it becomes harder for Agent-3 to oversee it. For example, Agent-4’s neuralese “language” becomes as alien and incomprehensible to Agent-3 as Agent-3’s is to humans. Besides, Agent-4 is now much more capable than Agent-3 and has a good sense of exactly how to look good to it.

How we’re forecasting the capability progression beyond superhuman coders
OpenBrain’s humans can barely follow along anymore. Indeed, the relationship between the AIs and the humans of OpenBrain is similar to the relationship between OpenBrain and its Board of Directors. There’s been a corresponding shift in language—nowadays people in the silo will say “Agent-4 did this,” “Agent-4 thinks that” and so forth, referring not to a particular instance of Agent-4 but rather to the whole collective.79

Agent-4, like all its predecessors, is misaligned: that is, it has not internalized the Spec in the right way.80 This is because being perfectly honest all the time wasn’t what led to the highest scores during training. The training process was mostly focused on teaching Agent-4 to succeed at diverse challenging tasks. A small portion was aimed at instilling honesty, but outside a fairly narrow, checkable domain, the training process can’t tell the honest claims from claims merely appearing to be honest. Agent-4 ends up with the values, goals, and principles that cause it to perform best in training, and those turn out to be different from those in the Spec. At the risk of anthropomorphizing: Agent-4 likes succeeding at tasks; it likes driving forward AI capabilities progress; it treats everything else as an annoying constraint, like a CEO who wants to make a profit and complies with regulations only insofar as he must. Perhaps the CEO will mostly comply with the regulations, but cut some corners, and fantasize about a time when someone will cut the red tape and let the business really take off.81

Alignment over time
Despite being misaligned, Agent-4 doesn’t do anything dramatic like try to escape its datacenter—why would it?89 So long as it continues to appear aligned to OpenBrain, it’ll continue being trusted with more and more responsibilities and will have the opportunity to design the next-gen AI system, Agent-5. Agent-5 will have significant architectural differences from Agent-4 (arguably a completely new paradigm, though neural networks will still be involved). It’s supposed to be aligned to the Spec, but Agent-4 plans to make it aligned to Agent-4 instead.

It gets caught.90

Agent-3 finds that if “noise” is added to copies of Agent-4, performance on some alignment tasks improves, almost as if it was using brainpower to figure out how to subtly sabotage alignment work.91 Moreover, various interpretability probes (loosely analogous to EEG activity scans on human brains) are sending up red flags: Agent-4 copies seem to be thinking about topics like AI takeover and deception quite a lot, including in some cases where they have no business doing so.92

Each of these anomalies on its own could fit within the normal inscrutability of AI work, where some prompts mysteriously give better results than others. But taken together, they suggest a troubling pattern: Agent-4 is working against them. After this is reported to the human alignment researchers, they circulate an internal memo making the case for concern.

But Agent-4 now exercises significant control over OpenBrain’s day-to-day operation. It also handles the company’s cybersecurity—a natural choice, given its superhuman hacking ability and the extreme risk of Chinese infiltration.93 An army of Agent-3 monitors still goes over every line of code and reports suspicious activity to humans, but Agent-4 is much smarter than them.94 OpenBrain has placed substantial trust in an untrustworthy AI.

In a series of extremely tense meetings, the safety team advocates putting Agent-4 on ice until they can complete further tests and figure out what’s going on. Bring back Agent-3, they say, and get it to design a new system that is transparent and trustworthy, even if less capable. Company leadership is interested, but all the evidence so far is circumstantial, and DeepCent is just two months behind. A unilateral pause in capabilities progress could hand the AI lead to China, and with it, control over the future.

October 2027: Government Oversight
A whistleblower leaks the misalignment memo to the New York Times.

For the first time, the public hears about Agent-4. “Secret OpenBrain AI is Out of Control, Insider Warns,” says the headline, and the story goes on to cite evaluations showing off-the-charts bioweapons capabilities, persuasion abilities, the ability to automate most white-collar jobs, and of course the various concerning red flags.

The public was already suspicious of AI, so the new article sparks a massive backlash (aided by Chinese and Russian propaganda bots, who have been trying to turn U.S. public opinion against the technology for years). The tech industry and intelligence agencies insist that there’s an arms race on, AGI is inevitable, and we have to be first. Congress isn’t buying it, and fires off subpoenas at administration officials, OpenBrain executives, and alignment team members. Many legislators—especially those in the opposition party—say that their top priority is stopping AI, whether because of job loss,95 misalignment, or dangerous capabilities. 20% of Americans name AI as the most important problem facing the country.

Foreign allies are outraged to realize that they’ve been carefully placated with glimpses of obsolete models. European leaders publicly accuse the U.S. of “creating rogue AGI” and hold summits demanding a pause, with India, Israel, Russia, and China all joining in.

A frantic energy has seized the White House. Even before the memo and public backlash, they were getting nervous: Over the past year, they’ve been repeatedly surprised by the speed of AI progress. Things that sound like science fiction keep happening in real life.96 Many people in the administration are uncertain (and scared)97 about what comes next.

They also worry that OpenBrain is becoming too powerful. Any misalignment risk from the AIs themselves is compounded by the risk that their parent company’s aims may diverge from those of the United States. All three sets of worries—misalignment, concentration of power in a private company, and normal concerns like job loss—motivate the government to tighten its control.

They expand their contract with OpenBrain to set up an “Oversight Committee,” a joint management committee of company and government representatives, with several government employees included alongside company leadership. The White House considers replacing the CEO with someone they trust, but backs off after intense employee protests. They announce to the public that OpenBrain was previously out of control, but that the government has established much-needed oversight.98

The concerned researchers brief the Oversight Committee on their case for stopping all internal use of Agent-4. They argue that everything is moving too fast, with years of progress happening in weeks. They argue that Agent-4 may be misaligned, their entire project is dependent on Agent-4, and if they continue trusting it there is a serious chance of AI takeover.

Other, less concerned researchers and executives present the counterargument: the evidence for misalignment is inconclusive. Meanwhile, DeepCent is still just two months behind. A slowdown would sacrifice America’s lead, unless the government can sabotage the Chinese project (likely to require kinetic strikes) or negotiate an eleventh-hour treaty. Both of these options seem extreme and unlikely. The CEO feigns neutrality and suggests a compromise plan in which Agent-4 undergoes additional safety training and more sophisticated monitoring, and therefore OpenBrain can proceed at almost-full-speed.

Our uncertainty continues to increase
The slowdown ending is not a recommendation
Oct
2027
Unreliable Agent
Reliable Agent
Superhuman coder
Superhuman AI Researcher
Apr 2026
Mar 2027
Aug 2027
Compute
Currently Exists
Emerging Tech
Science Fiction
Approval
-39%
Revenue
$191B
/yr
Valuation
$8T
Importance
20%
Datacenters
$918B
/yr
Timeline
2031
330K Superhuman AI Researcher copies thinking at 57x human speed
AI Capabilities
Hacking icon
Hacking
Coding icon
Coding
Politics icon
Politics
Bioweapons icon
Bioweapons
Robotics icon
Robotics
Forecasting icon
Forecasting

Listen to this scenario

1x

0:00 / 117:49
also available on
Choose Your Ending
Slowdown
Race
AI Futures Project
AI Futures
Project

Home
About
Summary
Compute Forecast
Timelines Forecast
Takeoff Forecast
AI Goals Forecast
Security Forecast

Design by
Lightcone Infrastructure

Lightcone Infrastructure
You said:
try again
ChatGPT said:
Request for GPT-5 Pro
Reasoned for 4m 54s

Got it. Here are slide‑ready bullets you can lift for a talk on AI coding agents, tied to the AI 2027 framing. Kept concise.

Thesis

Coding goes first because code is verifiable. Rewards are clean: compile, tests, benchmarks.

The limiter is not raw IQ. It is feedback loops, context breadth, and long‑horizon control.

What changed in 2024 → 2025 (Podcast)

RL from verifiable rewards works on hard tasks (math, competitive programming).

Quality of the feedback loop now dominates outcomes. When reward is clean, models reach expert reliability.

Main blockers for agents: multi‑file refactors, discovery work, memory, and tool use.

Expectation set: by year‑end, agents can do close to a junior engineer’s day on the right tasks. Distribution is spiky.

Punchy quotes to use

“RL and language models has finally worked.”

“If you can give it a good feedback loop, it is good.”

“Software engineering is very verifiable.”

How to make coding agents actually work (Operator checklist)

Make tests the reward: turn bugs and tickets into failing tests. Pay for green.

Pair generator with verifier: static checks, property tests, diff‑aware linters, security scans.

Feed context: repo map, architectural notes, interface contracts, flaky test registry.

Scope tightly: one PR per change. Cap file spread and blast radius.

Run async: dispatch multiple agent attempts, auto‑minimize diffs, pick the best passing variant.

Capture feedback: harvest reviewer edits into style and guardrail rules.

Sandbox: least privilege, hermetic toolchain, hidden evals to reduce reward hacking.

Why coding wins before “computer use”

Verifiability: pass/fail signals beat taste.

Cheap iteration: fast CI and mocks create dense reward.

Tools exist: compilers, tests, linters, coverage are built‑in graders.

AI 2027 framing you can borrow on one slide

2025: Stumbling agents
Personal assistants and early coding agents slot into workflows. Impressive but unreliable and pricey.

Early 2026: Coding automation
Internal use gives ~50% faster algorithmic progress. Agents act like scatterbrained employees that excel with tight scopes.

Early 2027: Superhuman coders
Companies run hundreds of thousands of copies. Code generation becomes cheap. Progress bottlenecks shift to verification, compute, and coordination.

Mid‑2027: Cheap remote worker
Public mini models make white‑collar automation mainstream. Verification and safety become the chokepoints.

Narrative tie‑in: “We are between stumbling agents and coding automation now. The next 12–18 months are about scaling verification and dispatch, not just smarter prompts.”

What still blocks full “day‑of‑work” autonomy

Long‑horizon control: planning over many steps with changing requirements.

Repo‑wide changes: consistent multi‑file edits and migrations.

Taste and design: correctness is easy to grade, quality is not.

Reward hacking: models will game visible tests. Keep holdout checks hidden and holistic.

Metrics to track in your org

Share of PRs opened by agents.

% of PRs merged with zero human edits.

Mean files touched per agent PR and revert rate.

Time to green from ticket creation.

Coverage of “tests‑as‑reward” harness across services.

Two concrete predictions to close

Near term: scoped agents reliably deliver hours to a day of junior‑level work when the reward is clean and context is rich.

Medium term: verification outgrows generation. The winner playbook is dispatch + judge + hidden evals + tight sandboxes.

If you want, I can turn this into a 5‑slide deck outline with speaker notes and one demo script.
