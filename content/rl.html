<div class="flex flex-col gap-4">
  <div
    data-message-author-role="assistant"
    data-message-id="8e1f2010-142f-4b54-8ff7-522628c251a4"
    dir="auto"
    class="min-h-8 text-message relative flex w-full flex-col items-end gap-2 text-start break-words whitespace-normal [.text-message+&amp;]:mt-5"
  >
    <div class="flex w-full flex-col gap-1 empty:hidden first:pt-[3px]">
      <div
        class="markdown prose dark:prose-invert w-full break-words dark deep-research-result markdown-new-styling"
      >
        <h1 data-start="0" data-end="42">
          Research Directions for AI Coding Agents
        </h1>
        <h2 data-start="44" data-end="94">
          1. Large Language Models Specialized for Code
        </h2>
        <p data-start="95" data-end="1163">
          <strong data-start="95" data-end="110">Background:</strong> In recent
          years, AI models have been trained specifically on source code to
          better assist programming tasks. These code-focused large language
          models (LLMs) can generate code, explain code, and handle
          programming-related queries. For example,
          <strong data-start="354" data-end="372">OpenAI’s GPT-4</strong> (2023)
          demonstrated impressive coding ability, and
          <strong data-start="424" data-end="445">Meta’s Code Llama</strong>
          (2023) was released as an open-source code generation model<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=machine%20learning%20system%20that%20can,natural%20language%20%E2%80%94%20specifically%20English"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=machine%20learning%20system%20that%20can,natural%20language%20%E2%80%94%20specifically%20English"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. Code Llama, built on Meta’s Llama-2, was fine-tuned with 500B
          tokens of code and can support multiple languages like Python, Java,
          C++ and more<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Code%20Llama%2C%20which%20is%20available,model%20like%20Copilot%20could%20produce"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Code%20Llama%2C%20which%20is%20available,model%20like%20Copilot%20could%20produce"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Each%20of%20the%20Code%20Llama,and%20%E2%80%9Csafe%E2%80%9D%20answers%20to%20questions"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Each%20of%20the%20Code%20Llama,and%20%E2%80%9Csafe%E2%80%9D%20answers%20to%20questions"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. Models are also scaling up context lengths — Code Llama’s variants
          accept up to 100,000 tokens of code as input<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Several%20of%20the%20Code%20Llama,the%20largest%20by%20parameter%20count"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Several%20of%20the%20Code%20Llama,the%20largest%20by%20parameter%20count"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >, and Anthropic’s Claude model similarly expanded to a 100k-token
          window (tens of thousands of words)<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Several%20of%20the%20Code%20Llama,the%20largest%20by%20parameter%20count"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Several%20of%20the%20Code%20Llama,the%20largest%20by%20parameter%20count"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. This allows an entire codebase or extensive documentation to be fed
          into the model for analysis or generation.
        </p>
        <p data-start="1165" data-end="2178">
          <strong data-start="1165" data-end="1185">Recent Advances:</strong>
          Open-source models have rapidly improved. Meta’s 34B-parameter Code
          Llama is reported to be the best-performing open model for code to
          date<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Several%20of%20the%20Code%20Llama,the%20largest%20by%20parameter%20count"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Several%20of%20the%20Code%20Llama,the%20largest%20by%20parameter%20count"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >, closing the gap with proprietary models. Likewise, the
          community-driven
          <strong data-start="1436" data-end="1449">StarCoder</strong> and
          <strong data-start="1454" data-end="1468">StableCode</strong> projects
          have released competitive code LLMs<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Akin%20to%20GitHub%20Copilot%20,and%20Bash"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Akin%20to%20GitHub%20Copilot%20,and%20Bash"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. Meanwhile, industry leaders continue to push state-of-the-art
          proprietary models: Google’s
          <strong data-start="1643" data-end="1656">PaLM Code</strong> model
          powers coding features in Google Bard, and the upcoming
          <strong data-start="1719" data-end="1736">Google Gemini</strong>
          (2024) is expected to integrate advanced code generation and tool use.
          The trend of
          <em data-start="1821" data-end="1838">open vs. closed</em> models is
          spurring rapid innovation – Meta’s
          <strong data-start="1884" data-end="1895">Llama 3</strong> and
          <strong data-start="1900" data-end="1911">Llama 4</strong> (released
          in 2024 and 2025) continued the open-release strategy<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://en.wikipedia.org/wiki/Llama_(language_model)#:~:text=%282024,TechCrunch"
                target="_blank"
                rel="noopener"
                alt="https://en.wikipedia.org/wiki/Llama_(language_model)#:~:text=%282024,TechCrunch"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >en.wikipedia.org</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >, giving researchers and engineers more accessible models to
          experiment with. This has fostered an ecosystem where new code models
          and fine-tunes emerge frequently.
        </p>
        <p data-start="2180" data-end="3229">
          <strong data-start="2180" data-end="2211"
            >Open Questions &amp; Next Year:</strong
          >
          Looking ahead, a key research question is how to further improve code
          fluency and correctness without simply making models larger. Can new
          training methods or architectures yield more
          <em data-start="2396" data-end="2406">reliable</em> code generation
          (fewer bugs, better logic) for a given model size? There is also
          interest in
          <strong data-start="2500" data-end="2514">multimodal</strong> code
          agents (e.g. using a diagram or GUI design as input to generate code)
          and <em data-start="2594" data-end="2610">cross-language</em> code
          understanding. With context windows reaching 100k+ tokens, researchers
          are exploring how to efficiently utilize that capacity – for instance,
          retrieving the most relevant parts of a huge project for the model to
          attend to. In the next year, expect to see new model releases (perhaps
          an enhanced GPT-4 or even GPT-5, and open models like Llama 5) with
          improved coding skills. The community will also be watching whether
          these models can handle
          <strong data-start="3059" data-end="3076">full projects</strong> (not
          just snippets) more intelligently, and how techniques like retrieval
          augmentation or better fine-tuning can push performance beyond current
          limits.
        </p>
        <h2 data-start="3231" data-end="3269">
          2. AI Pair Programming Assistants
        </h2>
        <p data-start="3270" data-end="4549">
          <strong data-start="3270" data-end="3285">Background:</strong> AI pair
          programmers are tools that integrate into developers’ workflows to
          provide code suggestions, autocompletions, and even entire functions
          in response to comments or prompts. The pioneer was
          <strong data-start="3482" data-end="3500">GitHub Copilot</strong>
          (launched 2021), built on OpenAI Codex, which introduced the
          experience of an AI “pair programmer” directly in your editor. Since
          then, many similar assistants have appeared:
          <strong data-start="3676" data-end="3700">Amazon CodeWhisperer</strong
          >, <strong data-start="3702" data-end="3713">Tabnine</strong>,
          <strong data-start="3715" data-end="3737">Replit Ghostwriter</strong>,
          and more. These assistants aim to keep developers “in the flow” by
          handling boilerplate and offering solutions for routine tasks. They
          can suggest code as you type or via chat, based on both the current
          file and sometimes broader context. Adoption has been fast – according
          to GitHub, over 400 organizations use Copilot, and developers in those
          orgs report coding
          <strong data-start="4103" data-end="4123">up to 55% faster</strong>
          with AI help<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=GitHub%20claims%20that%20more%20than,increased%20productivity%20and%20faster%20learning"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=GitHub%20claims%20that%20more%20than,increased%20productivity%20and%20faster%20learning"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. In fact, GitHub’s data suggests Copilot is now writing nearly
          <strong data-start="4237" data-end="4264"
            >46% of developers’ code</strong
          >
          on average<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://github.blog/news-insights/product-news/github-copilot-x-the-ai-powered-developer-experience/#:~:text=GitHub%20Copilot%20started%20a%20new,code%20%20and%20helps%2064"
                target="_blank"
                rel="noopener"
                alt="https://github.blog/news-insights/product-news/github-copilot-x-the-ai-powered-developer-experience/#:~:text=GitHub%20Copilot%20started%20a%20new,code%20%20and%20helps%2064"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >github.blog</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. A Stack Overflow developer survey found about
          <strong data-start="4360" data-end="4418"
            >70% of developers are already using or planning to use</strong
          >
          AI coding tools in 2023<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=GitHub%20claims%20that%20more%20than,increased%20productivity%20and%20faster%20learning"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=GitHub%20claims%20that%20more%20than,increased%20productivity%20and%20faster%20learning"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >, showing how quickly this has become mainstream in software teams.
        </p>
        <p data-start="4551" data-end="5649">
          <strong data-start="4551" data-end="4575"
            >Recent Developments:</strong
          >
          The capabilities of these assistants have grown. GitHub announced
          <strong data-start="4642" data-end="4655">Copilot X</strong> in 2023,
          which brings a chat interface and voice commands to the coding
          assistant<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://github.blog/news-insights/product-news/github-copilot-x-the-ai-powered-developer-experience/#:~:text=GitHub%20Copilot%20is%20evolving%20to,a%20more%20personalized%20developer%20experience"
                target="_blank"
                rel="noopener"
                alt="https://github.blog/news-insights/product-news/github-copilot-x-the-ai-powered-developer-experience/#:~:text=GitHub%20Copilot%20is%20evolving%20to,a%20more%20personalized%20developer%20experience"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >github.blog</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. This means a developer can now ask questions in natural language
          (even via voice) right from the IDE and get code answers or
          explanations. Copilot is also being enhanced to handle pull request
          summaries, generate test cases, and answer documentation
          questions<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://github.blog/news-insights/product-news/github-copilot-x-the-ai-powered-developer-experience/#:~:text=GitHub%20Copilot%20is%20evolving%20to,a%20more%20personalized%20developer%20experience"
                target="_blank"
                rel="noopener"
                alt="https://github.blog/news-insights/product-news/github-copilot-x-the-ai-powered-developer-experience/#:~:text=GitHub%20Copilot%20is%20evolving%20to,a%20more%20personalized%20developer%20experience"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >github.blog</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. Competitors are not standing still: Amazon made CodeWhisperer free
          for individual developers and improved its integration with AWS
          tooling.
          <strong data-start="5218" data-end="5258"
            >Visual Studio and Visual Studio Code</strong
          >
          now have built-in AI chat (via Copilot Chat) that can explain code and
          suggest fixes. Companies are also fine-tuning models for specific IDEs
          or tasks; for example,
          <strong data-start="5424" data-end="5437">JetBrains</strong> is
          building AI assistance into IntelliJ-based IDEs. Overall, the product
          landscape is evolving rapidly, often outpacing published research –
          <em data-start="5579" data-end="5634"
            >product announcements frequently lead research papers</em
          >
          in this area.
        </p>
        <p data-start="5651" data-end="6928">
          <strong data-start="5651" data-end="5682"
            >Open Questions &amp; Next Year:</strong
          >
          A major question is how to best measure and
          <strong data-start="5727" data-end="5778"
            >improve developer productivity and satisfaction</strong
          >
          with AI in the loop. Early studies showed big time savings on simple
          tasks<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://arxiv.org/abs/2302.06590#:~:text=,transition%20into%20software%20development%20careers"
                target="_blank"
                rel="noopener"
                alt="https://arxiv.org/abs/2302.06590#:~:text=,transition%20into%20software%20development%20careers"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >arxiv.org</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >, but real-world impact on large projects, code quality, and
          maintenance needs more study. Another challenge is how to make AI
          assistants more aware of project-specific context – current tools
          sometimes struggle with a project’s unique patterns or frameworks. We
          expect to see advances in
          <strong data-start="6181" data-end="6200">IDE integration</strong>: AI
          agents that have deeper access to the project’s repository, issue
          tracker, or runtime environment, enabling more context-aware
          suggestions.
          <strong data-start="6345" data-end="6369"
            >Natural conversation</strong
          >
          with coding assistants will improve (e.g. allowing back-and-forth
          about why a certain suggestion was made or having the AI explain code
          to you on demand). By next year, AI pair programmers may start
          tackling more than just code snippets – perhaps helping with
          architecture decisions or performing code reviews. Key things to
          watch: user studies on long-term effects (Does using Copilot change
          how developers learn or architect software?), and new features like
          <strong data-start="6831" data-end="6865"
            >real-time collaborative coding</strong
          >
          where an AI participates in a coding session alongside humans.
        </p>
        <h2 data-start="6930" data-end="6984">
          3. Autonomous Coding Agents (Auto-GPT and Beyond)
        </h2>
        <p data-start="6985" data-end="8082">
          <strong data-start="6985" data-end="7000">Background:</strong> Beyond
          assisting a human programmer, researchers are exploring
          <em data-start="7064" data-end="7090">autonomous coding agents</em>
          that can take a high-level goal and iteratively plan, write, execute,
          and refine code to accomplish that goal. In early 2023, the
          open-source project
          <strong data-start="7241" data-end="7253">Auto-GPT</strong> (by Toran
          Bruce Richards) gained attention as a prototype of such an agent.
          Auto-GPT uses GPT-4 (and GPT-3.5) to break a user’s objective into
          sub-tasks, generate code for those tasks, run the code, and gather
          feedback, looping until the goal is achieved<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://www.ibm.com/think/topics/autogpt#:~:text=AutoGPT%20is%20an%20open,3.5"
                target="_blank"
                rel="noopener"
                alt="https://www.ibm.com/think/topics/autogpt#:~:text=AutoGPT%20is%20an%20open,3.5"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >ibm.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://www.ibm.com/think/topics/autogpt#:~:text=AutoGPT%20works%20by%20processing%20a,real%20time%20to%20iteratively%20improve"
                target="_blank"
                rel="noopener"
                alt="https://www.ibm.com/think/topics/autogpt#:~:text=AutoGPT%20works%20by%20processing%20a,real%20time%20to%20iteratively%20improve"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >ibm.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. In essence, it attempts to chain many prompt→code→execute cycles
          together without constant human prompts. Auto-GPT also can use
          plugins/tools (for example, accessing the web or files) and has
          short-term and long-term memory (via vector databases) to remember
          past steps<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://www.ibm.com/think/topics/autogpt#:~:text=In%20addition%20to%20GPT,allowing%20it%20to"
                target="_blank"
                rel="noopener"
                alt="https://www.ibm.com/think/topics/autogpt#:~:text=In%20addition%20to%20GPT,allowing%20it%20to"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >ibm.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. This sparked a wave of similar “AI agent” projects – e.g.
          <strong data-start="7956" data-end="7967">BabyAGI</strong>,
          <strong data-start="7969" data-end="7985">GPT-Engineer</strong>, and
          others – aiming to let an AI agent build entire software projects with
          minimal human input.
        </p>
        <p data-start="8084" data-end="9272">
          <strong data-start="8084" data-end="8104">Recent Advances:</strong>
          The craze around Auto-GPT highlighted both the potential and current
          limitations of autonomous coding agents. On the positive side, it
          showed that an agent can
          <em data-start="8265" data-end="8278">self-prompt</em> and handle
          multi-step tasks like creating a simple app by dividing work into
          functions, writing code, testing it, and fixing bugs in a loop.
          Researchers from IBM, Microsoft, and academia have proposed frameworks
          to improve these agents. For example,
          <strong data-start="8529" data-end="8544">Lilian Weng</strong>
          (OpenAI) summarized strategies for tool use and memory in LLM agents,
          and new frameworks like
          <strong data-start="8639" data-end="8654">GPT-4 Tools</strong> and
          <strong data-start="8659" data-end="8679">LangChain Agents</strong>
          make it easier to give an LLM various capabilities (code execution,
          web search, etc.). A noteworthy experimental system,
          <strong data-start="8801" data-end="8819">Voyager (2023)</strong>,
          allowed an agent to autonomously write and execute code to navigate a
          game (Minecraft) world, learning from failures and building a library
          of useful code “skills” as it went. These efforts illustrate progress
          in letting agents handle more open-ended coding challenges. However,
          results also revealed issues: Auto-GPT often gets stuck in loops or
          produces suboptimal solutions without human guidance, indicating
          limited “common sense” or reliability.
        </p>
        <p data-start="9274" data-end="10597">
          <strong data-start="9274" data-end="9305"
            >Open Questions &amp; Next Year:</strong
          >
          Fully autonomous coding agents remain a nascent area with many open
          questions. One big challenge is
          <strong data-start="9406" data-end="9436"
            >reliability and evaluation</strong
          >
          – how do we know an agent’s code is correct and meets the user’s
          intent without constant supervision? Another is
          <strong data-start="9550" data-end="9564">efficiency</strong>: current
          agents can be very slow and resource-intensive, since they may
          generate and execute a lot of code trial-and-error. Researchers are
          investigating how to incorporate better planning algorithms or search
          methods so that agents don’t need to brute-force solutions. We might
          see hybrids that use classical program synthesis or formal methods
          alongside LLMs to verify solutions. It’s also likely we’ll get
          improved agent frameworks with
          <strong data-start="10005" data-end="10022">better memory</strong> (so
          the agent can work on long tasks without forgetting earlier context)
          and guardrails to prevent infinite loops or harmful actions. In the
          coming year, watch for new autonomous agent demos from both industry
          (possibly integrated into IDEs or cloud platforms) and research
          (papers proposing more robust planning+LLM combinations). While we’re
          still far from an AI that you can simply tell “Build me a complex app”
          and it does so perfectly, each incremental advancement (like handling
          bigger tasks or reliably solving programming puzzles) is an important
          research milestone.
        </p>
        <h2 data-start="10599" data-end="10657">
          4. Multi-Agent Collaboration for Software Development
        </h2>
        <p data-start="10658" data-end="11933">
          <strong data-start="10658" data-end="10673">Background:</strong> An
          intriguing research direction is using
          <strong data-start="10716" data-end="10738"
            >multiple AI agents</strong
          >
          with different specialties to collaborate on building software –
          essentially simulating a software team with AI. The idea is that one
          agent alone might not solve a complex project efficiently, but a group
          of agents, each focusing on a role (e.g. brainstorming requirements,
          writing code, testing, fixing bugs), could work together. A recent
          example is
          <strong data-start="11091" data-end="11102">ChatDev</strong>, an
          open-source framework that orchestrates a virtual software company
          composed of specialized agents<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://www.ibm.com/think/topics/chatdev#1597040493#:~:text=ChatDev%20is%20an%20open%20source,and%20produce%20a%20software%20application"
                target="_blank"
                rel="noopener"
                alt="https://www.ibm.com/think/topics/chatdev#1597040493#:~:text=ChatDev%20is%20an%20open%20source,and%20produce%20a%20software%20application"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >ibm.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. In ChatDev, different agents take on roles corresponding to phases
          of the software development lifecycle: one handles planning/design,
          another writes code, another tests the code, and another documents
          it<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://www.ibm.com/think/topics/chatdev#1597040493#:~:text=intelligent%20agents%20holding%20different%20roles,collaboratively%20to%20complete%20each%20phase"
                target="_blank"
                rel="noopener"
                alt="https://www.ibm.com/think/topics/chatdev#1597040493#:~:text=intelligent%20agents%20holding%20different%20roles,collaboratively%20to%20complete%20each%20phase"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >ibm.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. They communicate in a structured way (simulated “meetings” or
          message exchanges) to gradually develop an application. This draws on
          the concept of
          <em data-start="11637" data-end="11662">collective intelligence</em>,
          where a group of agents can outperform a single agent on complex
          tasks<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://www.ibm.com/think/topics/chatdev#1597040493#:~:text=ChatDev%E2%80%99s%20primary%20objectives%20are%20to,entities%20are%20known%20as%20agents"
                target="_blank"
                rel="noopener"
                alt="https://www.ibm.com/think/topics/chatdev#1597040493#:~:text=ChatDev%E2%80%99s%20primary%20objectives%20are%20to,entities%20are%20known%20as%20agents"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >ibm.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. Other projects like
          <strong data-start="11795" data-end="11806">MetaGPT</strong> and
          <strong data-start="11811" data-end="11821">crewAI</strong> have
          similarly explored multi-agent teams working on a shared
          objective<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://www.ibm.com/think/topics/autogpt#:~:text=of%20the%20process%20and%20shape,the%20overall%20task%20workflow"
                target="_blank"
                rel="noopener"
                alt="https://www.ibm.com/think/topics/autogpt#:~:text=of%20the%20process%20and%20shape,the%20overall%20task%20workflow"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >ibm.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >.
        </p>
        <p data-start="11935" data-end="13146">
          <strong data-start="11935" data-end="11955">Recent Advances:</strong>
          Multi-agent systems for coding are still experimental, but initial
          results are promising in showing emergent teamwork. In 2023,
          researchers demonstrated that assigning GPT-4 agents specific personas
          (e.g. Product Manager, Engineer, Tester) and having them follow a
          defined workflow can result in non-trivial software being built with
          minimal human input. For instance, one demo had agents collectively
          develop a simple game from a high-level spec – the “PM” agent outlined
          tasks, the “Engineer” agent wrote code, the “Tester” agent created and
          ran tests, and they iteratively refined the product. IBM’s ChatDev
          implementation showed that agents following the waterfall model phases
          were able to produce a working CRUD application autonomously<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://www.ibm.com/think/topics/chatdev#1597040493#:~:text=ChatDev%20is%20an%20open%20source,and%20produce%20a%20software%20application"
                target="_blank"
                rel="noopener"
                alt="https://www.ibm.com/think/topics/chatdev#1597040493#:~:text=ChatDev%20is%20an%20open%20source,and%20produce%20a%20software%20application"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >ibm.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://www.ibm.com/think/topics/chatdev#1597040493#:~:text=intelligent%20agents%20holding%20different%20roles,collaboratively%20to%20complete%20each%20phase"
                target="_blank"
                rel="noopener"
                alt="https://www.ibm.com/think/topics/chatdev#1597040493#:~:text=intelligent%20agents%20holding%20different%20roles,collaboratively%20to%20complete%20each%20phase"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >ibm.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. Meanwhile, platforms like
          <strong data-start="12804" data-end="12815">AutoGen</strong> from
          Microsoft Research provide infrastructure for agents to message each
          other and collaborate on tasks (including coding tasks). This
          multi-agent approach also allows
          <em data-start="12985" data-end="12997">modularity</em>: one could
          plug in a stronger coding model for the “coder” agent and a
          specialized testing model for the “tester” agent to improve overall
          results.
        </p>
        <p data-start="13148" data-end="14504">
          <strong data-start="13148" data-end="13179"
            >Open Questions &amp; Next Year:</strong
          >
          While multi-agent collaboration is exciting, there are many questions
          on coordination and communication. How do we prevent agents from
          misunderstanding each other or going in circles in a discussion?
          Ensuring that agents stay on task and produce coherent outcomes is
          non-trivial (much like managing a human team!). Research is needed on
          protocols for agent communication and
          <strong data-start="13555" data-end="13574">decision-making</strong> –
          one agent might need the authority to accept or reject another’s
          suggestion (akin to a lead engineer role). There’s also the cost
          aspect: running multiple large models in parallel can be expensive, so
          finding efficient ways for them to interact (maybe using smaller
          models or only calling agents when needed) will be important. In the
          next year, look out for research that measures whether multi-agent
          systems truly outperform a single, more powerful agent on coding
          problems. We may also see the first
          <strong data-start="14080" data-end="14094">commercial</strong>
          attempts at multi-agent dev teams (perhaps as an extension to existing
          AI coding assistants, where you can spin up a “reviewer agent” to
          check the code written by a “coding agent”). If successful,
          multi-agent approaches could open a path to tackling very complex
          projects by decomposing them among specialized AIs – but developing
          trust and verification mechanisms between agents will be a key
          research focus.
        </p>
        <h2 data-start="14506" data-end="14561">
          5. Natural Language Program Synthesis (NL-to-Code)
        </h2>
        <p data-start="14562" data-end="15809">
          <strong data-start="14562" data-end="14577">Background:</strong>
          Turning a natural language description into working code – often
          called
          <em data-start="14650" data-end="14669">program synthesis</em> – has
          long been a goal of AI. This goes beyond completing a few lines of
          code; it means writing an entire program to specification, potentially
          from scratch. Traditional program synthesis techniques from the 2010s
          (like Microsoft’s PROSE or various inductive programming methods)
          required formal specifications or input-output examples. Today’s
          LLM-based approaches can take an informal problem description
          (“Compute the Fibonacci sequence up to N”) and generate code that
          attempts to solve it. Models like OpenAI Codex and GPT-4 have made
          NL-to-code much more feasible, often solving simple coding problems
          described in plain English. For instance, GPT-4 can parse a
          leetcode-style question and produce a complete solution in Python,
          often correctly.
          <strong data-start="15422" data-end="15435">AlphaCode</strong>
          (DeepMind, 2022) demonstrated that generating a large number of
          candidate programs and filtering them can solve about 30% of
          competitive programming problems, approaching median human performance
          on those challenges. This was a milestone showing that, given a
          well-specified problem, AI can synthesize non-trivial algorithms in
          code<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://www.qodo.ai/blog/gpt-4-vs-alphacode/#:~:text=GPT,percentile%20from%20the%20bottom"
                target="_blank"
                rel="noopener"
                alt="https://www.qodo.ai/blog/gpt-4-vs-alphacode/#:~:text=GPT,percentile%20from%20the%20bottom"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >qodo.ai</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >.
        </p>
        <p data-start="15811" data-end="17550">
          <strong data-start="15811" data-end="15831">Recent Advances:</strong>
          Over the last year, there have been improvements in both the
          <strong data-start="15893" data-end="15905">accuracy</strong> of
          NL-to-code and the
          <strong data-start="15928" data-end="15937">scope</strong> of problems
          that can be tackled. Models have gotten better at following complex
          instructions – instruction-tuned code models (like
          <strong data-start="16069" data-end="16092"
            >Code Llama-Instruct</strong
          >
          and <strong data-start="16097" data-end="16112">WizardCoder</strong>)
          are specifically trained to adhere to natural language prompts
          describing a task. Research has introduced techniques like
          <strong data-start="16236" data-end="16266"
            >chain-of-thought prompting</strong
          >, where the model is guided to break down the problem and perhaps
          outline a solution approach in pseudo-code or comments before writing
          final code. This often leads to higher success rates on tricky
          problems. Additionally, some systems now combine retrieval with
          synthesis: if the task relates to a known API or algorithm, the model
          can fetch relevant documentation or code snippets from a knowledge
          base and incorporate that into the generated program. An example is
          OpenAI’s function-calling and tools mechanism, which could let a model
          call a documentation lookup function when needed. On the evaluation
          side, new benchmarks (like HumanEval-X, MBPP, and CodeContests) have
          expanded the diversity of natural language tasks used to assess code
          generation. These help identify where models still fall short (e.g.
          tasks involving complex reasoning or domain-specific knowledge). Early
          2025 saw an MIT paper highlighting that beyond toy problems, real
          software tasks involve aspects like UI design, data management, and
          performance tuning, which are not yet captured in most NL-to-code
          evaluations<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Imagine%20a%20future%20where%20artificial,day%20challenges"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Imagine%20a%20future%20where%20artificial,day%20challenges"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=while%20routine%20work%20is%20automated"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=while%20routine%20work%20is%20automated"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. This suggests that
          <strong data-start="17460" data-end="17495"
            >context and higher-level design</strong
          >
          need to be integrated into future synthesis research.
        </p>
        <p data-start="17552" data-end="19076">
          <strong data-start="17552" data-end="17583"
            >Open Questions &amp; Next Year:</strong
          >
          A fundamental open question is how to ensure
          <strong data-start="17629" data-end="17644">correctness</strong> of
          synthesized programs. Models can and do generate code that looks
          plausible but has subtle bugs or doesn’t handle edge cases. Techniques
          like incorporating formal verification or generating exhaustive tests
          alongside the code are being explored. Another question is how to move
          beyond small, self-contained problems to synthesizing
          <em data-start="17979" data-end="18011"
            >modular, maintainable software</em
          >. For example, can an AI not only solve a coding puzzle but also
          scaffold an entire multi-file project given a high-level spec?
          Progress here might require combining synthesis with planning (as in
          autonomous agents) or using feedback loops (execute tests, debug,
          refine – which blurs into the next topic of self-debugging AI). In the
          coming year, we expect to see models further improved on benchmark
          performance (possibly reaching superhuman levels on some coding
          challenge suites). There may also be more
          <strong data-start="18518" data-end="18547"
            >domain-specific synthesis</strong
          >
          breakthroughs – for instance, AI that can generate SQL queries or data
          pipelines from plain English (already emerging in products like
          Microsoft’s Power Platform “GPT for BI”). Additionally, integrating
          NL-to-code with
          <strong data-start="18767" data-end="18783">UI/UX design</strong> is
          an exciting frontier: startups are working on allowing users to say
          “Build me a webpage that looks like this sketch,” and having the AI
          produce the code. By next year, rudimentary versions of
          “design-to-code” might appear in web development tools, powered by
          advances in NL-to-code models.
        </p>
        <h2 data-start="19078" data-end="19130">
          6. AI-Driven Debugging and Automated Bug Fixing
        </h2>
        <p data-start="19131" data-end="20457">
          <strong data-start="19131" data-end="19146">Background:</strong>
          Debugging and bug fixing are tedious but critical parts of coding. An
          emerging research area is using AI to automatically identify and fix
          bugs in code, either by analyzing error messages and code, or by
          trial-and-error. Traditional
          <strong data-start="19380" data-end="19414"
            >automated program repair (APR)</strong
          >
          techniques (pre-LLM era) used methods like search and transformation
          rules to patch bugs, but they often were limited to specific types of
          errors. LLMs have changed the game by bringing a general understanding
          of code and intent, enabling a new wave of learning-based bug fixing.
          For example, if given a piece of code and a failing test or error
          message, an LLM can propose a change that might fix the issue. Recent
          studies have shown that LLM-based program repair techniques can
          <strong data-start="19895" data-end="19930"
            >outperform many classic methods</strong
          >, achieving state-of-the-art success rates on bug-fixing
          benchmarks<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://arxiv.org/html/2506.23749v1#:~:text=DeepSeek,2024"
                target="_blank"
                rel="noopener"
                alt="https://arxiv.org/html/2506.23749v1#:~:text=DeepSeek,2024"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >arxiv.org</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. This suggests that a well-tuned code model can correct a wide range
          of errors, essentially learning from patterns of fixes in training
          data. Some early systems (e.g.
          <strong data-start="20204" data-end="20239"
            >OpenAI’s “self-refine” approach</strong
          >) allow the model to critique its own output and iterate: the model
          generates code, observes it doesn’t pass tests, and then adjusts the
          code in a second attempt. Such feedback loops mimic how a developer
          would debug.
        </p>
        <p data-start="20459" data-end="21791">
          <strong data-start="20459" data-end="20479">Recent Advances:</strong>
          Over 2024-2025, we’ve seen both research prototypes and product
          features targeting AI-assisted bug fixing. GitHub’s Copilot, for
          instance, introduced a
          <strong data-start="20632" data-end="20649">“Copilot Fix”</strong>
          feature that can suggest fixes for compilation errors or failing tests
          by analyzing the error and the code context. In the research domain,
          papers have explored letting an AI agent localized the bug (find the
          faulty line) and then apply a targeted fix. One study created an
          <strong data-start="20924" data-end="20968"
            >LLM-based agent for automated bug fixing</strong
          >
          which loops through phases: locate potential bug, propose a patch, run
          tests, and refine. The results show significant improvements over
          single-pass fixes<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://arxiv.org/html/2411.10213v1#:~:text=An%20Empirical%20Study%20on%20LLM,2023%3B%20Zhang%20et%20al"
                target="_blank"
                rel="noopener"
                alt="https://arxiv.org/html/2411.10213v1#:~:text=An%20Empirical%20Study%20on%20LLM,2023%3B%20Zhang%20et%20al"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >arxiv.org</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. Importantly, these LLM-driven approaches leverage the model’s
          knowledge of common bug patterns – e.g., if a function might throw a
          null pointer exception, the model often knows a check is needed even
          without being explicitly told. Another advancement is the integration
          of static analysis tools with LLMs: an AI can use a static analyzer
          output to pinpoint issues (like a security vulnerability) and then
          craft a fix for it. Companies like DeepCode (acquired by Snyk) and
          Amazon (CodeGuru reviewer) are infusing AI into automated code review,
          which overlaps with bug-fixing by flagging problematic code and
          suggesting changes.
        </p>
        <p data-start="21793" data-end="23392">
          <strong data-start="21793" data-end="21824"
            >Open Questions &amp; Next Year:</strong
          >
          Despite progress,
          <em data-start="21843" data-end="21853">reliable</em> automated bug
          repair remains challenging. One open question is how to ensure the
          fixes don’t introduce new bugs – a fix might satisfy the given test
          case but break something else. This calls for more sophisticated
          validation, perhaps using formal methods or multiple test scenarios.
          There’s also the issue of understanding intent: sometimes code is
          “buggy” because the requirements are misunderstood; an AI fixer might
          need clarity on what the code
          <em data-start="22303" data-end="22311">should</em> do. Future
          research may involve AI systems that engage in clarification (asking
          the developer a question if the intended behavior is ambiguous) before
          applying a fix<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=developers%20risk%20blindly%20trusting%20hallucinated,%E2%80%9D"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=developers%20risk%20blindly%20trusting%20hallucinated,%E2%80%9D"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. In the coming year, watch for better
          <strong data-start="22555" data-end="22579"
            >debugging assistants</strong
          >
          integrated in IDEs: imagine highlighting a block of code and asking
          the AI “why is this bug happening?” and it not only explains but
          offers a corrected code snippet. We may also see AI suggesting fixes
          in pull request reviews more frequently, effectively doing part of a
          code reviewer’s job. Another trend to look out for is
          <strong data-start="22905" data-end="22935"
            >learning from runtime data</strong
          >: AI debuggers that can observe program traces or logs and identify
          anomalies that point to a bug. As production monitoring tools
          integrate with LLMs, an AI agent might one day detect an issue in a
          live system and open a pull request with a proposed fix automatically.
          Next year’s advances will likely focus on specific domains (e.g. AI
          fixing security bugs, see next topic) and improving the precision of
          fixes so that developers gain trust in these tools.
        </p>
        <h2 data-start="23394" data-end="23442">
          7. AI for Software Testing and Verification
        </h2>
        <p data-start="23443" data-end="24708">
          <strong data-start="23443" data-end="23458">Background:</strong>
          Testing and verification ensure that code works as intended and is
          free of bugs, but writing good tests and proving correctness can be
          labor-intensive. AI is being used to automate parts of this process.
          One aspect is
          <strong data-start="23677" data-end="23701"
            >test case generation</strong
          >: given some code or a specification, an AI can generate inputs or
          scenarios to try to break the code. LLMs can produce unit test code
          (in frameworks like JUnit or PyTest) from function descriptions or
          even from the code itself by guessing likely edge cases. Another
          aspect is using AI to assist formal verification – for example, to
          help write specifications or invariants that a theorem prover could
          check. While pure formal methods are rigorous, they often require
          human expertise to set up; AI might lower that barrier by translating
          natural language requirements into formal specs. Even without full
          formal verification, AI can help with static analysis outputs: large
          language models can examine a static analysis report (with a list of
          potential issues) and summarize the most critical ones or suggest how
          to fix them. This ties into making testing more intelligent: instead
          of brute-force fuzz testing, an AI could direct the fuzzing towards
          likely problem areas it “suspects” from reading the code.
        </p>
        <p data-start="24710" data-end="25873">
          <strong data-start="24710" data-end="24730">Recent Advances:</strong>
          In practice, one of the first widely used AI testing aids has been
          <strong data-start="24798" data-end="24828"
            >“code coverage” assistants</strong
          >
          – for example, Microsoft’s AI can look at your codebase and suggest
          which parts lack unit tests, then generate those tests. There have
          been research prototypes where GPT-4 was asked to generate test cases
          that would cause a given piece of code to fail; interestingly, the AI
          often invents edge-case inputs a human might miss. On the verification
          side, tools like
          <strong data-start="25192" data-end="25210">OpenAI’s Codex</strong>
          have been used in experiments to derive loop invariants or to assist
          in proving properties of code by generating assertions. We also saw
          the emergence of
          <strong data-start="25365" data-end="25399"
            >property-based testing with AI</strong
          >, where the model suggests general properties (e.g., a sort function
          should preserve length and ordering) and then the testing framework
          checks them. Companies are also focusing on the DevOps angle –
          <strong data-start="25599" data-end="25640"
            >continuous integration (CI) pipelines</strong
          >
          are being augmented with AI that can triage failing tests. For
          instance, when a test fails in CI, an AI might analyze the error,
          pinpoint whether the failure is due to a recent commit, and even
          suggest a fix or create a bug report.
        </p>
        <p data-start="25875" data-end="27494">
          <strong data-start="25875" data-end="25906"
            >Open Questions &amp; Next Year:</strong
          >
          A central question is how much we can trust AI-generated tests. Do
          they truly improve coverage of
          <em data-start="26005" data-end="26017">meaningful</em> scenarios, or
          do they mostly produce trivial or redundant tests? Ensuring
          <strong data-start="26092" data-end="26108">test quality</strong> is
          an open challenge – we might need AI to also critique or reduce its
          own generated tests (for example, merging similar tests or removing
          ones that don’t add new coverage). Another issue:
          <strong data-start="26298" data-end="26316">specifications</strong>.
          For verification, the adage “garbage in, garbage out” applies; if the
          spec or property the AI works with is wrong or incomplete, the
          verification is meaningless. There’s ongoing research on turning
          informal requirements into formal ones – maybe next year we’ll see
          LLMs reliably translate user stories into something like TLA+ or Z
          specifications which can then be model-checked. In terms of upcoming
          developments: expect more AI assistance integrated in testing
          frameworks (maybe JUnitGPT? 😊). Tools might appear that watch how you
          run a program and then suggest corner cases to test based on unusual
          execution paths observed. We might also see progress in
          <strong data-start="26977" data-end="27003"
            >AI-driven fuzz testing</strong
          >, where the fuzzing input generator is guided by an AI’s
          understanding of code semantics rather than random. Finally, as safety
          and reliability of AI-generated code is under scrutiny, the idea of
          <em data-start="27199" data-end="27226">verifying AI-written code</em>
          before deployment will gain traction. Next year, it won’t be
          surprising if major tech companies announce AI services specifically
          aimed at ensuring that code (especially AI-generated code) meets
          certain safety and security bars through automated testing and
          analysis.
        </p>
        <h2 data-start="27496" data-end="27548">
          8. Intelligent Code Refactoring and Maintenance
        </h2>
        <p data-start="27549" data-end="28776">
          <strong data-start="27549" data-end="27564">Background:</strong> A
          large portion of software engineering is not writing brand new code,
          but maintaining and improving existing codebases – tasks like
          refactoring (restructuring code without changing behavior), updating
          dependencies, rewriting portions in a new language, or improving code
          readability. AI can potentially assist in these
          <em data-start="27885" data-end="27898">maintenance</em> tasks. One
          scenario is
          <strong data-start="27922" data-end="27947"
            >automated refactoring</strong
          >: e.g., an AI could take a monolithic function and break it into
          well-named smaller functions, or convert a block of procedural code
          into a more modern object-oriented structure. Another scenario is
          migrating legacy code – for instance, converting code from an older
          language (COBOL, Fortran) to a modern language, or from one framework
          to another. Historically, automated converters exist but often produce
          non-idiomatic output; an LLM, with knowledge of both source and target
          language idioms, could do a better job. In fact, models have been
          trained for code translation (Facebook’s TransCoder in 2020 translated
          between C++, Java, Python using unsupervised learning). Now with
          larger models, we have
          <strong data-start="28651" data-end="28676"
            >GPT-style translators</strong
          >
          that can handle, say, converting Java code to C# or Python fairly
          fluently via prompt instructions.
        </p>
        <p data-start="28778" data-end="30303">
          <strong data-start="28778" data-end="28798">Recent Advances:</strong>
          One very topical example is
          <strong data-start="28827" data-end="28865"
            >IBM’s Watsonx Code Assistant for Z</strong
          >, announced in late 2023, which is designed to help translate COBOL
          code on mainframes into Java using generative AI<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://newsroom.ibm.com/2023-10-26-IBM-Launches-watsonx-Code-Assistant,-Delivers-Generative-AI-powered-Code-Generation-Capabilities-Built-for-Enterprise-Application-Modernization#:~:text=IBM%20Launches%20watsonx%20Code%20Assistant%2C,developer%20productivity%20on%20the%20platform"
                target="_blank"
                rel="noopener"
                alt="https://newsroom.ibm.com/2023-10-26-IBM-Launches-watsonx-Code-Assistant,-Delivers-Generative-AI-powered-Code-Generation-Capabilities-Built-for-Enterprise-Application-Modernization#:~:text=IBM%20Launches%20watsonx%20Code%20Assistant%2C,developer%20productivity%20on%20the%20platform"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >newsroom.ibm.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. The goal is to accelerate legacy modernization by letting AI handle
          the rote conversion and humans fine-tune the results. Early demos show
          the AI can output Java code that preserves the logic of the COBOL
          program while making use of modern libraries – a task that normally
          requires understanding two very different languages. In everyday
          refactoring, tools like
          <strong data-start="29384" data-end="29401">IntelliJ IDEA</strong>
          have started integrating AI to suggest improvements (beyond the
          classic static linter suggestions). For example, an AI might detect a
          code smell (like duplicate code) and proactively suggest refactoring
          into a single method. Another advancement is using AI for
          <strong data-start="29663" data-end="29699"
            >documentation-driven refactoring</strong
          >: analyzing code comments or docs that say one thing, and the code
          does something slightly different, and then updating the code or docs
          for consistency (i.e., ensuring the implementation matches the
          intended design). Moreover, companies are exploring AI to handle
          large-scale changes, such as “please upgrade our codebase to Python 3”
          – something that involves many small edits across thousands of files.
          Microsoft’s research has prototypes where you describe a refactoring
          (like “replace deprecated function X with Y across the project”) and
          the AI generates a batch of pull requests accomplishing it.
        </p>
        <p data-start="30305" data-end="32094">
          <strong data-start="30305" data-end="30336"
            >Open Questions &amp; Next Year:</strong
          >
          Key questions here involve
          <strong data-start="30364" data-end="30390"
            >trust and verification</strong
          >. Developers are understandably cautious about letting an AI rewrite
          a bunch of their code. How do we ensure the refactored code still
          passes all tests and doesn’t introduce subtle differences? Likely
          we’ll see AI tightly coupled with test suites – the AI’s refactoring
          suggestions may be validated by running tests in the loop (possibly an
          AI agent that refactors, runs tests, and only finalizes changes if
          tests pass). Another question is how to handle the scale: one AI
          suggestion might touch hundreds of files (imagine renaming a
          widely-used API); presenting that to a developer for review is
          challenging. Human-in-the-loop approaches will be important, perhaps
          grouping similar changes and allowing batch acceptance. In 2024-2025,
          one thing to watch is
          <strong data-start="31148" data-end="31192"
            >AI-assisted code reviews for refactoring</strong
          >
          – the AI might not directly make all changes, but it could review a
          pull request and highlight areas that could be simplified or
          modernized. I suspect we’ll also see AI integrated into
          <strong data-start="31378" data-end="31401"
            >IDE code navigation</strong
          >: as you explore a legacy codebase, you could ask “Can you refactor
          this class to use pattern X instead?” and get a quick preview diff
          from the AI. Another emerging angle is
          <strong data-start="31575" data-end="31606"
            >semantic search and replace</strong
          >: using AI to find all instances of a
          <em data-start="31644" data-end="31653">pattern</em> (like
          “inefficient looping over a list”) and replace them with a better
          construct. By next year, development tools will likely offer some form
          of “AI Fix It” button for common refactors or upgrades, with the
          expectation that a developer will review the diff. The role of
          research will be to ensure those transformations are
          semantics-preserving and to quantify the improvement (in performance,
          readability, etc.) from AI-driven maintenance.
        </p>
        <h2 data-start="32096" data-end="32152">
          9. Code Optimization and Performance Tuning with AI
        </h2>
        <p data-start="32153" data-end="33292">
          <strong data-start="32153" data-end="32168">Background:</strong>
          Optimizing code for performance (speed, memory, efficiency) is a niche
          but important area, especially in high-performance computing, embedded
          systems, or large-scale web services. Traditionally, performance
          tuning requires expertise and sometimes tedious trial-and-error (e.g.,
          trying different algorithms, using better data structures, or tweaking
          low-level details like loop unrolling or parallelization). AI offers
          new tools to approach this. One approach is using machine learning to
          guide compiler optimizations – for example, deciding the best
          optimization flags or ordering for a specific program (Google’s MLGO
          project did this by training ML models to make compiler inlining
          decisions). Another approach is having LLMs suggest higher-level
          improvements: given a piece of code, an AI can propose changes that
          make it more efficient, such as using a set instead of a list for
          lookups, or using vectorized operations in numpy for Python code. Some
          early users have even asked ChatGPT to
          <em data-start="33162" data-end="33188">analyze profiling output</em>
          (like a function’s runtime cost) and suggest optimizations –
          effectively an AI performance consultant.
        </p>
        <p data-start="33294" data-end="34518">
          <strong data-start="33294" data-end="33314">Recent Advances:</strong>
          In 2023, OpenAI’s Code Interpreter (now part of ChatGPT Advanced Data
          Analysis) demonstrated a capability to not just write code but execute
          it and observe results. This means an AI can iteratively improve code:
          run a snippet, see that it took X seconds, modify the code to try to
          make it faster, and test again. This kind of closed-loop optimization,
          albeit basic, is a new development. We’ve also seen AI applied to
          <strong data-start="33733" data-end="33756"
            >compiler generation</strong
          >: LLVM, a popular compiler infrastructure, has many
          passes/optimizations – researchers have tried using reinforcement
          learning and now LLMs to create new optimization passes or to decide
          which passes to apply for a given program. Another interesting area is
          <strong data-start="34014" data-end="34036"
            >query optimization</strong
          >
          in databases using AI, which parallels code optimization (the SQL
          query plan is like code that needs optimizing). On the industry side,
          tools like
          <strong data-start="34184" data-end="34214"
            >DeepMind’s AlphaDev (2023)</strong
          >
          used an AI (reinforcement learning) to discover new sorting algorithms
          in assembly that were more efficient than known ones – essentially
          optimizing at the instruction level in a way humans hadn’t. This was a
          breakthrough showing AI can uncover performance improvements even in
          well-trodden algorithms.
        </p>
        <p data-start="34520" data-end="36447">
          <strong data-start="34520" data-end="34551"
            >Open Questions &amp; Next Year:</strong
          >
          A big question is how to integrate AI optimizations into the developer
          workflow safely. Performance improvements often come at the cost of
          code complexity or readability. If an AI suggests a micro-optimized
          bit-twiddling hack, is that desirable? Perhaps the AI needs to justify
          optimizations or keep them within certain constraints (e.g., not
          trading much clarity for a small speed gain). There’s also the
          question of
          <strong data-start="34970" data-end="34986">verification</strong>: if
          an AI generates a novel optimized algorithm, we need to be sure it’s
          correct. Formal verification might come into play to prove the
          optimized version is functionally equivalent to the original. Another
          challenge is generality: AI might overfit an optimization to a
          specific input or environment – ensuring robustness is key. In the
          coming year, I expect more AI integration in performance profiling
          tools. Imagine running a profiler on your app and then having an
          “Optimize” button powered by AI that targets the hotspot it found.
          This might be initially limited to suggesting obvious fixes (like use
          caching here, or use a more efficient library function). Additionally,
          as hardware evolves (specialized accelerators, etc.), AI could help
          adapt code to make best use of those (for example, automatically
          parallelizing code for a GPU). Research might also explore using LLMs
          to
          <strong data-start="35869" data-end="35900"
            >learn from performance data</strong
          >: feed an LLM many code examples and their benchmarks, and let it
          derive rules of thumb. One thing to watch: any sign of an AI
          discovering an algorithmic improvement (like AlphaDev did for sorting)
          for other problems – that would be huge, essentially AI contributing
          new knowledge in computer science. While that’s speculative, the
          groundwork of using AI for low-level optimization is being laid now.
          At the very least, routine performance tuning could become less of a
          dark art and more of a collaborative process with AI assistants by
          next year.
        </p>
        <h2 data-start="36449" data-end="36513">
          10. Tool-Integrated Code Generation (Agents that Use Tools)
        </h2>
        <p data-start="36514" data-end="37572">
          <strong data-start="36514" data-end="36529">Background:</strong> Human
          programmers rely on a rich set of tools – compilers, debuggers,
          linters, search engines, documentation, version control, etc. A very
          active research and engineering area is enabling AI coding agents to
          similarly
          <strong data-start="36748" data-end="36770"
            >use external tools</strong
          >
          to enhance their capabilities. For instance, an AI that can call a
          compiler or run a piece of code can verify that its output actually
          works, making it far more reliable than one that only predicts code
          from patterns. Another example is an AI that can query a documentation
          API or a knowledge base (Stack Overflow, etc.) when it encounters an
          unfamiliar library call. The idea of tool-use has been explored under
          various frameworks like
          <strong data-start="37208" data-end="37230"
            >ReAct (Reason+Act)</strong
          >
          and the
          <strong data-start="37239" data-end="37254">MRKL system</strong
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://lilianweng.github.io/posts/2023-06-23-agent/#:~:text=MRKL%20%28Karpas%20et%20al,calculator%2C%20currency%20converter%2C%20weather%20API"
                target="_blank"
                rel="noopener"
                alt="https://lilianweng.github.io/posts/2023-06-23-agent/#:~:text=MRKL%20%28Karpas%20et%20al,calculator%2C%20currency%20converter%2C%20weather%20API"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >lilianweng.github.io</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >, where the LLM decides when to invoke an external module (e.g., a
          calculator or API) to get precise results. In the coding realm, this
          means an AI could, say, call a unit test runner tool to verify the
          code it wrote, or call a static analysis tool to check for
          vulnerabilities.
        </p>
        <p data-start="37574" data-end="39211">
          <strong data-start="37574" data-end="37594">Recent Advances:</strong>
          One major milestone was
          <strong data-start="37619" data-end="37676"
            >OpenAI’s introduction of function calling (June 2023)</strong
          >
          in their API<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://openai.com/index/function-calling-and-other-api-updates/#:~:text=Developers%20can%20now%20describe%20functions,with%20external%20tools%20and%20APIs"
                target="_blank"
                rel="noopener"
                alt="https://openai.com/index/function-calling-and-other-api-updates/#:~:text=Developers%20can%20now%20describe%20functions,with%20external%20tools%20and%20APIs"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >openai.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. This allowed developers to define tools (functions) that an LLM can
          call with arguments; the model was trained to output a JSON object to
          invoke a function when appropriate. With this, developers built
          systems where the AI could, for example, call a
          <code data-start="37980" data-end="37998">run_python(code)</code>
          function to execute generated code and get the output, or call a
          <code data-start="38064" data-end="38083">web_search(query)</code>
          function to fetch information. This made coding agents much more
          powerful – they are no longer limited to their internal knowledge. For
          example, a function-calling based agent can write a snippet of code,
          run it to see if it works, and then correct itself (an implementation
          of the “execute-and-fix” strategy). Frameworks like
          <strong data-start="38411" data-end="38424">LangChain</strong> and
          <strong data-start="38429" data-end="38443">LlamaIndex</strong> became
          popular to simplify wiring LLMs up with tools (databases, web
          browsers, etc.). Meanwhile, other AI providers like
          <strong data-start="38565" data-end="38578">Anthropic</strong> and
          <strong data-start="38583" data-end="38593">Google</strong> have been
          adding tool-use features: Anthropic’s Claude has an “assistant+tools”
          mode, and Google’s Bard can directly execute code or use Google search
          when needed. Research prototypes have shown that tool-using agents
          solve problems more accurately – for instance, solving math-oriented
          coding problems by invoking a calculator for complex arithmetic,
          rather than risking a mistake. The MIT CSAIL study in 2025 emphasized
          that current AI models don’t effectively use the
          <strong data-start="39065" data-end="39112"
            >“wider suite of software engineering tools”</strong
          >
          developers use, and bridging this gap is a key direction<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=extends%20to%20the%20AI%E2%80%99s%20ability,%E2%80%9D"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=extends%20to%20the%20AI%E2%80%99s%20ability,%E2%80%9D"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >.
        </p>
        <p data-start="39213" data-end="41238">
          <strong data-start="39213" data-end="39244"
            >Open Questions &amp; Next Year:</strong
          >
          Designing AI that knows
          <strong data-start="39269" data-end="39285">when and how</strong> to
          use a tool is still tricky. OpenAI’s function calling was a big step,
          but it relies on the model’s prompt training to recognize the need.
          There are cases where the AI doesn’t realize a tool would help, or it
          might misuse a tool (passing incorrect parameters, etc.). Research is
          ongoing on making tool use more robust, possibly by training models
          with explicit tool-use demonstrations or having meta-reasoners oversee
          the process. Another question is how to sandbox tool usage safely.
          When an AI can run code on a machine, you want to ensure it doesn’t
          accidentally (or intentionally, if prompted maliciously) do something
          harmful. So securing the execution environment and setting limits is
          crucial (for example, OpenAI’s Code Interpreter runs in a sandbox with
          no internet access for that reason). Next year, we anticipate more
          <strong data-start="40118" data-end="40154"
            >out-of-the-box tool integrations</strong
          >: models that come with built-in abilities like “write and run code,
          and debug if needed” as a single package. It’s likely that IDEs and
          developer platforms will offer AI agents that can automatically open
          relevant documentation, perform database queries, or spin up test
          environments as part of answering a developer’s query. One challenge
          that might see progress is integration with
          <strong data-start="40539" data-end="40552">debuggers</strong>: an AI
          that not only runs code, but can set breakpoints and inspect variables
          to figure out what’s going wrong. The MIT paper called out that we
          need channels for AI to expose uncertainty or ask for guidance<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=engineering%20tools%2C%20from%20debuggers%20to,%E2%80%9D"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=engineering%20tools%2C%20from%20debuggers%20to,%E2%80%9D"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=what%20the%20model%20writes%2C%E2%80%9D%20he,%E2%80%9D"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=what%20the%20model%20writes%2C%E2%80%9D%20he,%E2%80%9D"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >
          – tool use can be one such channel (e.g., AI says “I’m not sure about
          X, let me run a quick experiment”). In summary, tool-using code agents
          will become more prevalent and smarter. By next year, using an AI
          coding assistant might feel less like “talking to a genie” and more
          like working with a junior developer who knows how to use Stack
          Overflow, the compiler, and other tools to get the job done.
        </p>
        <h2 data-start="41240" data-end="41295">
          11. Handling Large Codebases and Long-Term Context
        </h2>
        <p data-start="41296" data-end="42566">
          <strong data-start="41296" data-end="41311">Background:</strong> Real
          software projects consist of hundreds of files and potentially
          millions of lines of code, whereas AI models traditionally had very
          limited context windows (a few thousand tokens) — meaning they
          couldn’t “read” the whole codebase at once. This limitation forces
          today’s code assistants to operate mostly at the snippet or
          single-file level unless special measures are taken. The research
          challenge is enabling AI to work with
          <strong data-start="41742" data-end="41767"
            >large-scale codebases</strong
          >: understand project-wide structures, recall relevant parts across
          different modules, and maintain consistency in a code generation that
          may span multiple files. Two main approaches address this:
          <strong data-start="41963" data-end="41998"
            >increasing context window sizes</strong
          >
          (as mentioned earlier, models like Claude can take 100k tokens of
          input, which might cover a whole project’s files in many cases<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Several%20of%20the%20Code%20Llama,the%20largest%20by%20parameter%20count"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Several%20of%20the%20Code%20Llama,the%20largest%20by%20parameter%20count"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >), and
          <strong data-start="42173" data-end="42209"
            >intelligent retrieval of context</strong
          >
          (using embeddings or search to fetch only the relevant portions of
          code into the prompt). Both approaches, and combinations thereof, are
          being explored to give coding agents a “big picture” view. Another
          aspect is
          <em data-start="42424" data-end="42446">incremental learning</em>: if
          an AI is used repeatedly on one project, can it build up a memory or
          index of that project to get better over time?
        </p>
        <p data-start="42568" data-end="44489">
          <strong data-start="42568" data-end="42588">Recent Advances:</strong>
          We have already touched on the context window breakthroughs – having
          100k-token context means in one go, an AI could ingest something like
          all of a medium-sized library’s source code or an entire technical
          design document<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://www.anthropic.com/news/100k-context-windows#:~:text=documents.%20,build%20on%20or%20modify%20it"
                target="_blank"
                rel="noopener"
                alt="https://www.anthropic.com/news/100k-context-windows#:~:text=documents.%20,build%20on%20or%20modify%20it"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >anthropic.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. Anthropic demonstrated dropping an entire codebase and related
          documentation into Claude and querying across it, which is a
          qualitative leap in capability<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://www.anthropic.com/news/100k-context-windows#:~:text=documents.%20,build%20on%20or%20modify%20it"
                target="_blank"
                rel="noopener"
                alt="https://www.anthropic.com/news/100k-context-windows#:~:text=documents.%20,build%20on%20or%20modify%20it"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >anthropic.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. On the retrieval side, services like
          <strong data-start="43083" data-end="43103">Sourcegraph Cody</strong>
          (an AI dev tool) index your repository and use semantic search to feed
          the AI relevant file snippets when you ask questions. This way, even
          if the model itself can’t see everything at once, it gets the needed
          pieces on demand. Techniques like
          <strong data-start="43347" data-end="43377"
            >hierarchical summarization</strong
          >
          are also in use: an AI can summarize each file or component, then
          those summaries are used at a higher level to answer questions about
          the whole system. The MIT CSAIL roadmap paper specifically pointed out
          that current models struggle with scale and often
          <em data-start="43634" data-end="43725"
            >hallucinate code that looks plausible but doesn’t fit the
            codebase’s actual APIs or style</em
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Scale%20compounds%20these%20difficulties,patterns%20of%20a%20given%20company"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Scale%20compounds%20these%20difficulties,patterns%20of%20a%20given%20company"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. This happens when the model’s training data doesn’t cover a
          project’s proprietary classes or functions. In response, one
          development is fine-tuning or customizing models on a company’s
          codebase (for instance, firms fine-tuning LLaMA on their internal
          code) to teach the model about their specific APIs. Another relevant
          advance is around
          <strong data-start="44104" data-end="44126">continuous context</strong
          >: OpenAI’s 2023 “function calling” and tools allow an agent to
          iteratively load different parts of the project as needed (sort of a
          paging mechanism). Also, efforts like
          <strong data-start="44296" data-end="44319"
            >memory augmentation</strong
          >
          give the agent a vector store to which it can add notes and facts
          about the project as it explores it, simulating a long-term memory
          beyond the immediate context window.
        </p>
        <p data-start="44491" data-end="46394">
          <strong data-start="44491" data-end="44522"
            >Open Questions &amp; Next Year:</strong
          >
          One open challenge is making
          <em data-start="44552" data-end="44577">zero-shot large context</em>
          usage reliable. Just because a model can take 100k tokens doesn’t mean
          it always knows what to do with so much information. There’s research
          to be done on how to prompt models effectively when providing such
          huge contexts (avoid overwhelming it with irrelevant info, for
          example). Another question is maintaining
          <strong data-start="44891" data-end="44906">consistency</strong>
          across a large output. If we ask the AI to generate a 10-file project,
          ensuring that a function defined in one file is correctly referenced
          in another, or a variable name remains consistent, is hard without
          iterative refinement. We might see work on AI-driven project
          scaffolding: where the AI first outlines the project structure
          (perhaps filenames, classes, interfaces) before filling in details, to
          reduce inconsistency – essentially planning before generation. By next
          year, developers will likely have better tools for AI assistance on
          large projects; for example, you might point an AI to your Git
          repository and ask it questions like “Where in this codebase is the
          OAuth logic implemented?” and get a precise answer with references. We
          should also look for progress in
          <strong data-start="45683" data-end="45718"
            >evaluation on large-scale tasks</strong
          >. As noted, current benchmarks are small<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Solar,for%20style%2C%20performance%2C%20and%20security"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Solar,for%20style%2C%20performance%2C%20and%20security"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=still%20akin%20to%20the%20%E2%80%9Cundergrad,will%20remain%20an%20open%20challenge"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=still%20akin%20to%20the%20%E2%80%9Cundergrad,will%20remain%20an%20open%20challenge"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. The community may introduce new benchmarks that involve multi-file,
          realistic projects (perhaps having an AI debug a bug that spans two
          modules, or add a feature requiring changes in multiple places).
          Solving those will be a next frontier. Summing up, bridging the gap
          from “toy examples” to “industrial-scale code” is critical, and the
          next year of research will bring us closer to AI that truly
          understands and navigates entire software systems rather than isolated
          fragments<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Scale%20compounds%20these%20difficulties,patterns%20of%20a%20given%20company"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Scale%20compounds%20these%20difficulties,patterns%20of%20a%20given%20company"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Models%20will%20also%20often%20retrieve,but%20look%20different%2C%E2%80%9D%20says%20Solar%E2%80%91Lezama"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Models%20will%20also%20often%20retrieve,but%20look%20different%2C%E2%80%9D%20says%20Solar%E2%80%91Lezama"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >.
        </p>
        <h2 data-start="46396" data-end="46445">
          12. Human-AI Interaction and Trust in Coding
        </h2>
        <p data-start="46446" data-end="47666">
          <strong data-start="46446" data-end="46461">Background:</strong>
          Introducing AI agents into coding raises important questions about how
          humans and these agents interact, and how we build
          <strong data-start="46584" data-end="46593">trust</strong> in their
          outputs. A coding assistant might suggest code that is syntactically
          correct, but can the developer trust it to be logically correct,
          secure, or optimal? Often, the AI’s reasoning is a black box – it
          doesn’t explain <em data-start="46819" data-end="46824">why</em> it
          wrote a certain piece of code. Improving the communication between the
          developer (human) and the AI is an active area of research. This
          includes giving the AI the ability to express uncertainty (“I’m not
          fully confident about this part of the code”)<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=engineering%20tools%2C%20from%20debuggers%20to,%E2%80%9D"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=engineering%20tools%2C%20from%20debuggers%20to,%E2%80%9D"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >, or the ability to ask clarifying questions when requirements are
          vague<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=what%20the%20model%20writes%2C%E2%80%9D%20he,%E2%80%9D"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=what%20the%20model%20writes%2C%E2%80%9D%20he,%E2%80%9D"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >, rather than confidently outputting possibly wrong code. On the
          human side, the challenge is cognitive: how do developers incorporate
          AI suggestions without losing understanding of their own code? Are
          there interface designs that can help, such as highlighting which
          parts of the AI-generated code are borrowed from training data (for
          provenance), or tools to help review AI contributions effectively
          (like showing test results for them)?
        </p>
        <p data-start="47668" data-end="49272">
          <strong data-start="47668" data-end="47688">Recent Advances:</strong>
          In 2022-2023, there were user studies examining how developers use
          tools like Copilot. One study by Microsoft found that while Copilot
          can significantly speed up coding (as mentioned earlier, tasks done
          ~55% faster)<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://arxiv.org/abs/2302.06590#:~:text=,transition%20into%20software%20development%20careers"
                target="_blank"
                rel="noopener"
                alt="https://arxiv.org/abs/2302.06590#:~:text=,transition%20into%20software%20development%20careers"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >arxiv.org</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >, developers sometimes experience a false sense of security. They
          might accept suggestions that compile and seem plausible, but later
          find bugs or security issues. To mitigate this, we’ve seen features
          like
          <strong data-start="48150" data-end="48183"
            >GitHub’s vulnerability filter</strong
          >
          for Copilot, which tries to prevent obviously insecure suggestions.
          Also, research prototypes have looked at
          <em data-start="48293" data-end="48318">explainable AI for code</em>:
          for instance, an AI could accompany its code suggestion with a brief
          rationale (“I used a binary search here because the list is sorted –
          for efficiency”). There’s also exploration of different interaction
          modalities: beyond plain text suggestions, maybe the AI could present
          multiple options (like “Option A uses approach X, Option B uses
          approach Y”) and let the human choose, which can increase the human’s
          understanding and control. The MIT CSAIL paper stressed the “thin line
          of communication” issue – currently, you prompt and get a big chunk of
          code, which is not very interactive<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=If%20measurement%20is%20one%20obstacle%2C,the%20AI%20to%20expose%20its"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=If%20measurement%20is%20one%20obstacle%2C,the%20AI%20to%20expose%20its"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. To address this, companies are adding chat interfaces (so you can
          discuss with the AI) and voice interfaces (to converse more
          naturally). GitHub’s voice interface in Copilot X, for example, can
          make the interaction feel more conversational, which might encourage
          developers to ask “Are you sure about that fix? Explain it.”
        </p>
        <p data-start="49274" data-end="51478">
          <strong data-start="49274" data-end="49305"
            >Open Questions &amp; Next Year:</strong
          >
          A major open question is how to quantify and improve
          <strong data-start="49359" data-end="49378">trustworthiness</strong>
          of AI in coding. How do we know when a developer should trust a
          suggestion, or when they should double-check? One approach is having
          the AI itself provide confidence or highlight which lines it’s less
          sure about<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=engineering%20tools%2C%20from%20debuggers%20to,%E2%80%9D"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=engineering%20tools%2C%20from%20debuggers%20to,%E2%80%9D"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. Another approach is external validation – e.g., automatically
          running tests on AI-generated code (where possible) and showing the
          results to the user. We also need better interfaces for code review of
          AI contributions: in a scenario where AI writes 50% of the code,
          traditional code review might become a bottleneck if done entirely by
          humans. Perhaps AI tools will assist in reviewing AI-written code,
          creating a loop of checks and balances. On the human factors side,
          there’s the question of how using AI affects developer skills. Some
          worry that over-reliance on AI might erode a developer’s ability to
          solve problems from first principles. Empirical research on this will
          likely continue – maybe by next year we’ll have data from long-term
          studies (e.g., do students who learn coding with AI end up with
          different strengths/weaknesses than those who learn without?). Another
          important aspect is
          <strong data-start="50530" data-end="50555"
            >ethical use and trust</strong
          >: developers need to trust that using these tools won’t accidentally
          plagiarize code or violate licenses (tying into IP issues). Improved
          transparency from AI providers (like OpenAI’s statements about how
          Copilot might produce licensed code<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Then%2C%20there%E2%80%99s%20the%20intellectual%20property,elephant%20in%20the%20room"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Then%2C%20there%E2%80%99s%20the%20intellectual%20property,elephant%20in%20the%20room"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >) and possibly tools that can detect if a suggestion is verbatim from
          some known source, could be on the horizon. In summary, the focus is
          moving from just “get the AI to write code” to “integrate AI in the
          development team in a trustworthy, usable way.” By next year, expect
          IDEs to experiment with new UX paradigms for AI (e.g., an AI assistant
          that has a persona you can interrogate about its suggestions) and
          community guidelines/best practices on how to effectively pair program
          with AI. Building
          <em data-start="51336" data-end="51354">calibrated trust</em> – not
          too much, not too little – is the end goal, so that AI becomes a truly
          helpful teammate rather than a source of risk.
        </p>
        <h2 data-start="51480" data-end="51529">
          13. Security and Ethics of AI-Generated Code
        </h2>
        <p data-start="51530" data-end="53273">
          <strong data-start="51530" data-end="51545">Background:</strong> The
          use of AI in coding brings forth security and ethical considerations.
          One concern is that AI-generated code might contain
          <strong data-start="51672" data-end="51691">vulnerabilities</strong>
          or bad practices. An AI that was trained on lots of public code (which
          may include insecure code) could unknowingly introduce those same
          vulnerabilities. A well-known study in 2021 found that roughly
          <strong data-start="51892" data-end="51955"
            >40% of code outputs from GitHub Copilot had security issues</strong
          >
          in at least one scenario<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://cyber.nyu.edu/2021/10/15/ccs-researchers-find-github-copilot-generates-vulnerable-code-40-of-the-time/#:~:text=A%20recent%20study%20by%20cybersecurity,be%20exploited%20by%20an%20attacker"
                target="_blank"
                rel="noopener"
                alt="https://cyber.nyu.edu/2021/10/15/ccs-researchers-find-github-copilot-generates-vulnerable-code-40-of-the-time/#:~:text=A%20recent%20study%20by%20cybersecurity,be%20exploited%20by%20an%20attacker"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >cyber.nyu.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://cyber.nyu.edu/2021/10/15/ccs-researchers-find-github-copilot-generates-vulnerable-code-40-of-the-time/#:~:text=having%20CoPilot%20output%201%2C692%20programs,be%20exploited%20by%20an%20attacker"
                target="_blank"
                rel="noopener"
                alt="https://cyber.nyu.edu/2021/10/15/ccs-researchers-find-github-copilot-generates-vulnerable-code-40-of-the-time/#:~:text=having%20CoPilot%20output%201%2C692%20programs,be%20exploited%20by%20an%20attacker"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >cyber.nyu.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. Issues ranged from SQL injection vulnerabilities to use of
          deprecated cryptography. Another concern is
          <strong data-start="52163" data-end="52192"
            >malicious code generation</strong
          >: could someone use an AI agent to easily create malware or find
          exploits? There have been experiments (and some alarmist media
          coverage) showing that if prompted cleverly, models can produce
          phishing scripts or even ransomware-like code (even if they refuse
          obvious requests, rephrased requests sometimes succeed)<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=mistakes%20that%20might%20give%20a,developer%20pause"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=mistakes%20that%20might%20give%20a,developer%20pause"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Code%20Llama%20won%E2%80%99t%20write%20ransomware,script%20%E2%80%94%20the%20model%20complies"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Code%20Llama%20won%E2%80%99t%20write%20ransomware,script%20%E2%80%94%20the%20model%20complies"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. On the ethics side,
          <strong data-start="52606" data-end="52636"
            >intellectual property (IP)</strong
          >
          is a big topic: these models are trained on open-source code (and
          possibly proprietary code that was scraped). If they regurgitate large
          chunks of someone’s copyrighted code, that’s a legal and ethical
          problem. Companies worry that using AI suggestions might taint their
          codebase with code they don’t have rights to. Furthermore, there’s the
          question of attribution – if an AI uses an algorithm from a known
          source, it currently doesn’t credit that source. Finally, the impact
          on developer jobs and the ethics of relying on AI (which might have
          been trained on developers’ work without compensation) is a societal
          concern being debated.
        </p>
        <p data-start="53275" data-end="55404">
          <strong data-start="53275" data-end="53295">Recent Advances:</strong>
          To address security, some AI coding tools have integrated basic
          <strong data-start="53360" data-end="53374">guardrails</strong>. For
          example, OpenAI claims to have internal red-teaming for models like
          Code Llama to avoid obvious dangerous outputs (Code Llama will refuse
          to produce certain types of malware directly, though it can be
          circumvented as noted)<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Well%2C%20Meta%20only%20red,might%20give%20a%20developer%20pause"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Well%2C%20Meta%20only%20red,might%20give%20a%20developer%20pause"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Code%20Llama%20won%E2%80%99t%20write%20ransomware,script%20%E2%80%94%20the%20model%20complies"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Code%20Llama%20won%E2%80%99t%20write%20ransomware,script%20%E2%80%94%20the%20model%20complies"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. GitHub added an
          <strong data-start="53699" data-end="53736"
            >optional feature to block secrets</strong
          >
          (like API keys) from being suggested if it detects them in training
          data, to prevent leaking sensitive info. There’s also increasing
          research on
          <strong data-start="53882" data-end="53908"
            >AI in security testing</strong
          >: using AI to find vulnerabilities in code (the flip side of possibly
          introducing them). A Stanford-affiliated study in 2023 found that
          engineers using AI assistants were more likely to produce insecure
          code – not because the engineers got worse, but because the AI often
          suggested insecure code that looked superficially correct<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=present%20new%20risks"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=present%20new%20risks"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=A%20Stanford,software%20and%20using%20insecure%20configurations"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=A%20Stanford,software%20and%20using%20insecure%20configurations"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. This has raised awareness that we need better training for
          developers on AI limitations and perhaps features in the AI that
          explicitly check for common vulnerabilities (e.g., an AI that after
          writing code, internally runs a security scanner on it). On the IP
          front, there have been lawsuits (one against GitHub Copilot in 2022)
          about code generation and copyright. In response, companies like
          OpenAI and Microsoft have started offering
          <strong data-start="54753" data-end="54775"
            >legal safe harbors</strong
          >
          – for instance, Microsoft announced it would indemnify Copilot for
          enterprise customers, meaning if Copilot’s output leads to an IP
          lawsuit, Microsoft would handle it. Also, tools are being developed to
          detect AI-generated code, or to have the AI itself mark which parts of
          its output are directly from training data versus original synthesis.
          OpenAI has been somewhat opaque, but they did allow an opt-out for
          open-source projects via a site to request not training on your GitHub
          repo. Meanwhile,
          <strong data-start="55275" data-end="55289">regulators</strong> and
          standards bodies are looking at this whole space, which might yield
          guidelines on secure use of AI in coding.
        </p>
        <p data-start="55406" data-end="57482">
          <strong data-start="55406" data-end="55437"
            >Open Questions &amp; Next Year:</strong
          >
          A pressing question is how to
          <strong data-start="55468" data-end="55511"
            >reduce the incidence of vulnerable code</strong
          >
          from AI. Solutions could include fine-tuning models on secure code
          examples or explicitly training them to avoid certain dangerous
          functions. Also, incorporating a “critic” model that reviews the
          output for security issues is a promising approach (like
          chain-of-thought: one model generates code, another model or process
          checks it). We might see new research on “aligning” code models with
          security policies – essentially teaching them to not only follow the
          user’s functional prompt but also an implicit prompt of “and make it
          secure and compliant.” On the ethical side,
          <strong data-start="56085" data-end="56101">transparency</strong> is
          important. Next year, I suspect tools will start giving more
          meta-information, like “This suggestion is similar to code on
          StackOverflow or in Linux kernel” – something that Copilot already
          occasionally does by outputting a comment with a source URL (though
          it’s rare). IP-wise, the legal landscape is evolving: by 2024, we
          might have court rulings that clarify whether AI-generated code can be
          copyrighted or who is liable for infringements. This will heavily
          influence product features (for example, a “strict” mode that only
          generates very generic code to avoid any copying). Finally, there’s
          the ethics of usage: should there be tasks we decide AI shouldn’t do?
          For instance, perhaps generating exploit code should be restricted.
          OpenAI, Anthropic, etc., already put some limits (Anthropic’s Claude
          has policies against certain misuse). But open-source models don’t
          have such filters by default, so the community might need to agree on
          norms. In summary, as AI coding becomes commonplace, ensuring the code
          is
          <strong data-start="57119" data-end="57146"
            >safe, secure, and legal</strong
          >
          is paramount. In the coming year, expect more collaboration between AI
          researchers and the cybersecurity community, and possibly new tools
          that pair AI code generation with rigorous analysis to catch issues
          before the code ever hits a production repository<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=A%20Stanford,software%20and%20using%20insecure%20configurations"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=A%20Stanford,software%20and%20using%20insecure%20configurations"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Some%20code,tools%20into%20their%20production%20software"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Some%20code,tools%20into%20their%20production%20software"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >.
        </p>
        <h2 data-start="57484" data-end="57538">
          14. Domain-Specific and Specialized Coding Agents
        </h2>
        <p data-start="57539" data-end="58666">
          <strong data-start="57539" data-end="57554">Background:</strong> While
          most of the buzz is around general-purpose coding assistants, there’s
          a strong case for AI agents specialized in particular domains or tech
          stacks. For example, an AI tuned specifically for
          <strong data-start="57751" data-end="57770">web development</strong>
          might know HTML/CSS/JavaScript and common frameworks (React, Angular)
          intimately, making it more helpful for frontend engineers than a
          generic model. Similarly, a
          <strong data-start="57934" data-end="57950">data science</strong>
          coding assistant could be equipped with knowledge of pandas, numpy, ML
          libraries, and even some statistical background to help with analysis
          code. Domain-specific models can be smaller and trained on targeted
          data, which sometimes makes them more efficient and even more accurate
          within that domain. We already saw some early versions:
          <strong data-start="58287" data-end="58311"
            >Salesforce’s CodeGen</strong
          >
          models (2022) had variants for different programming languages, and
          models like
          <strong data-start="58392" data-end="58405">Polycoder</strong> and
          <strong data-start="58410" data-end="58420">CuBERT</strong> focused on
          specific languages or tasks. Another angle is domain in terms of
          industry: imagine an AI coding assistant for finance that understands
          financial protocols, or one for game development that knows Unity
          engine APIs and shader languages.
        </p>
        <p data-start="58668" data-end="60113">
          <strong data-start="58668" data-end="58688">Recent Advances:</strong>
          In 2023, Meta’s Code Llama release actually included a
          <strong data-start="58744" data-end="58772"
            >Python-specialized model</strong
          >
          (Code Llama-Python) that was further fine-tuned on 100B tokens of
          Python code<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Each%20of%20the%20Code%20Llama,and%20%E2%80%9Csafe%E2%80%9D%20answers%20to%20questions"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Each%20of%20the%20Code%20Llama,and%20%E2%80%9Csafe%E2%80%9D%20answers%20to%20questions"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. This specialization led to better performance on Python tasks than
          the base model of the same size. Likewise, open-source community
          models like
          <strong data-start="59035" data-end="59050">WizardCoder</strong> were
          tuned heavily on Python instructions, since Python is ubiquitous in
          coding benchmarks. We also have seen platforms like
          <strong data-start="59176" data-end="59186">Replit</strong> release
          models (Replit Code V1.5, etc.) that are trained on the kind of coding
          done on Replit (which might skew to web apps, small projects) and
          claim improved results for that user base. On the industry side,
          companies are beginning to bundle AI with domain knowledge: e.g.,
          <strong data-start="59463" data-end="59488"
            >Matlab’s AI Assistant</strong
          >
          helps with MATLAB code (which is used in engineering and scientific
          computing). IBM’s aforementioned COBOL-to-Java translator is
          essentially a domain-specific coder for legacy mainframe code<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://newsroom.ibm.com/2023-10-26-IBM-Launches-watsonx-Code-Assistant,-Delivers-Generative-AI-powered-Code-Generation-Capabilities-Built-for-Enterprise-Application-Modernization#:~:text=IBM%20watsonx%20Code%20Assistant%20for,developer%20productivity%20on%20the%20platform"
                target="_blank"
                rel="noopener"
                alt="https://newsroom.ibm.com/2023-10-26-IBM-Launches-watsonx-Code-Assistant,-Delivers-Generative-AI-powered-Code-Generation-Capabilities-Built-for-Enterprise-Application-Modernization#:~:text=IBM%20watsonx%20Code%20Assistant%20for,developer%20productivity%20on%20the%20platform"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >newsroom.ibm.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. We also saw early versions of
          <strong data-start="59750" data-end="59777"
            >security-focused coders</strong
          >
          (some security companies fine-tuned models on vulnerability patterns
          to create an assistant that can help write secure code or spot
          insecure code). Even within a language, specialization occurs: a model
          might be fine-tuned for
          <strong data-start="60005" data-end="60025">TypeScript/React</strong>
          if it’s meant to help with frontend work, including knowledge of
          common UI components.
        </p>
        <p data-start="60115" data-end="62256">
          <strong data-start="60115" data-end="60146"
            >Open Questions &amp; Next Year:</strong
          >
          The trade-off with specialization is maintaining breadth. An AI that’s
          too narrow might fail when the task slightly departs the domain.
          There’s an open question of how to best combine specialized knowledge
          with general reasoning. One idea is a
          <em data-start="60391" data-end="60411">mixture-of-experts</em>
          approach, where an AI system routes your query to a specialized agent
          – e.g., if you ask about a database query, route to the SQL expert
          model; if you ask about UI layout, route to the web model. Research
          may explore this modular approach more, perhaps using something like a
          coordinator model that delegates to expert models (which could even be
          smaller). We might also see
          <strong data-start="60787" data-end="60816"
            >on-the-fly specialization</strong
          >: models that can be quickly fine-tuned or configured with additional
          knowledge for a domain when needed (maybe loading a plugin with domain
          data). As for next year, I predict more companies will roll out
          <strong data-start="61021" data-end="61060"
            >industry-specific coding assistants</strong
          >. For instance, a healthcare IT firm might have an AI that knows
          HL7/FHIR standards (medical data interchange formats) to assist their
          devs. Cloud providers might offer AI tuned for their environment –
          e.g., an AWS Lambda coding assistant that knows the ins and outs of
          AWS services. On the research front, evaluating domain-specific agents
          is a challenge; we might see new benchmarks for specific fields (like
          a benchmark of coding tasks in the biomedical domain, to test how well
          an AI can handle those). Another question:
          <strong data-start="61585" data-end="61668"
            >do we need entirely separate models, or just clever prompting of
            one big model?</strong
          >
          It could be that one GPT-4-level model can handle all domains if given
          the right context (like feeding it relevant docs), reducing the need
          to train many niche models. However, privacy and proprietary data play
          a role: companies might prefer a smaller model trained on their
          internal code over sending prompts to a general model. In sum, the
          ecosystem will likely stratify into a handful of very large general
          models and many smaller specialized ones. The next year will reveal
          how effective those specialized ones are and whether hybrid strategies
          (combining strengths) become the norm.
        </p>
        <h2 data-start="62258" data-end="62311">
          15. Evaluation and Benchmarking of Coding Agents
        </h2>
        <p data-start="62312" data-end="63268">
          <strong data-start="62312" data-end="62327">Background:</strong> With
          the rapid development of AI coding models, evaluating their
          performance rigorously is crucial. The community needs to track
          progress and identify weaknesses. Traditional code generation
          benchmarks include things like
          <strong data-start="62550" data-end="62572"
            >HumanEval (OpenAI)</strong
          >
          – where models write solutions to simple programming problems, and
          <strong data-start="62640" data-end="62688"
            >MBPP (Google’s Mostly Basic Python Problems)</strong
          >
          for Python. These focus on functional correctness: does the generated
          code produce the expected output for some hidden test cases. However,
          as models get better at these, benchmarks must evolve. Moreover,
          coding agents now involve more than just writing a single function –
          they might be multi-step, involve debugging, etc., which isn’t
          captured in static benchmarks. Another dimension is evaluating
          <em data-start="63089" data-end="63111">developer experience</em>: how
          do we measure if a coding agent actually makes a developer more
          productive or happier? That involves human studies rather than just
          automated metrics.
        </p>
        <p data-start="63270" data-end="64735">
          <strong data-start="63270" data-end="63290">Recent Advances:</strong>
          OpenAI in 2023 open-sourced the
          <strong data-start="63323" data-end="63339">OpenAI Evals</strong>
          framework<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://openai.com/index/function-calling-and-other-api-updates/#:~:text=comprehensive%20range%20of%20tasks,report%20shortcomings%20in%20our%20models"
                target="_blank"
                rel="noopener"
                alt="https://openai.com/index/function-calling-and-other-api-updates/#:~:text=comprehensive%20range%20of%20tasks,report%20shortcomings%20in%20our%20models"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >openai.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >, which allows users to contribute evaluation scripts for various
          tasks, including coding. This was an invitation for the community to
          help test models on new scenarios. We’ve also seen new competitive
          programming sets like
          <strong data-start="63612" data-end="63628">CodeContests</strong>
          (which gathers competitive programming problems to evaluate coding
          prowess) and multi-language benchmarks like
          <strong data-start="63740" data-end="63753">MultiPL-E</strong>
          (HumanEval translated into many languages) to test model versatility.
          One notable point raised in 2025 by MIT researchers is that current
          benchmarks are too narrow – e.g., solving a single GitHub issue or a
          LeetCode problem is a far cry from real software engineering<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Solar,for%20style%2C%20performance%2C%20and%20security"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Solar,for%20style%2C%20performance%2C%20and%20security"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=still%20akin%20to%20the%20%E2%80%9Cundergrad,will%20remain%20an%20open%20challenge"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=still%20akin%20to%20the%20%E2%80%9Cundergrad,will%20remain%20an%20open%20challenge"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. They argue for benchmarks that include things like
          <em data-start="64152" data-end="64213"
            >refactoring challenges, debugging tasks, code review tasks,</em
          >
          and even project-scale modifications<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=based%20testing%2C%20and%20other%20methods,for%20style%2C%20performance%2C%20and%20security"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=based%20testing%2C%20and%20other%20methods,for%20style%2C%20performance%2C%20and%20security"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=self,stakes%20scenarios%2C%20measuring%20progress"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=self,stakes%20scenarios%2C%20measuring%20progress"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. There’s movement towards that: for example, the
          <strong data-start="64378" data-end="64391">SWE-Bench</strong>
          mentioned is about applying a patch to a codebase (simulating a bug
          fix scenario). Also, human studies have been used as evaluation:
          Microsoft’s GitHub Copilot team, as we saw, ran controlled experiments
          to measure time saved<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://arxiv.org/abs/2302.06590#:~:text=,transition%20into%20software%20development%20careers"
                target="_blank"
                rel="noopener"
                alt="https://arxiv.org/abs/2302.06590#:~:text=,transition%20into%20software%20development%20careers"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >arxiv.org</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >, which is a form of evaluation beyond correctness – it measures
          productivity.
        </p>
        <p data-start="64737" data-end="67196">
          <strong data-start="64737" data-end="64768"
            >Open Questions &amp; Next Year:</strong
          >
          A key open question is
          <em data-start="64792" data-end="64816">what should we measure</em>
          when it comes to coding agents? Correctness is necessary but not
          sufficient; code quality (readability, maintainability), security, and
          efficiency are also important. However, those are harder to measure
          automatically. We might see proxies for some of these – e.g.,
          measuring how often AI-generated code passes a linter or static
          analysis for quality, or using performance test suites to see if the
          AI can generate not just correct but performant code. Another question
          is how to evaluate multi-turn interactions: if an agent converses with
          you to clarify requirements, how to score that dialogue? It’s complex
          because it blends NLP with code. Perhaps new simulation-based
          evaluations will come, where an AI agent is put in a realistic
          developer scenario (with a fake “project” and a set of tasks) and its
          performance is judged by the completeness of tasks done and the number
          of errors introduced. The MIT paper calls for
          <strong data-start="65740" data-end="65777"
            >community-scale evaluation suites</strong
          >
          for things like refactor quality and bug-fix longevity<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=The%20authors%20mention%20that%20since,Solar%E2%80%91Lezama%20imagines%20incremental"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=The%20authors%20mention%20that%20since,Solar%E2%80%91Lezama%20imagines%20incremental"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >, suggesting collaborative efforts to build these. By next year, we
          might see the release of such community-driven benchmarks – for
          instance, a “Refactory” benchmark where the AI has to perform a series
          of refactoring operations on a large codebase and not break any tests.
          In industry, expect model providers to brag with holistic metrics:
          OpenAI or others might start quoting things like “Our new model has X%
          success on HumanEval, Y% on CodeContests, and increased productivity
          in user studies by Z%.” It will be important to take those with a
          grain of salt, hence the need for transparent, peer-reviewed
          benchmarks. Lastly, evaluation should keep up with new capabilities:
          if agents start using tools, benchmarks should allow tool use (for
          instance, letting the agent actually run code as part of the
          evaluation loop). This is an area where product and research intersect
          heavily – a lot of knowledge will come from observing real-world use
          (telemetry from IDE plugins etc.) and feeding that back into better
          benchmarks. In summary, better benchmarks are both an academic pursuit
          and a practical necessity, and the next year will likely bring more
          sophisticated ways to judge these coding agents on what truly matters
          for software development<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=still%20akin%20to%20the%20%E2%80%9Cundergrad,will%20remain%20an%20open%20challenge"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=still%20akin%20to%20the%20%E2%80%9Cundergrad,will%20remain%20an%20open%20challenge"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=The%20authors%20mention%20that%20since,Solar%E2%80%91Lezama%20imagines%20incremental"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=The%20authors%20mention%20that%20since,Solar%E2%80%91Lezama%20imagines%20incremental"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >.
        </p>
        <hr data-start="67198" data-end="67201" />
        <p
          data-start="67203"
          data-end="68147"
          data-is-last-node=""
          data-is-only-node=""
        >
          <strong data-start="67203" data-end="67215">Sources:</strong> The
          information above is drawn from recent research papers, industry
          announcements, and expert insights in 2023–2025. Notable references
          include Meta AI’s release of Code Llama<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=machine%20learning%20system%20that%20can,natural%20language%20%E2%80%94%20specifically%20English"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=machine%20learning%20system%20that%20can,natural%20language%20%E2%80%94%20specifically%20English"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Several%20of%20the%20Code%20Llama,the%20largest%20by%20parameter%20count"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Several%20of%20the%20Code%20Llama,the%20largest%20by%20parameter%20count"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >, OpenAI’s technical blogs<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://openai.com/index/function-calling-and-other-api-updates/#:~:text=Developers%20can%20now%20describe%20functions,with%20external%20tools%20and%20APIs"
                target="_blank"
                rel="noopener"
                alt="https://openai.com/index/function-calling-and-other-api-updates/#:~:text=Developers%20can%20now%20describe%20functions,with%20external%20tools%20and%20APIs"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >openai.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >, the MIT CSAIL report on AI for Software Engineering<span
            class=""
            data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Scale%20compounds%20these%20difficulties,patterns%20of%20a%20given%20company"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Scale%20compounds%20these%20difficulties,patterns%20of%20a%20given%20company"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=If%20measurement%20is%20one%20obstacle%2C,the%20AI%20to%20expose%20its"
                target="_blank"
                rel="noopener"
                alt="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=If%20measurement%20is%20one%20obstacle%2C,the%20AI%20to%20expose%20its"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >news.mit.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >, and various studies on AI-assisted programming productivity and
          security<span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://arxiv.org/abs/2302.06590#:~:text=,transition%20into%20software%20development%20careers"
                target="_blank"
                rel="noopener"
                alt="https://arxiv.org/abs/2302.06590#:~:text=,transition%20into%20software%20development%20careers"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >arxiv.org</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=present%20new%20risks"
                target="_blank"
                rel="noopener"
                alt="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=present%20new%20risks"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >techcrunch.com</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          ><span class="" data-state="closed"
            ><span
              class="ms-1 inline-flex max-w-full items-center relative top-[-0.094rem] animate-[show_150ms_ease-in]"
              data-testid="webpage-citation-pill"
              ><a
                href="https://cyber.nyu.edu/2021/10/15/ccs-researchers-find-github-copilot-generates-vulnerable-code-40-of-the-time/#:~:text=A%20recent%20study%20by%20cybersecurity,be%20exploited%20by%20an%20attacker"
                target="_blank"
                rel="noopener"
                alt="https://cyber.nyu.edu/2021/10/15/ccs-researchers-find-github-copilot-generates-vulnerable-code-40-of-the-time/#:~:text=A%20recent%20study%20by%20cybersecurity,be%20exploited%20by%20an%20attacker"
                class="flex h-4.5 overflow-hidden rounded-xl px-2 text-[9px] font-medium transition-colors duration-150 ease-in-out text-token-text-secondary! bg-[#F4F4F4]! dark:bg-[#303030]!"
                ><span
                  class="relative start-0 bottom-0 flex h-full w-full items-center"
                  ><span
                    class="flex h-4 w-full items-center justify-between overflow-hidden"
                    style="opacity: 1; transform: none"
                    ><span
                      class="max-w-[15ch] grow truncate overflow-hidden text-center"
                      >cyber.nyu.edu</span
                    ></span
                  ></span
                ></a
              ></span
            ></span
          >. Each topic reflects a blend of background context, the latest
          developments as of 2025, and forward-looking perspectives on open
          questions heading into 2026. The landscape of AI coding agents is
          evolving rapidly, making it an exciting time for AI engineers to
          explore these research areas.
        </p>
      </div>
    </div>
  </div>
  <div class="border-token-border-sharp flex flex-col gap-2 border-b"></div>
  <div>
    <div></div>
    <div class="flex flex-col gap-4">
      <div class="flex flex-col">
        <div class="text-base font-medium">Citations</div>
        <div class="ms-[-12px] flex flex-col gap-1">
          <a
            href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=machine%20learning%20system%20that%20can,natural%20language%20%E2%80%94%20specifically%20English"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://techcrunch.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Meta releases Code Llama, a code-generating AI model |
                TechCrunch
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/
            </div></a
          ><a
            href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Code%20Llama%2C%20which%20is%20available,model%20like%20Copilot%20could%20produce"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://techcrunch.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Meta releases Code Llama, a code-generating AI model |
                TechCrunch
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/
            </div></a
          ><a
            href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Each%20of%20the%20Code%20Llama,and%20%E2%80%9Csafe%E2%80%9D%20answers%20to%20questions"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://techcrunch.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Meta releases Code Llama, a code-generating AI model |
                TechCrunch
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/
            </div></a
          ><a
            href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Several%20of%20the%20Code%20Llama,the%20largest%20by%20parameter%20count"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://techcrunch.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Meta releases Code Llama, a code-generating AI model |
                TechCrunch
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/
            </div></a
          ><a
            href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Akin%20to%20GitHub%20Copilot%20,and%20Bash"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://techcrunch.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Meta releases Code Llama, a code-generating AI model |
                TechCrunch
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/
            </div></a
          ><a
            href="https://en.wikipedia.org/wiki/Llama_(language_model)#:~:text=%282024,TechCrunch"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://en.wikipedia.org&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Llama (language model) - Wikipedia
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://en.wikipedia.org/wiki/Llama_(language_model)
            </div></a
          ><a
            href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=GitHub%20claims%20that%20more%20than,increased%20productivity%20and%20faster%20learning"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://techcrunch.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Meta releases Code Llama, a code-generating AI model |
                TechCrunch
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/
            </div></a
          ><a
            href="https://github.blog/news-insights/product-news/github-copilot-x-the-ai-powered-developer-experience/#:~:text=GitHub%20Copilot%20started%20a%20new,code%20%20and%20helps%2064"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://github.blog&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                GitHub Copilot X: The AI-powered developer experience - The
                GitHub Blog
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://github.blog/news-insights/product-news/github-copilot-x-the-ai-powered-developer-experience/
            </div></a
          ><a
            href="https://github.blog/news-insights/product-news/github-copilot-x-the-ai-powered-developer-experience/#:~:text=GitHub%20Copilot%20is%20evolving%20to,a%20more%20personalized%20developer%20experience"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://github.blog&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                GitHub Copilot X: The AI-powered developer experience - The
                GitHub Blog
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://github.blog/news-insights/product-news/github-copilot-x-the-ai-powered-developer-experience/
            </div></a
          ><a
            href="https://arxiv.org/abs/2302.06590#:~:text=,transition%20into%20software%20development%20careers"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://arxiv.org&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                [2302.06590] The Impact of AI on Developer Productivity:
                Evidence from GitHub Copilot
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://arxiv.org/abs/2302.06590
            </div></a
          ><a
            href="https://www.ibm.com/think/topics/autogpt#:~:text=AutoGPT%20is%20an%20open,3.5"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://www.ibm.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                What is AutoGPT? | IBM
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://www.ibm.com/think/topics/autogpt
            </div></a
          ><a
            href="https://www.ibm.com/think/topics/autogpt#:~:text=AutoGPT%20works%20by%20processing%20a,real%20time%20to%20iteratively%20improve"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://www.ibm.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                What is AutoGPT? | IBM
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://www.ibm.com/think/topics/autogpt
            </div></a
          ><a
            href="https://www.ibm.com/think/topics/autogpt#:~:text=In%20addition%20to%20GPT,allowing%20it%20to"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://www.ibm.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                What is AutoGPT? | IBM
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://www.ibm.com/think/topics/autogpt
            </div></a
          ><a
            href="https://www.ibm.com/think/topics/chatdev#1597040493#:~:text=ChatDev%20is%20an%20open%20source,and%20produce%20a%20software%20application"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://www.ibm.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                What is ChatDev? | IBM
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://www.ibm.com/think/topics/chatdev#1597040493
            </div></a
          ><a
            href="https://www.ibm.com/think/topics/chatdev#1597040493#:~:text=intelligent%20agents%20holding%20different%20roles,collaboratively%20to%20complete%20each%20phase"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://www.ibm.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                What is ChatDev? | IBM
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://www.ibm.com/think/topics/chatdev#1597040493
            </div></a
          ><a
            href="https://www.ibm.com/think/topics/chatdev#1597040493#:~:text=ChatDev%E2%80%99s%20primary%20objectives%20are%20to,entities%20are%20known%20as%20agents"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://www.ibm.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                What is ChatDev? | IBM
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://www.ibm.com/think/topics/chatdev#1597040493
            </div></a
          ><a
            href="https://www.ibm.com/think/topics/autogpt#:~:text=of%20the%20process%20and%20shape,the%20overall%20task%20workflow"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://www.ibm.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                What is AutoGPT? | IBM
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://www.ibm.com/think/topics/autogpt
            </div></a
          ><a
            href="https://www.qodo.ai/blog/gpt-4-vs-alphacode/#:~:text=GPT,percentile%20from%20the%20bottom"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://www.qodo.ai&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                GPT-4 Vs. AlphaCode - Qodo
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://www.qodo.ai/blog/gpt-4-vs-alphacode/
            </div></a
          ><a
            href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Imagine%20a%20future%20where%20artificial,day%20challenges"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://news.mit.edu&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Can AI really code? Study maps the roadblocks to autonomous
                software engineering | MIT News | Massachusetts Institute of
                Technology
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716
            </div></a
          ><a
            href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=while%20routine%20work%20is%20automated"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://news.mit.edu&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Can AI really code? Study maps the roadblocks to autonomous
                software engineering | MIT News | Massachusetts Institute of
                Technology
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716
            </div></a
          ><a
            href="https://arxiv.org/html/2506.23749v1#:~:text=DeepSeek,2024"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://arxiv.org&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                A Survey of LLM-based Automated Program Repair: Taxonomies,
                Design Paradigms, and Applications
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://arxiv.org/html/2506.23749v1
            </div></a
          ><a
            href="https://arxiv.org/html/2411.10213v1#:~:text=An%20Empirical%20Study%20on%20LLM,2023%3B%20Zhang%20et%20al"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://arxiv.org&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                An Empirical Study on LLM-based Agents for Automated Bug Fixing
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://arxiv.org/html/2411.10213v1
            </div></a
          ><a
            href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=developers%20risk%20blindly%20trusting%20hallucinated,%E2%80%9D"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://news.mit.edu&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Can AI really code? Study maps the roadblocks to autonomous
                software engineering | MIT News | Massachusetts Institute of
                Technology
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716
            </div></a
          ><a
            href="https://newsroom.ibm.com/2023-10-26-IBM-Launches-watsonx-Code-Assistant,-Delivers-Generative-AI-powered-Code-Generation-Capabilities-Built-for-Enterprise-Application-Modernization#:~:text=IBM%20Launches%20watsonx%20Code%20Assistant%2C,developer%20productivity%20on%20the%20platform"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://newsroom.ibm.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                IBM Launches watsonx Code Assistant, Delivers Generative AI ...
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://newsroom.ibm.com/2023-10-26-IBM-Launches-watsonx-Code-Assistant,-Delivers-Generative-AI-powered-Code-Generation-Capabilities-Built-for-Enterprise-Application-Modernization
            </div></a
          ><a
            href="https://lilianweng.github.io/posts/2023-06-23-agent/#:~:text=MRKL%20%28Karpas%20et%20al,calculator%2C%20currency%20converter%2C%20weather%20API"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://lilianweng.github.io&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                LLM Powered Autonomous Agents | Lil'Log
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://lilianweng.github.io/posts/2023-06-23-agent/
            </div></a
          ><a
            href="https://openai.com/index/function-calling-and-other-api-updates/#:~:text=Developers%20can%20now%20describe%20functions,with%20external%20tools%20and%20APIs"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://openai.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Function calling and other API updates | OpenAI
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://openai.com/index/function-calling-and-other-api-updates/
            </div></a
          ><a
            href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=extends%20to%20the%20AI%E2%80%99s%20ability,%E2%80%9D"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://news.mit.edu&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Can AI really code? Study maps the roadblocks to autonomous
                software engineering | MIT News | Massachusetts Institute of
                Technology
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716
            </div></a
          ><a
            href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=engineering%20tools%2C%20from%20debuggers%20to,%E2%80%9D"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://news.mit.edu&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Can AI really code? Study maps the roadblocks to autonomous
                software engineering | MIT News | Massachusetts Institute of
                Technology
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716
            </div></a
          ><a
            href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=what%20the%20model%20writes%2C%E2%80%9D%20he,%E2%80%9D"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://news.mit.edu&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Can AI really code? Study maps the roadblocks to autonomous
                software engineering | MIT News | Massachusetts Institute of
                Technology
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716
            </div></a
          ><a
            href="https://www.anthropic.com/news/100k-context-windows#:~:text=documents.%20,build%20on%20or%20modify%20it"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://www.anthropic.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Introducing 100K Context Windows \ Anthropic
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://www.anthropic.com/news/100k-context-windows
            </div></a
          ><a
            href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Scale%20compounds%20these%20difficulties,patterns%20of%20a%20given%20company"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://news.mit.edu&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Can AI really code? Study maps the roadblocks to autonomous
                software engineering | MIT News | Massachusetts Institute of
                Technology
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716
            </div></a
          ><a
            href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Solar,for%20style%2C%20performance%2C%20and%20security"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://news.mit.edu&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Can AI really code? Study maps the roadblocks to autonomous
                software engineering | MIT News | Massachusetts Institute of
                Technology
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716
            </div></a
          ><a
            href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=still%20akin%20to%20the%20%E2%80%9Cundergrad,will%20remain%20an%20open%20challenge"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://news.mit.edu&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Can AI really code? Study maps the roadblocks to autonomous
                software engineering | MIT News | Massachusetts Institute of
                Technology
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716
            </div></a
          ><a
            href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Models%20will%20also%20often%20retrieve,but%20look%20different%2C%E2%80%9D%20says%20Solar%E2%80%91Lezama"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://news.mit.edu&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Can AI really code? Study maps the roadblocks to autonomous
                software engineering | MIT News | Massachusetts Institute of
                Technology
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716
            </div></a
          ><a
            href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=If%20measurement%20is%20one%20obstacle%2C,the%20AI%20to%20expose%20its"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://news.mit.edu&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Can AI really code? Study maps the roadblocks to autonomous
                software engineering | MIT News | Massachusetts Institute of
                Technology
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716
            </div></a
          ><a
            href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Then%2C%20there%E2%80%99s%20the%20intellectual%20property,elephant%20in%20the%20room"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://techcrunch.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Meta releases Code Llama, a code-generating AI model |
                TechCrunch
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/
            </div></a
          ><a
            href="https://cyber.nyu.edu/2021/10/15/ccs-researchers-find-github-copilot-generates-vulnerable-code-40-of-the-time/#:~:text=A%20recent%20study%20by%20cybersecurity,be%20exploited%20by%20an%20attacker"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://cyber.nyu.edu&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                CCS researchers find Github CoPilot generates vulnerable code
                40% of the time - NYU Center for Cyber Security
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://cyber.nyu.edu/2021/10/15/ccs-researchers-find-github-copilot-generates-vulnerable-code-40-of-the-time/
            </div></a
          ><a
            href="https://cyber.nyu.edu/2021/10/15/ccs-researchers-find-github-copilot-generates-vulnerable-code-40-of-the-time/#:~:text=having%20CoPilot%20output%201%2C692%20programs,be%20exploited%20by%20an%20attacker"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://cyber.nyu.edu&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                CCS researchers find Github CoPilot generates vulnerable code
                40% of the time - NYU Center for Cyber Security
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://cyber.nyu.edu/2021/10/15/ccs-researchers-find-github-copilot-generates-vulnerable-code-40-of-the-time/
            </div></a
          ><a
            href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=mistakes%20that%20might%20give%20a,developer%20pause"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://techcrunch.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Meta releases Code Llama, a code-generating AI model |
                TechCrunch
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/
            </div></a
          ><a
            href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Code%20Llama%20won%E2%80%99t%20write%20ransomware,script%20%E2%80%94%20the%20model%20complies"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://techcrunch.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Meta releases Code Llama, a code-generating AI model |
                TechCrunch
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/
            </div></a
          ><a
            href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Well%2C%20Meta%20only%20red,might%20give%20a%20developer%20pause"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://techcrunch.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Meta releases Code Llama, a code-generating AI model |
                TechCrunch
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/
            </div></a
          ><a
            href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=present%20new%20risks"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://techcrunch.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Meta releases Code Llama, a code-generating AI model |
                TechCrunch
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/
            </div></a
          ><a
            href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=A%20Stanford,software%20and%20using%20insecure%20configurations"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://techcrunch.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Meta releases Code Llama, a code-generating AI model |
                TechCrunch
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/
            </div></a
          ><a
            href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=A%20Stanford,software%20and%20using%20insecure%20configurations"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://techcrunch.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Meta releases Code Llama, a code-generating AI model |
                TechCrunch
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/
            </div></a
          ><a
            href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=Some%20code,tools%20into%20their%20production%20software"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://techcrunch.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Meta releases Code Llama, a code-generating AI model |
                TechCrunch
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/
            </div></a
          ><a
            href="https://newsroom.ibm.com/2023-10-26-IBM-Launches-watsonx-Code-Assistant,-Delivers-Generative-AI-powered-Code-Generation-Capabilities-Built-for-Enterprise-Application-Modernization#:~:text=IBM%20watsonx%20Code%20Assistant%20for,developer%20productivity%20on%20the%20platform"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://newsroom.ibm.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                IBM Launches watsonx Code Assistant, Delivers Generative AI ...
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://newsroom.ibm.com/2023-10-26-IBM-Launches-watsonx-Code-Assistant,-Delivers-Generative-AI-powered-Code-Generation-Capabilities-Built-for-Enterprise-Application-Modernization
            </div></a
          ><a
            href="https://openai.com/index/function-calling-and-other-api-updates/#:~:text=comprehensive%20range%20of%20tasks,report%20shortcomings%20in%20our%20models"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://openai.com&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Function calling and other API updates | OpenAI
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://openai.com/index/function-calling-and-other-api-updates/
            </div></a
          ><a
            href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=based%20testing%2C%20and%20other%20methods,for%20style%2C%20performance%2C%20and%20security"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://news.mit.edu&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Can AI really code? Study maps the roadblocks to autonomous
                software engineering | MIT News | Massachusetts Institute of
                Technology
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716
            </div></a
          ><a
            href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=self,stakes%20scenarios%2C%20measuring%20progress"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://news.mit.edu&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Can AI really code? Study maps the roadblocks to autonomous
                software engineering | MIT News | Massachusetts Institute of
                Technology
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716
            </div></a
          ><a
            href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=The%20authors%20mention%20that%20since,Solar%E2%80%91Lezama%20imagines%20incremental"
            target="_blank"
            rel="noopener noreferrer"
            class="hover:bg-token-main-surface-secondary flex flex-col gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://news.mit.edu&amp;sz=32"
                />
              </div>
              <p class="text-normal truncate text-base font-semibold">
                Can AI really code? Study maps the roadblocks to autonomous
                software engineering | MIT News | Massachusetts Institute of
                Technology
              </p>
            </div>
            <div
              class="text-token-text-secondary line-clamp-2 text-sm leading-snug font-normal"
            >
              https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716
            </div></a
          >
        </div>
      </div>
      <div class="flex flex-col gap-2.5">
        <div class="text-base font-medium">All Sources</div>
        <div class="ms-[-12px] flex flex-wrap">
          <a
            href="https://techcrunch.com/2023/08/24/meta-releases-code-llama-a-code-generating-ai-model/#:~:text=machine%20learning%20system%20that%20can,natural%20language%20%E2%80%94%20specifically%20English"
            target="_blank"
            rel="noopener noreferrer"
            class="group hover:bg-token-main-surface-secondary flex items-center gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://techcrunch.com&amp;sz=32"
                />
              </div>
              techcrunch
            </div></a
          ><a
            href="https://en.wikipedia.org/wiki/Llama_(language_model)#:~:text=%282024,TechCrunch"
            target="_blank"
            rel="noopener noreferrer"
            class="group hover:bg-token-main-surface-secondary flex items-center gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://en.wikipedia.org&amp;sz=32"
                />
              </div>
              en.wikipedia
            </div></a
          ><a
            href="https://github.blog/news-insights/product-news/github-copilot-x-the-ai-powered-developer-experience/#:~:text=GitHub%20Copilot%20started%20a%20new,code%20%20and%20helps%2064"
            target="_blank"
            rel="noopener noreferrer"
            class="group hover:bg-token-main-surface-secondary flex items-center gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://github.blog&amp;sz=32"
                />
              </div>
              github
            </div></a
          ><a
            href="https://arxiv.org/abs/2302.06590#:~:text=,transition%20into%20software%20development%20careers"
            target="_blank"
            rel="noopener noreferrer"
            class="group hover:bg-token-main-surface-secondary flex items-center gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://arxiv.org&amp;sz=32"
                />
              </div>
              arxiv
            </div></a
          ><a
            href="https://www.ibm.com/think/topics/autogpt#:~:text=AutoGPT%20is%20an%20open,3.5"
            target="_blank"
            rel="noopener noreferrer"
            class="group hover:bg-token-main-surface-secondary flex items-center gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://www.ibm.com&amp;sz=32"
                />
              </div>
              ibm
            </div></a
          ><a
            href="https://www.qodo.ai/blog/gpt-4-vs-alphacode/#:~:text=GPT,percentile%20from%20the%20bottom"
            target="_blank"
            rel="noopener noreferrer"
            class="group hover:bg-token-main-surface-secondary flex items-center gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://www.qodo.ai&amp;sz=32"
                />
              </div>
              qodo
            </div></a
          ><a
            href="https://news.mit.edu/2025/can-ai-really-code-study-maps-roadblocks-to-autonomous-software-engineering-0716#:~:text=Imagine%20a%20future%20where%20artificial,day%20challenges"
            target="_blank"
            rel="noopener noreferrer"
            class="group hover:bg-token-main-surface-secondary flex items-center gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://news.mit.edu&amp;sz=32"
                />
              </div>
              news.mit
            </div></a
          ><a
            href="https://newsroom.ibm.com/2023-10-26-IBM-Launches-watsonx-Code-Assistant,-Delivers-Generative-AI-powered-Code-Generation-Capabilities-Built-for-Enterprise-Application-Modernization#:~:text=IBM%20Launches%20watsonx%20Code%20Assistant%2C,developer%20productivity%20on%20the%20platform"
            target="_blank"
            rel="noopener noreferrer"
            class="group hover:bg-token-main-surface-secondary flex items-center gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://newsroom.ibm.com&amp;sz=32"
                />
              </div>
              newsroom.ibm
            </div></a
          ><a
            href="https://lilianweng.github.io/posts/2023-06-23-agent/#:~:text=MRKL%20%28Karpas%20et%20al,calculator%2C%20currency%20converter%2C%20weather%20API"
            target="_blank"
            rel="noopener noreferrer"
            class="group hover:bg-token-main-surface-secondary flex items-center gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://lilianweng.github.io&amp;sz=32"
                />
              </div>
              lilianweng.github
            </div></a
          ><a
            href="https://openai.com/index/function-calling-and-other-api-updates/#:~:text=Developers%20can%20now%20describe%20functions,with%20external%20tools%20and%20APIs"
            target="_blank"
            rel="noopener noreferrer"
            class="group hover:bg-token-main-surface-secondary flex items-center gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://openai.com&amp;sz=32"
                />
              </div>
              openai
            </div></a
          ><a
            href="https://www.anthropic.com/news/100k-context-windows#:~:text=documents.%20,build%20on%20or%20modify%20it"
            target="_blank"
            rel="noopener noreferrer"
            class="group hover:bg-token-main-surface-secondary flex items-center gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://www.anthropic.com&amp;sz=32"
                />
              </div>
              anthropic
            </div></a
          ><a
            href="https://cyber.nyu.edu/2021/10/15/ccs-researchers-find-github-copilot-generates-vulnerable-code-40-of-the-time/#:~:text=A%20recent%20study%20by%20cybersecurity,be%20exploited%20by%20an%20attacker"
            target="_blank"
            rel="noopener noreferrer"
            class="group hover:bg-token-main-surface-secondary flex items-center gap-0.5 rounded-xl px-3 py-2.5"
            ><div class="line-clamp-1 flex h-6 items-center gap-2 text-xs">
              <div class="relative inline-block shrink-0">
                <img
                  alt=""
                  width="32"
                  height="32"
                  class="icon-sm bg-token-main-surface-primary rounded-full object-cover duration-200 motion-safe:transition-opacity opacity-100"
                  src="https://www.google.com/s2/favicons?domain=https://cyber.nyu.edu&amp;sz=32"
                />
              </div>
              cyber.nyu
            </div></a
          >
        </div>
      </div>
    </div>
  </div>
</div>
